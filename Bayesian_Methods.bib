@article{ahoModelSelectionEcologists2014,
  title = {Model Selection for Ecologists: The Worldviews of {{AIC}} and {{BIC}}},
  shorttitle = {Model Selection for Ecologists},
  author = {Aho, Ken and Derryberry, DeWayne and Peterson, Teri},
  year = {2014},
  month = mar,
  journal = {Ecology},
  volume = {95},
  number = {3},
  pages = {631--636},
  issn = {0012-9658, 1939-9170},
  doi = {10.1890/13-1452.1},
  urldate = {2023-12-20},
  langid = {english},
  note = {https://esajournals.onlinelibrary.wiley.com/doi/10.1890/13-1452.1},
  timestamp = {2023-12-20T10:05:59Z},
  file = {Aho et al. - 2014 - Model selection for ecologists the worldviews of .pdf:/Users/simonmartin/Zotero/storage/ZDV8YSHL/Aho et al. - 2014 - Model selection for ecologists the worldviews of .pdf:application/pdf}
}

@article{allisonComparisonSamplingTechniques2014,
  title = {Comparison of Sampling Techniques for {{Bayesian}} Parameter Estimation},
  author = {Allison, Rupert and Dunkley, Joanna},
  year = {2014},
  month = feb,
  journal = {Monthly Notices of the Royal Astronomical Society},
  volume = {437},
  number = {4},
  pages = {3918--3928},
  issn = {0035-8711},
  doi = {10.1093/mnras/stt2190},
  urldate = {2023-05-31},
  abstract = {The posterior probability distribution for a set of model parameters encodes all that the data have to tell us in the context of a given model; it is the fundamental quantity for Bayesian parameter estimation. In order to infer the posterior probability distribution we have to decide how to explore parameter space. Here we compare three prescriptions for how parameter space is navigated, discussing their relative merits. We consider Metropolis--Hasting sampling, nested sampling and affine-invariant ensemble Markov chain Monte Carlo (MCMC) sampling. We focus on their performance on toy-model Gaussian likelihoods and on a real-world cosmological data set. We outline the sampling algorithms themselves and elaborate on performance diagnostics such as convergence time, scope for parallelization, dimensional scaling, requisite tunings and suitability for non-Gaussian distributions. We find that nested sampling delivers high-fidelity estimates for posterior statistics at low computational cost, and should be adopted in favour of Metropolis--Hastings in many cases. Affine-invariant MCMC is competitive when computing clusters can be utilized for massive parallelization. Affine-invariant MCMC and existing extensions to nested sampling naturally probe multimodal and curving distributions.},
  note = {https://doi.org/10.1093/mnras/stt2190},
  timestamp = {2023-05-31T10:11:53Z},
  file = {Allison_Dunkley_2014_Comparison of sampling techniques for Bayesian parameter estimation.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Allison_Dunkley_2014_Comparison of sampling techniques for Bayesian parameter estimation.pdf:application/pdf;Snapshot:/Users/simonmartin/Zotero/storage/6PSBAZJI/1011939.html:text/html}
}

@article{Andrew2004,
  title = {{{INFORMATION THEORY}}, {{INFERENCE}}, {{AND LEARNING ALGORITHMS}}, by {{David J}}. {{C}}. {{MacKay}}, {{Cambridge University Press}}, {{Cambridge}}, 2003, Hardback, Xii + 628 Pp., {{ISBN}} 0-521-64298-1 (30.00)},
  author = {Andrew, Alex M.},
  year = {2004},
  month = jun,
  journal = {Robotica},
  volume = {22},
  number = {3},
  pages = {348--349},
  issn = {0263-5747},
  doi = {10.1017/S026357470426043X},
  note = {http://www.journals.cambridge.org/abstract\_S026357470426043X},
  timestamp = {2011-07-14T13:47:26Z}
}

@article{andrieuIntroductionMCMCMachine,
  title = {An {{Introduction}} to {{MCMC}} for {{Machine Learning}}},
  author = {Andrieu, Christophe and Andrieu, C},
  abstract = {This purpose of this introductory paper is threefold. First, it introduces the Monte Carlo method with emphasis on probabilistic machine learning. Second, it reviews the main building blocks of modern Markov chain Monte Carlo simulation, thereby providing and introduction to the remaining papers of this special issue. Lastly, it discusses new interesting research horizons.},
  langid = {english},
  keywords = {No DOI found},
  timestamp = {2023-12-15T13:30:42Z},
  file = {Andrieu and Andrieu - An Introduction to MCMC for Machine Learning.pdf:/Users/simonmartin/Zotero/storage/7CPPT785/Andrieu and Andrieu - An Introduction to MCMC for Machine Learning.pdf:application/pdf}
}

@article{annisBayesianInferenceStan2017,
  title = {Bayesian Inference with {{Stan}}: {{A}} Tutorial on Adding Custom Distributions},
  author = {Annis, Jeffrey and Miller, Brent J. and Palmeri, Thomas J.},
  year = {2017},
  journal = {Behavior Research Methods},
  volume = {49},
  number = {3},
  pages = {863--886},
  issn = {1554-3528},
  doi = {10.3758/s13428-016-0746-9},
  abstract = {When evaluating cognitive models based on fits to observed data (or, really, any model that has free parameters), parameter estimation is critically important. Traditional techniques like hill climbing by minimizing or maximizing a fit statistic often result in point estimates. Bayesian approaches instead estimate parameters as posterior probability distributions, and thus naturally account for the uncertainty associated with parameter estimation; Bayesian approaches also offer powerful and principled methods for model comparison. Although software applications such as WinBUGS (Lunn, Thomas, Best, \& Spiegelhalter, Statistics and Computing, 10, 325--337, 2000) and JAGS (Plummer, 2003) provide ``turnkey''-style packages for Bayesian inference, they can be inefficient when dealing with models whose parameters are correlated, which is often the case for cognitive models, and they can impose significant technical barriers to adding custom distributions, which is often necessary when implementing cognitive models within a Bayesian framework. A recently developed software package called Stan (Stan Development Team, 2015) can solve both problems, as well as provide a turnkey solution to Bayesian inference. We present a tutorial on how to use Stan and how to add custom distributions to it, with an example using the linear ballistic accumulator model (Brown \& Heathcote, Cognitive Psychology, 57, 153--178. doi: 10.1016/j.cogpsych.2007.12.002 , 2008).},
  pmid = {27287444},
  keywords = {accumulator,bayesian inference,Bayesian inference,Bayesian inferenceStanLinear ballistic accumulat,linear ballistic,Linear ballistic accumulat,probabilistic programming,stan,Stan},
  note = {http://link.springer.com/10.3758/s13428-016-0746-9},
  timestamp = {2023-04-28T14:30:38Z},
  file = {Annis et al_2017_Bayesian inference with Stan.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Annis et al_2017_Bayesian inference with Stan.pdf:application/pdf}
}

@article{ashtonNestedSamplingPhysical2022,
  title = {Nested Sampling for Physical Scientists},
  author = {Ashton, Greg and Bernstein, Noam and Buchner, Johannes and Chen, Xi and Cs{\'a}nyi, G{\'a}bor and Fowlie, Andrew and Feroz, Farhan and Griffiths, Matthew and Handley, Will and Habeck, Michael and Higson, Edward and Hobson, Michael and Lasenby, Anthony and Parkinson, David and P{\'a}rtay, Livia B. and Pitkin, Matthew and Schneider, Doris and Speagle, Joshua S. and South, Leah and Veitch, John and Wacker, Philipp and Wales, David J. and Yallup, David},
  year = {2022},
  month = may,
  doi = {10.1038/s43586-022-00121-x},
  abstract = {We review Skilling's nested sampling (NS) algorithm for Bayesian inference and more broadly multi-dimensional integration. After recapitulating the principles of NS, we survey developments in implementing efficient NS algorithms in practice in high-dimensions, including methods for sampling from the so-called constrained prior. We outline the ways in which NS may be applied and describe the application of NS in three scientific fields in which the algorithm has proved to be useful: cosmology, gravitational-wave astronomy, and materials science. We close by making recommendations for best practice when using NS and by summarizing potential limitations and optimizations of NS.},
  note = {http://arxiv.org/abs/2205.15570 http://dx.doi.org/10.1038/s43586-022-00121-x},
  timestamp = {2023-03-24T12:15:10Z},
  file = {Ashton et al_2022_Nested sampling for physical scientists.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Ashton et al_2022_Nested sampling for physical scientists.pdf:application/pdf}
}

@article{assisApplicationMonteCarloSimulation2022,
  title = {Application of {{Monte-Carlo Simulation Towards}} a {{Better Understanding}} of {{Bayes}}' {{Theorem}} in {{Engineering Education}}},
  author = {Assis, Rui and Carmona Marques, Pedro and Vidal, Raphaela},
  year = {2022},
  month = feb,
  journal = {U.Porto Journal of Engineering},
  volume = {8},
  number = {1},
  pages = {2--11},
  issn = {2183-6493},
  doi = {10.24840/2183-6493_008.001_0002},
  urldate = {2023-06-08},
  abstract = {Bayes' Theorem (BT) is treated in probability theory and statistics. The BT shows how to change the probabilities a priori in view of new evidence, to obtain probabilities a posteriori. With the Bayesian interpretation of probability, the BT is expressed as the probability of an event (or the degree of belief in the occurrence of an event) should be changed, after considering evidence about the occurrence of that event. Bayesian inference is fundamental to Bayesian statistics. An example of practical application of this theorem in Health Systems is to consider the existence of false positives and false negatives in diagnoses. At the Academy, the theme of BT is exposed almost exclusively in its analytical form. With this article, the authors intend to contribute to clarify the logic behind this theorem, and get students better understanding of its important fields of application, using three methods: the classic analytical (Bayesian inference), the frequentist (frequency inference) and the numerical simulation of Monte-Carlo. Thus, it intends to explain BT on a practical and friendly way that provides understanding to students avoiding memorizing the formulas. We provide a spreadsheet that is accessible to any professor. Moreover, we highlight the methodology could be extended to other topics.},
  langid = {english},
  note = {https://journalengineering.fe.up.pt/index.php/upjeng/article/view/2183-6493\_008-001\_0002},
  timestamp = {2023-06-08T15:25:28Z},
  file = {Assis et al. - 2022 - Application of Monte-Carlo Simulation Towards a Be.pdf:/Users/simonmartin/Zotero/storage/XYRD6DJQ/Assis et al. - 2022 - Application of Monte-Carlo Simulation Towards a Be.pdf:application/pdf}
}

@article{bahariDesignDevelopmentMultifingered2012,
  title = {Design and {{Development}} of a {{Multifingered Prosthetic Hand}}},
  author = {Bahari, M. Saiful and Jaffar, Ahmed and Low, Cheng Yee and Jaafar, Roseleena and Roese, Kolja and Yussof, Hanafiah},
  year = {2012},
  month = nov,
  journal = {International Journal of Social Robotics},
  volume = {4},
  number = {S1},
  pages = {59--66},
  issn = {1875-4791, 1875-4805},
  doi = {10.1007/s12369-011-0133-8},
  urldate = {2024-01-25},
  abstract = {This paper focuses on the development of a robotic hand having fourteen degree-of-freedom (DOF) to trigger finger movement. The design combination and integration of fingers will produce a prosthetic hand which is approximately the size of a human hand. The prototype hand comprised of servo motors located at the palm to drive the pulley and belt mechanism connected to each of the finger joints enabling independent flexion finger link movement. A lightweight Light Dependent Resistor (LDR) sensor is positioned at the middle of the palm is to close around objects. The prosthetic provides three different controls of finger movement through the use of a Graphical User Interface (GUI) with computer interface, program and manually controlled and autonomous movement provided by a microcontroller of this robotic hand. The prototype hand model was developed using rapid prototype technique in order to get the best part quality and reduction in weight. The link parameter analysis to determine the end tip position of the robotic finger at certain rotation angle was computed. Experimental work has been carried out on the prototype robotic hand to ensure the entire rotation angle and movement of each link is functioning as desired.},
  langid = {english},
  note = {http://link.springer.com/10.1007/s12369-011-0133-8},
  timestamp = {2024-01-25T14:00:37Z},
  file = {Bahari et al. - 2012 - Design and Development of a Multifingered Prosthet.pdf:/Users/simonmartin/Zotero/storage/F9NQBZKR/Bahari et al. - 2012 - Design and Development of a Multifingered Prosthet.pdf:application/pdf}
}

@article{bandyopadhayayCurveFittingProblem1996,
  title = {The {{Curve Fitting Problem}}: {{A Bayesian Approach}}},
  shorttitle = {The {{Curve Fitting Problem}}},
  author = {Bandyopadhayay, Prasanta S. and Boik, Robert J. and Vineberg, Susan},
  year = {1996},
  journal = {Philosophy of Science},
  volume = {63},
  number = {S3},
  pages = {S264-S272},
  issn = {0031-8248, 1539-767X},
  doi = {10.1086/289960},
  urldate = {2024-02-06},
  abstract = {In the curve fitting problem two conflicting desiderata, simplicity and goodness-of-fit, pull in opposite directions. To this problem, we propose a solution that strikes a balance between simplicity and goodness-of-fit. Using Bayes' theorem we argue that the notion of prior probability represents a measurement of simplicity of a theory, whereas the notion of likelihood represents the theory's goodness-of-fit. We justify the use of prior probability and show how to calculate the likelihood of a family of curves. We diagnose the relationship between simplicity of a theory and its predictive accuracy.},
  langid = {english},
  note = {https://www.cambridge.org/core/product/identifier/S0031824800075383/type/journal\_article},
  timestamp = {2024-02-06T10:35:03Z},
  file = {Bandyopadhayay et al. - 1996 - The Curve Fitting Problem A Bayesian Approach.pdf:/Users/simonmartin/Zotero/storage/9UERDQPK/Bandyopadhayay et al. - 1996 - The Curve Fitting Problem A Bayesian Approach.pdf:application/pdf}
}

@article{bandyopadhayayCurveFittingProblem1996a,
  title = {The {{Curve Fitting Problem}}: {{A Bayesian Approach}}},
  shorttitle = {The {{Curve Fitting Problem}}},
  author = {Bandyopadhayay, Prasanta S. and Boik, Robert J. and Vineberg, Susan},
  year = {1996},
  journal = {Philosophy of Science},
  volume = {63},
  number = {S3},
  pages = {S264-S272},
  issn = {0031-8248, 1539-767X},
  doi = {10.1086/289960},
  urldate = {2024-01-05},
  abstract = {In the curve fitting problem two conflicting desiderata, simplicity and goodness-of-fit, pull in opposite directions. To this problem, we propose a solution that strikes a balance between simplicity and goodness-of-fit. Using Bayes' theorem we argue that the notion of prior probability represents a measurement of simplicity of a theory, whereas the notion of likelihood represents the theory's goodness-of-fit. We justify the use of prior probability and show how to calculate the likelihood of a family of curves. We diagnose the relationship between simplicity of a theory and its predictive accuracy.},
  langid = {english},
  note = {https://www.cambridge.org/core/product/identifier/S0031824800075383/type/journal\_article},
  timestamp = {2024-01-05T08:25:06Z},
  file = {Bandyopadhayay et al. - 1996 - The Curve Fitting Problem A Bayesian Approach.pdf:/Users/simonmartin/Zotero/storage/G9U9T295/Bandyopadhayay et al. - 1996 - The Curve Fitting Problem A Bayesian Approach.pdf:application/pdf}
}

@article{Bernardo2003,
  title = {Bayesian Statistics},
  author = {Bernardo, Jos{\'e} M and Estad{\'i}stica, Departamento De and Matem{\'a}ticas, Facultad De},
  year = {2003},
  pages = {1--46},
  keywords = {amount of information,and phrases,dations of inference,decision theory,discrepancy,exchangeability,foun-,hypothesis testing,interval estimation,intrinsic,maximum entropy,point estimation,rational degree of belief,reference analysis,scientific reporting},
  timestamp = {2023-03-28T08:53:11Z},
  file = {Bernardo et al_2003_Bayesian statistics.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Bernardo et al_2003_Bayesian statistics.pdf:application/pdf}
}

@article{berryTeachingElementaryBayesian1997,
  title = {Teaching {{Elementary Bayesian Statistics}} with {{Real Applications}} in {{Science}}},
  author = {Berry, Donald A.},
  year = {1997},
  journal = {The American Statistician},
  volume = {51},
  number = {3},
  eprint = {2684895},
  eprinttype = {jstor},
  pages = {241--246},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  issn = {0003-1305},
  doi = {10.2307/2684895},
  urldate = {2023-05-05},
  abstract = {University courses in elementary statistics are usually taught from a frequentist perspective. In this paper I suggest how such courses can be taught using a Bayesian approach, and I indicate why beginning students are well served by a Bayesian course. A principal focus of any good elementary course is the application of statistics to real and important scientific problems. The Bayesian approach fits neatly with a scientific focus. Bayesians take a larger view, and one not limited to data analysis. In particular, the Bayesian approach is subjective, and requires assessing prior probabilities. This requirement forces users to relate current experimental evidence to other available information-including previous experiments of a related nature, where "related" is judged subjectively. I discuss difficulties faced by instructors and students in elementary Bayesian courses, and provide a sample syllabus for an elementary Bayesian course.},
  note = {https://www.jstor.org/stable/2684895},
  timestamp = {2023-05-05T07:32:44Z},
  file = {Berry_1997_Teaching Elementary Bayesian Statistics with Real Applications in Science.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Berry_1997_Teaching Elementary Bayesian Statistics with Real Applications in Science.pdf:application/pdf}
}

@article{Billingsley1960,
  title = {{{THE LINDEBERG-L{\'E}VY THEOREM FOR MARTINGALES}}},
  author = {Billingsley, Patrick},
  year = {1961},
  journal = {Proceedings of the American Mathematical Society},
  volume = {12},
  number = {5},
  pages = {788--792},
  timestamp = {2023-08-11T08:20:18Z},
  file = {Billingsley_1960_Elü ^ -i ].pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Billingsley_1960_Elü ^ -i ].pdf:application/pdf}
}

@article{Bretthorst1990,
  title = {An {{Introduction}} to {{Parameter Estimation Using Bayesian Probability Theory}}},
  author = {Bretthorst, G. Larry},
  year = {1990},
  journal = {Maximum Entropy and Bayesian Methods},
  pages = {53--79},
  doi = {10.1007/978-94-009-0683-9_5},
  abstract = {Bayesian probability theory does not define a probability as a frequency of occurence; rather it defines it as a reasonable degree of belief.},
  timestamp = {2020-07-24T11:32:10Z},
  file = {Bretthorst_1990_An Introduction to Parameter Estimation Using Bayesian Probability Theory.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Bretthorst_1990_An Introduction to Parameter Estimation Using Bayesian Probability Theory.pdf:application/pdf}
}

@misc{brewerDNest4DiffusiveNested2016,
  title = {{{DNest4}}: {{Diffusive Nested Sampling}} in {{C}}++ and {{Python}}},
  shorttitle = {{{DNest4}}},
  author = {Brewer, Brendon J. and {Foreman-Mackey}, Daniel},
  year = {2016},
  month = nov,
  number = {arXiv:1606.03757},
  eprint = {1606.03757},
  primaryclass = {astro-ph, physics:physics, stat},
  publisher = {arXiv},
  urldate = {2023-03-28},
  abstract = {In probabilistic (Bayesian) inferences, we typically want to compute properties of the posterior distribution, describing knowledge of unknown quantities in the context of a particular dataset and the assumed prior information. The marginal likelihood, also known as the "evidence", is a key quantity in Bayesian model selection. The Diffusive Nested Sampling algorithm, a variant of Nested Sampling, is a powerful tool for generating posterior samples and estimating marginal likelihoods. It is effective at solving complex problems including many where the posterior distribution is multimodal or has strong dependencies between variables. DNest4 is an open source (MIT licensed), multi-threaded implementation of this algorithm in C++11, along with associated utilities including: i) RJObject, a class template for finite mixture models, (ii) A Python package allowing basic use without C++ coding, and iii) Experimental support for models implemented in Julia. In this paper we demonstrate DNest4 usage through examples including simple Bayesian data analysis, finite mixture models, and Approximate Bayesian Computation.},
  archiveprefix = {arxiv},
  howpublished = {http://arxiv.org/abs/1606.03757},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Physics - Data Analysis Statistics and Probability,Statistics - Computation},
  timestamp = {2023-03-28T08:42:25Z},
  file = {Brewer_Foreman-Mackey_2016_DNest4.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Brewer_Foreman-Mackey_2016_DNest4.pdf:application/pdf;arXiv.org Snapshot:/Users/simonmartin/Zotero/storage/JTK2628H/1606.html:text/html}
}

@article{brillemanBayesianPiecewiseLinear2017,
  title = {Bayesian {{Piecewise Linear Mixed Models With}} a {{Random Change Point}}},
  author = {Brilleman, Samuel L. and Howe, Laura D. and Wolfe, Rory and Tilling, Kate},
  year = {2017},
  month = nov,
  journal = {Epidemiology (Cambridge, Mass.)},
  volume = {28},
  number = {6},
  pages = {827--833},
  issn = {1044-3983},
  doi = {10.1097/EDE.0000000000000723},
  urldate = {2023-08-22},
  abstract = {Background: Body mass index (BMI) rebound refers to the beginning of the second rise in BMI during childhood. Accurate estimation of an individual's timing of BMI rebound is important because it is associated with health outcomes in later life. Methods: We estimated BMI trajectories for 6545 children from the Avon Longitudinal Study of Parents and Children. We used a novel Bayesian two-phase piecewise linear mixed model where the ``change point'' was an individual-level random effect corresponding to the individual-specific timing of BMI rebound. The model's individual-level random effects (intercept, prechange slope, postchange slope, change point) were multivariate normally distributed with an unstructured variance--covariance matrix, thereby, allowing for correlation between all random effects. Results: Average age at BMI rebound (mean change point) was 6.5 (95\% credible interval: 6.4 to 6.6) years. The standard deviation of the individual-specific timing of BMI rebound (random effects) was 2.0 years for females and 1.6 years for males. Correlation between the prechange slope and change point was 0.57, suggesting that faster rates of decline in BMI prior to rebound were associated with rebound occurring at an earlier age. Simulations showed that estimates from the model were less biased than those from models, assuming a common change point for all individuals or a nonlinear trajectory based on fractional polynomials. Conclusions: Our model flexibly estimated the individual-specific timing of BMI rebound, while retaining parameters that are meaningful and easy to interpret. It is applicable in any situation where one wishes to estimate a change-point process which varies between individuals.},
  pmcid = {PMC5625953},
  pmid = {28817471},
  note = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5625953/},
  timestamp = {2023-08-22T15:25:58Z},
  file = {Brilleman et al_2017_Bayesian Piecewise Linear Mixed Models With a Random Change Point.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Brilleman et al_2017_Bayesian Piecewise Linear Mixed Models With a Random Change Point.pdf:application/pdf}
}

@article{broemelingDetectingStructuralChange1981,
  title = {Detecting Structural Change in Linear Models},
  author = {Broemeling, L. D. and Choy, J. H. Chin},
  year = {1981},
  month = jan,
  journal = {Communications in Statistics - Theory and Methods},
  publisher = {Marcel Dekker, Inc.},
  doi = {10.1080/03610928108828209},
  urldate = {2023-10-30},
  copyright = {Copyright Taylor and Francis Group, LLC},
  langid = {english},
  note = {https://www.tandfonline.com/doi/abs/10.1080/03610928108828209},
  timestamp = {2023-10-30T16:27:53Z},
  file = {Snapshot:/Users/simonmartin/Zotero/storage/CDK6PSSV/03610928108828209.html:text/html}
}

@article{browneIntroductionMCMCMethods,
  title = {An {{Introduction}} to {{MCMC}} Methods and {{Bayesian Statistics}}},
  author = {Browne, William},
  langid = {english},
  keywords = {No DOI found},
  timestamp = {2024-01-29T12:22:54Z},
  file = {Browne - An Introduction to MCMC methods and Bayesian Stati.pdf:/Users/simonmartin/Zotero/storage/MW7UQH53/Browne - An Introduction to MCMC methods and Bayesian Stati.pdf:application/pdf}
}

@article{campbellIntroductionPhysicalStatisticalModelling,
  title = {An {{Introduction}} to {{Physical-Statistical Modelling Using Bayesian Methods}}},
  author = {Campbell, Edward P},
  langid = {english},
  timestamp = {2023-06-08T15:20:44Z},
  file = {Campbell - An Introduction to Physical-Statistical Modelling .pdf:/Users/simonmartin/Zotero/storage/8LDPLZQM/Campbell - An Introduction to Physical-Statistical Modelling .pdf:application/pdf}
}

@inproceedings{catichaRelativeEntropyInductive2004,
  title = {Relative {{Entropy}} and {{Inductive Inference}}},
  booktitle = {{{AIP Conference Proceedings}}},
  author = {Caticha, Ariel},
  year = {2004},
  volume = {707},
  eprint = {physics/0311093},
  pages = {75--96},
  issn = {0094243X},
  doi = {10.1063/1.1751358},
  urldate = {2023-08-21},
  abstract = {We discuss how the method of maximum entropy, MaxEnt, can be extended beyond its original scope, as a rule to assign a probability distribution, to a full-fledged method for inductive inference. The main concept is the (relative) entropy S[p{\textbar}q] which is designed as a tool to update from a prior probability distribution q to a posterior probability distribution p when new information in the form of a constraint becomes available. The extended method goes beyond the mere selection of a single posterior p, but also addresses the question of how much less probable other distributions might be. Our approach clarifies how the entropy S[p{\textbar}q] is used while avoiding the question of its meaning. Ultimately, entropy is a tool for induction which needs no interpretation. Finally, being a tool for generalization from special examples, we ask whether the functional form of the entropy depends on the choice of the examples and we find that it does. The conclusion is that there is no single general theory of inductive inference and that alternative expressions for the entropy are possible.},
  archiveprefix = {arxiv},
  keywords = {Physics - Data Analysis Statistics and Probability,Physics - General Physics},
  note = {http://arxiv.org/abs/physics/0311093},
  timestamp = {2023-08-21T08:19:02Z},
  file = {Caticha_2004_Relative Entropy and Inductive Inference.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Caticha_2004_Relative Entropy and Inductive Inference.pdf:application/pdf;arXiv.org Snapshot:/Users/simonmartin/Zotero/storage/X3I94DPP/0311093.html:text/html}
}

@article{chakrabartyBayesianEstimationDensity2015,
  title = {Bayesian {{Estimation}} of {{Density}} via {{Multiple Sequential Inversions}} of {{Two-Dimensional Images With Application}} to {{Electron Microscopy}}},
  author = {Chakrabarty, Dalia and Gabrielyan, Nare and Rigat, Fabio and Beanland, Richard and Paul, Shashi},
  year = {2015},
  month = apr,
  journal = {Technometrics},
  volume = {57},
  number = {2},
  pages = {217--233},
  publisher = {Taylor \& Francis},
  issn = {0040-1706},
  doi = {10.1080/00401706.2014.923789},
  urldate = {2023-04-27},
  abstract = {We present a new Bayesian methodology to learn the unknown material density of a given sample by inverting its two-dimensional images that are taken with a scanning electron microscope. An image results from a sequence of projections of the convolution of the density function with the unknown microscopy correction function that we also learn from the data; thus, learning of the unknowns demands multiple inversions. We invoke a novel design of experiment, involving imaging at multiple values of the parameter that controls the subsurface depth from which information about the density structure is carried, to result in the image. Real-life material density functions are characterized by high-density contrasts and are highly discontinuous, implying that they exhibit correlation structures that do not vary smoothly. In the absence of training data, modeling such correlation structures of real material density functions is not possible. So we discretize the material sample and treat values of the density function at chosen locations inside it as independent and distribution-free parameters. Resolution of the available image dictates the discretization length of the model; three models pertaining to distinct resolution classes (at micrometer to nanometer scale lengths) are developed. We develop priors on the material density, such that these priors adapt to the sparsity inherent in the density function. The likelihood is defined in terms of the distance between the convolution of the unknown functions and the image data. The posterior probability density of the unknowns given the data is expressed using the developed priors on the density and priors on the microscopy correction function as elicited from the microscopy literature. We achieve posterior samples using an adaptive Metropolis-within-Gibbs inference scheme. The method is applied to learn the material density of a three-dimensional sample of a nano-structure, using real image data. Illustrations on simulated image data of alloy samples are also included.},
  keywords = {Adaptive Metropolis,Bayesian methods,Inverse problems,Priors on sparsity.},
  note = {https://doi.org/10.1080/00401706.2014.923789},
  timestamp = {2023-04-27T16:20:22Z},
  file = {Full Text PDF:/Users/simonmartin/Zotero/storage/ZU7A3PW3/Chakrabarty et al. - 2015 - Bayesian Estimation of Density via Multiple Sequen.pdf:application/pdf}
}

@techreport{chenBayesianFilteringKalman,
  title = {Bayesian {{Filtering}}: {{From Kalman Filters}} to {{Particle Filters}}, and {{Beyond}}},
  author = {Chen, Zhe},
  abstract = {In this self-contained survey/review paper, we systematically investigate the roots of Bayesian filtering as well as its rich leaves in the literature. Stochastic filtering theory is briefly reviewed with emphasis on nonlinear and non-Gaussian filtering. Following the Bayesian statistics, different Bayesian filtering techniques are developed given different scenarios. Under linear quadratic Gaussian circumstance, the celebrated Kalman filter can be derived within the Bayesian framework. Optimal/suboptimal nonlinear filtering techniques are extensively investigated. In particular, we focus our attention on the Bayesian filtering approach based on sequential Monte Carlo sampling, the so-called particle filters. Many variants of the particle filter as well as their features (strengths and weaknesses) are discussed. Related theoretical and practical issues are addressed in detail. In addition, some other (new) directions on Bayesian filtering are also explored.},
  timestamp = {2023-03-29T12:58:49Z},
  file = {Chen_Bayesian Filtering.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Chen_Bayesian Filtering.pdf:application/pdf}
}

@article{chenBayesianFilteringKalmana,
  title = {Bayesian Filtering: {{From Kalman}} Filters to Particle Filters, and Beyond},
  author = {Chen, Zhe},
  abstract = {In this self-contained survey/review paper, we systematically investigate the roots of Bayesian filtering as well as its rich leaves in the literature. Stochastic filtering theory is briefly reviewed with emphasis on nonlinear and non-Gaussian filtering. Following the Bayesian statistics, different Bayesian filtering techniques are developed given different scenarios. Under linear quadratic Gaussian circumstance, the celebrated Kalman filter can be derived within the Bayesian framework. Optimal/suboptimal nonlinear filtering techniques are extensively investigated. In particular, we focus our attention on the Bayesian filtering approach based on sequential Monte Carlo sampling, the so-called particle filters. Many variants of the particle filter as well as their features (strengths and weaknesses) are discussed. Related theoretical and practical issues are addressed in detail. In addition, some other (new) directions on Bayesian filtering are also explored.},
  langid = {english},
  timestamp = {2023-09-12T19:36:22Z},
  file = {Chen - Bayesian filtering From Kalman filters to particl.pdf:/Users/simonmartin/Zotero/storage/8W4G4ZKF/Chen - Bayesian filtering From Kalman filters to particl.pdf:application/pdf}
}

@article{chopinPropertiesNestedSampling2010,
  title = {Properties of {{Nested Sampling}}},
  author = {Chopin, Nicolas and Robert, Christian},
  year = {2010},
  month = sep,
  journal = {Biometrika},
  volume = {97},
  number = {3},
  eprint = {0801.3887},
  primaryclass = {math, stat},
  pages = {741--755},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/asq021},
  urldate = {2023-05-04},
  abstract = {Nested sampling is a simulation method for approximating marginal likelihoods proposed by Skilling (2006). We establish that nested sampling has an approximation error that vanishes at the standard Monte Carlo rate and that this error is asymptotically Gaussian. We show that the asymptotic variance of the nested sampling approximation typically grows linearly with the dimension of the parameter. We discuss the applicability and efficiency of nested sampling in realistic problems, and we compare it with two current methods for computing marginal likelihood. We propose an extension that avoids resorting to Markov chain Monte Carlo to obtain the simulated points.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Computation},
  note = {http://arxiv.org/abs/0801.3887},
  timestamp = {2023-05-04T11:25:35Z},
  file = {Chopin_Robert_2010_Properties of Nested Sampling.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Chopin_Robert_2010_Properties of Nested Sampling.pdf:application/pdf;arXiv.org Snapshot:/Users/simonmartin/Zotero/storage/QCSBCVFG/0801.html:text/html}
}

@misc{copleyUnderstandingMonteCarlo2023,
  title = {Understanding the {{Monte Carlo Markov Chain}}: {{A Key}} to {{Bayesian Inference}}},
  shorttitle = {Understanding the {{Monte Carlo Markov Chain}}},
  author = {Copley, Charles},
  year = {2023},
  month = jun,
  journal = {Medium},
  urldate = {2024-01-29},
  abstract = {When it comes to advanced statistical analysis and probabilistic modeling, one of the most powerful techniques to emerge in recent years is{\dots}},
  howpublished = {https://charlescopley.medium.com/understanding-the-monte-carlo-markov-chain-a-key-to-bayesian-inference-163b03f9fd2d},
  langid = {english},
  timestamp = {2024-01-29T13:04:38Z},
  file = {Snapshot:/Users/simonmartin/Zotero/storage/JIRBGZ8Q/understanding-the-monte-carlo-markov-chain-a-key-to-bayesian-inference-163b03f9fd2d.html:text/html}
}

@article{coxProbabilityFrequencyReasonable1946,
  title = {Probability, {{Frequency}}, and {{Reasonable Expectation}}},
  author = {Cox, R T},
  year = {1946},
  journal = {American Journal of Physics},
  volume = {14},
  number = {1},
  pages = {1--13},
  urldate = {2023-11-17},
  note = {http://jimbeck.caltech.edu/summerlectures/references/ProbabilityFrequencyReasonableExpectation.pdf},
  timestamp = {2023-12-14T09:05:07Z},
  file = {ProbabilityFrequencyReasonableExpectation.pdf:/Users/simonmartin/Zotero/storage/ZL5GCRJA/ProbabilityFrequencyReasonableExpectation.pdf:application/pdf}
}

@article{deisenrothMathematicsMachineLearning,
  title = {Mathematics for {{Machine Learning}}},
  author = {Deisenroth, Marc Peter and Faisal, A Aldo and Ong, Cheng Soon},
  langid = {english},
  timestamp = {2023-04-03T09:59:15Z},
  file = {Deisenroth et al. - Mathematics for Machine Learning.pdf:/Users/simonmartin/Zotero/storage/V2YFI9QJ/Deisenroth et al. - Mathematics for Machine Learning.pdf:application/pdf}
}

@article{dellaportasBayesianModelVariable,
  title = {On {{Bayesian}} Model and Variable Selection Using {{MCMC}}},
  author = {Dellaportas, Petros and Forster, Jonathan J and Ntzoufras, Ioannis},
  abstract = {Several MCMC methods have been proposed for estimating probabilities of models and associated `model-averaged' posterior distributions in the presence of model uncertainty. We discuss, compare, develop and illustrate several of these methods, focussing on connections between them.},
  langid = {english},
  keywords = {No DOI found},
  timestamp = {2024-01-29T12:59:03Z},
  file = {Dellaportas et al. - On Bayesian model and variable selection using MCM.pdf:/Users/simonmartin/Zotero/storage/J4KMPCD8/Dellaportas et al. - On Bayesian model and variable selection using MCM.pdf:application/pdf}
}

@article{dinizBayesianInferenceLinear2003,
  title = {Bayesian Inference of a Linear Segmented Regression Model},
  author = {Diniz, Carlos Alberto R. and Milan, Luis A. and Mazucheli, Josmar},
  year = {2003},
  journal = {Brazilian Journal of Probability and Statistics},
  volume = {17},
  number = {1},
  eprint = {43601020},
  eprinttype = {jstor},
  pages = {1--16},
  publisher = {[Brazilian Statistical Association, Institute of Mathematical Statistics]},
  issn = {0103-0752},
  urldate = {2023-07-18},
  abstract = {Robust Bayesian Inference in a linear-linear segmented regression model, assuming non-homogeneous error variance, is explored for cases in which the errors follow Student's t distributions. Metropolis-within-Gibbs algorithms are used in order to estimate the posterior distribution of the change point and the model parameters. The methodology is illustrated by the analysis of two simulated data sets and a real data set from a clinical study designed to determine the anaerobic threshold of a healthy male during dynamic exercise.},
  note = {https://www.jstor.org/stable/43601020},
  timestamp = {2023-07-18T15:52:27Z},
  file = {Diniz et al_2003_Bayesian inference of a linear segmented regression model.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Diniz et al_2003_Bayesian inference of a linear segmented regression model.pdf:application/pdf}
}

@misc{dogucuDesignImplementationBayesian2023,
  title = {The {{Design}} and {{Implementation}} of a {{Bayesian Data Analysis Lesson}} for {{Pre-Service Mathematics}} and {{Science Teachers}}},
  author = {Dogucu, Mine and Kazak, Sibel and Rosenberg, Joshua},
  year = {2023},
  month = apr,
  number = {arXiv:2304.01276},
  eprint = {2304.01276},
  primaryclass = {stat},
  publisher = {arXiv},
  urldate = {2023-05-05},
  abstract = {With the rise of the popularity of Bayesian methods and accessible computer software, teaching and learning about Bayesian methods are expanding. However, most educational opportunities are geared toward statistics and data science students and are less available in the broader STEM fields. In addition, there are fewer opportunities at the K-12 level. With the indirect aim of introducing Bayesian methods at the K-12 level, we have developed a Bayesian Data Analysis activity and implemented it with 35 mathematics and science pre-service teachers. In this manuscript, we describe the activity, the web app supporting the activity, and pre-service teachers' perceptions of the activity. Lastly, we discuss future directions for preparing K-12 teachers in teaching and learning about Bayesian methods.},
  archiveprefix = {arxiv},
  howpublished = {http://arxiv.org/abs/2304.01276},
  keywords = {Statistics - Other Statistics},
  timestamp = {2023-05-05T07:36:08Z},
  file = {Dogucu et al_2023_The Design and Implementation of a Bayesian Data Analysis Lesson for.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Dogucu et al_2023_The Design and Implementation of a Bayesian Data Analysis Lesson for.pdf:application/pdf;arXiv.org Snapshot:/Users/simonmartin/Zotero/storage/CC2S8Q7I/2304.html:text/html}
}

@article{doseBayesianInferencePhysics2003,
  title = {Bayesian Inference in Physics: Case Studies},
  author = {Dose, V},
  year = {2003},
  journal = {Reports on Progress in Physics},
  volume = {66},
  number = {9},
  pages = {1421--1461},
  issn = {0034-4885},
  doi = {10.1088/0034-4885/66/9/202},
  abstract = {This report describes the Bayesian approach to probability theory with emphasis on the application to the evaluation of experimental data. A brief summary of Bayesian principles is given, with a discussion of concepts, terminology and pitfalls. The step from Bayesian principles to data processing involves major numerical efforts. We address the presently employed procedures of numerical integration, which are mainly based on the Monte Carlo method. The case studies include examples from electron spectroscopies, plasma physics, ion beam analysis and mass spectrometry. Bayesian solutions to the ubiquitous problem of spectrum restoration are presented and advantages and limitations are discussed. Parameter estimation within the Bayesian framework is shown to allow for the incorporation of expert knowledge which in turn allows the treatment of under-determined problems which are inaccessible by the traditional maximum likelihood method. A unique and extremely valuable feature of Bayesian theory is the model comparison option. Bayesian model comparison rests on Ockham's razor which limits the complexity of a model to the amount necessary to explain the data without fitting noise. Finally we deal with the treatment of inconsistent data. They arise frequently in experimental work either from incorrect estimation of the errors associated with a measurement or alternatively from distortions of the measurement signal by some unrecognized spurious source. Bayesian data analysis sometimes meets with spectacular success. However, the approach cannot do wonders, but it does result in optimal robust inferences on the basis of all available and explicitly declared information.},
  isbn = {00344885},
  timestamp = {2024-02-02T14:44:00Z},
  file = {Dose - 2003 - Bayesian inference in physics case studies.pdf:/Users/simonmartin/Zotero/storage/U9NQ5J6F/Dose - 2003 - Bayesian inference in physics case studies.pdf:application/pdf}
}

@book{downeyThinkBayes2nd2021,
  title = {Think {{Bayes}}, 2nd {{Edition}}},
  author = {Downey, Allen B.},
  year = {2021},
  publisher = {O'Reilly Media, Inc.},
  isbn = {978-1-4920-8946-9},
  note = {https://greenteapress.com/wp/think-bayes/},
  timestamp = {2024-01-29T12:07:46Z},
  file = {thinkbayes.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/thinkbayes.pdf:application/pdf}
}

@book{downeyThinkBayesBayesian2012,
  title = {Think {{Bayes Bayesian Statistics Made Simple Second Edition}}},
  author = {Downey, Allen B.},
  year = {2012},
  note = {https://greenteapress.com/wp/think-bayes/},
  timestamp = {2024-01-29T11:46:59Z}
}

@article{Dunkley2004a,
  title = {Fast and Reliable {{MCMC}} for Cosmological Parameter Estimation},
  author = {Dunkley, Joanna and Bucher, Martin and Ferreira, Pedro G. and Moodley, Kavilan and Skordis, Constantinos},
  year = {2004},
  volume = {000},
  number = {February},
  eprint = {astro-ph/0405462},
  doi = {10.1111/j.1365-2966.2004.08464.x},
  abstract = {Markov Chain Monte Carlo (MCMC) techniques are now widely used for cosmological parameter estimation. Chains are generated to sample the posterior probability distribution obtained following the Bayesian approach. An important issue is how to optimize the efficiency of such sampling and how to diagnose whether a finite-length chain has adequately sampled the underlying posterior probability distribution. We show how the power spectrum of a single such finite chain may be used as a convergence diagnostic by means of a fitting function, and discuss strategies for optimizing the distribution for the proposed steps. The methods developed are applied to current CMB and LSS data interpreted using both a pure adiabatic cosmological model and a mixed adiabatic/isocurvature cosmological model including possible correlations between modes. For the latter application, because of the increased dimensionality and the presence of degeneracies, the need for tuning MCMC methods for maximum efficiency becomes particularly acute.},
  archiveprefix = {arxiv},
  keywords = {cosmic microwave background,data analysis,methods,statistical},
  note = {http://arxiv.org/abs/astro-ph/0405462\%0Ahttp://dx.doi.org/10.1111/j.1365-2966.2004.08464.x},
  timestamp = {2023-03-29T13:03:05Z},
  file = {Dunkley et al_2004_Fast and reliable MCMC for cosmological parameter estimation.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Dunkley et al_2004_Fast and reliable MCMC for cosmological parameter estimation.pdf:application/pdf}
}

@article{dunstanEasyComputationBayes2022,
  title = {Easy Computation of the {{Bayes}} Factor to Fully Quantify {{Occam}}'s Razor in Least-Squares Fitting and to Guide Actions},
  author = {Dunstan, D. J. and Crowne, J. and Drew, A. J.},
  year = {2022},
  month = jan,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {993},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-04694-7},
  urldate = {2023-12-21},
  abstract = {The Bayes factor is the gold-standard figure of merit for comparing fits of models to data, for hypothesis selection and parameter estimation. However, it is little-used because it has been considered to be subjective, and to be computationally very intensive. A simple computational method has been known for at least 30~years, but has been dismissed as an approximation. We show here that all three criticisms are misplaced. The method should be used to complement and augment all least-squares fitting, because it can give very different, and better outcomes than classical methods. It can discriminate between models with equal numbers of parameters and equally good fits to data. It quantifies the Occam's Razor injunction against over-fitting, and it demands that physically-meaningful parameters rejected by classical significance testing be included in the fitting, to avoid spurious precision and incorrect values for the other parameters. It strongly discourages the use of physically-meaningless parameters, thereby satisfying the Occam's Razor injunction to use existing entities for explanation rather than multiplying new ones. More generally, as a relative probability, the Bayes factor combines naturally with other quantitative information to guide action in the absence of certain knowledge.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Biological techniques,Biophysics,Chemistry,Computational biology and bioinformatics,Ecology,Engineering,Materials science,Medical research,Physics},
  note = {https://www.nature.com/articles/s41598-021-04694-7},
  timestamp = {2023-12-21T10:16:55Z},
  file = {Dunstan et al_2022_Easy computation of the Bayes factor to fully quantify Occam’s razor in.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Dunstan et al_2022_Easy computation of the Bayes factor to fully quantify Occam’s razor in.pdf:application/pdf}
}

@article{dybowskiSinglePassageMouse,
  title = {Single Passage in Mouse Organs Enhances the Survival and Spread of {{Salmonella}} Enterica},
  author = {Dybowski, Richard and Restif, Olivier and Goupy, Alexandre and Maskell, Duncan J. and Mastroeni, Piero and Grant, Andrew J.},
  journal = {Interface},
  volume = {12},
  number = {20150702},
  doi = {10.1098/rsif.2015.0702},
  urldate = {2023-03-28},
  abstract = {Intravenous inoculation of Salmonella enterica serovar Typhimurium into mice is a prime experimental model of invasive salmonellosis. The use of wild-type isogenic tagged strains (WITS) in this system has revealed that bacteria undergo independent bottlenecks in the liver and spleen before establishing a systemic infection. We recently showed that those bacteria that survived the bottleneck exhibited enhanced growth when transferred to naive mice. In this study, we set out to disentangle the components of this in vivo adaptation by inoculating mice with WITS grown either in vitro or in vivo. We developed an original method to estimate the replication and killing rates of bacteria from experimental data, which involved solving the probability-generating function of a non-homogeneous birth death --immigration process. This revealed a low initial mortality in bacteria obtained from a donor animal. Next, an analysis of WITS distributions in the livers and spleens of recipient animals indicated that in vivo-passaged bacteria started spreading between organs earlier than in vitro-grown bacteria. These results further our understanding of the influence of passage in a host on the fitness and virulence of Salmonella enterica and represent an advance in the power of investigation on the patterns and mechanisms of host --pathogen interactions.},
  langid = {english},
  note = {https://royalsocietypublishing.org/doi/epdf/10.1098/rsif.2015.0702},
  timestamp = {2023-03-28T12:27:34Z},
  file = {Dybowski et al_Single passage in mouse organs enhances the survival and spread of Salmonella.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Dybowski et al_Single passage in mouse organs enhances the survival and spread of Salmonella.pdf:application/pdf;Snapshot:/Users/simonmartin/Zotero/storage/LR9B36V6/rsif.2015.html:text/html}
}

@article{ekeSpeedyPixonImage2001,
  title = {A Speedy Pixon Image Reconstruction Algorithm},
  author = {Eke, Vincent},
  year = {2001},
  month = jun,
  journal = {Monthly Notices of the Royal Astronomical Society},
  volume = {324},
  number = {1},
  eprint = {astro-ph/9912078},
  pages = {108--118},
  issn = {0035-8711, 1365-2966},
  doi = {10.1046/j.1365-8711.2001.04253.x},
  urldate = {2023-09-02},
  abstract = {A speedy pixon algorithm for image reconstruction is described. Two applications of the method to simulated astronomical data sets are also reported. In one case, galaxy clusters are extracted from multiwavelength microwave sky maps using the spectral dependence of the Sunyaev-Zel'dovich effect to distinguish them from the microwave background fluctuations and the instrumental noise. The second example involves the recovery of a sharply peaked emission profile, such as might be produced by a galaxy cluster observed in X-rays. These simulations show the ability of the technique both to detect sources in low signal-to-noise data and to deconvolve a telescope beam in order to recover the internal structure of a source.},
  archiveprefix = {arxiv},
  keywords = {Astrophysics},
  note = {http://arxiv.org/abs/astro-ph/9912078},
  timestamp = {2023-09-02T19:40:24Z},
  file = {Eke_2001_A speedy pixon image reconstruction algorithm.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Eke_2001_A speedy pixon image reconstruction algorithm.pdf:application/pdf;arXiv.org Snapshot:/Users/simonmartin/Zotero/storage/3ELDC4C8/9912078.html:text/html}
}

@article{FastObjectiveMultidimensional,
  title = {A Fast and Objective Multidimensional Kernel Density Estimation Method: {{fastKDE}}},
  shorttitle = {A Fast and Objective Multidimensional Kernel Density Estimation Method},
  doi = {10.1016/j.csda.2016.02.014},
  urldate = {2023-03-28},
  abstract = {Numerous facets of scientific research implicitly or explicitly call for the estimation of probability densities. Histograms and kernel density estimates (KDEs) are two commonly used techniques for estimating such information, with the KDE generally providing a higher fidelity representation of the probability density function (PDF). Both methods require specification of either a bin width or a kernel bandwidth. While techniques exist for choosing the kernel bandwidth optimally and objectively, they are computationally intensive, since they require repeated calculation of the KDE. A solution for objectively and optimally choosing both the kernel shape and width has recently been developed by Bernacchia and Pigolotti (2011). While this solution theoretically applies to multidimensional KDEs, it has not been clear how to practically do so. A method for practically extending the Bernacchia--Pigolotti KDE to multidimensions is introduced. This multidimensional extension is combined with a recently-developed computational improvement to their method that makes it computationally efficient: a 2D KDE on 105 samples only takes 1 s on a modern workstation. This fast and objective KDE method, called the fastKDE method, retains the excellent statistical convergence properties that have been demonstrated for univariate samples. The fastKDE method exhibits statistical accuracy that is comparable to state-of-the-science KDE methods publicly available in R, and it produces kernel density estimates several orders of magnitude faster. The fastKDE method does an excellent job of encoding covariance information for bivariate samples. This property allows for direct calculation of conditional PDFs with fastKDE. It is demonstrated how this capability might be leveraged for detecting non-trivial relationships between quantities in physical systems, such as transitional behavior.},
  langid = {english},
  note = {https://reader.elsevier.com/reader/sd/pii/S0167947316300408?token=672114EB33A5B720E85445E021771652EA0B46B3ED9F2C94C855C88C25C58825AE1DD52AE2B9EEE9F11C7AA72D23D20A\&originRegion=eu-west-1\&originCreation=20230328094245},
  timestamp = {2023-04-27T08:40:03Z},
  file = {A fast and objective multidimensional kernel density estimation method.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/A fast and objective multidimensional kernel density estimation method.pdf:application/pdf}
}

@article{feldmannLEOPyEstimatingLikelihoods2019,
  title = {{{LEO-Py}}: {{Estimating}} Likelihoods for Correlated, Censored, and Uncertain Data with given Marginal Distributions},
  shorttitle = {{{LEO-Py}}},
  author = {Feldmann, R.},
  year = {2019},
  month = oct,
  journal = {Astronomy and Computing},
  volume = {29},
  eprint = {1910.02958},
  primaryclass = {astro-ph, stat},
  pages = {100331},
  issn = {22131337},
  doi = {10.1016/j.ascom.2019.100331},
  urldate = {2023-04-03},
  abstract = {Data with uncertain, missing, censored, and correlated values are commonplace in many research fields including astronomy. Unfortunately, such data are often treated in an ad hoc way in the astronomical literature potentially resulting in inconsistent parameter estimates. Furthermore, in a realistic setting, the variables of interest or their errors may have non-normal distributions which complicates the modeling. I present a novel approach to compute the likelihood function for such data sets. This approach employs Gaussian copulas to decouple the correlation structure of variables and their marginal distributions resulting in a flexible method to compute likelihood functions of data in the presence of measurement uncertainty, censoring, and missing data. I demonstrate its use by determining the slope and intrinsic scatter of the star forming sequence of nearby galaxies from observational data. The outlined algorithm is implemented as the flexible, easy-to-use, open-source Python package LEO-Py.},
  archiveprefix = {arxiv},
  keywords = {Astrophysics - Astrophysics of Galaxies,Astrophysics - Instrumentation and Methods for Astrophysics,Statistics - Applications,Statistics - Methodology},
  note = {http://arxiv.org/abs/1910.02958},
  timestamp = {2023-04-03T09:57:48Z},
  file = {arXiv Fulltext PDF:/Users/simonmartin/Zotero/storage/RKYADMU4/Feldmann - 2019 - LEO-Py Estimating likelihoods for correlated, cen.pdf:application/pdf;arXiv.org Snapshot:/Users/simonmartin/Zotero/storage/3N2XXRAB/1910.html:text/html}
}

@article{ferozImportanceNestedSampling2019,
  title = {Importance {{Nested Sampling}} and the {{MultiNest Algorithm}}},
  author = {Feroz, F. and Hobson, M. P. and Cameron, E. and Pettitt, A. N.},
  year = {2019},
  month = nov,
  journal = {The Open Journal of Astrophysics},
  volume = {2},
  number = {1},
  eprint = {1306.2144},
  primaryclass = {astro-ph, physics:physics, stat},
  pages = {10.21105/astro.1306.2144},
  issn = {2565-6120},
  doi = {10.21105/astro.1306.2144},
  urldate = {2023-05-31},
  abstract = {Bayesian inference involves two main computational challenges. First, in estimating the parameters of some model for the data, the posterior distribution may well be highly multi-modal: a regime in which the convergence to stationarity of traditional Markov Chain Monte Carlo (MCMC) techniques becomes incredibly slow. Second, in selecting between a set of competing models the necessary estimation of the Bayesian evidence for each is, by definition, a (possibly high-dimensional) integration over the entire parameter space; again this can be a daunting computational task, although new Monte Carlo (MC) integration algorithms offer solutions of ever increasing efficiency. Nested sampling (NS) is one such contemporary MC strategy targeted at calculation of the Bayesian evidence, but which also enables posterior inference as a by-product, thereby allowing simultaneous parameter estimation and model selection. The widely-used MultiNest algorithm presents a particularly efficient implementation of the NS technique for multi-modal posteriors. In this paper we discuss importance nested sampling (INS), an alternative summation of the MultiNest draws, which can calculate the Bayesian evidence at up to an order of magnitude higher accuracy than `vanilla' NS with no change in the way MultiNest explores the parameter space. This is accomplished by treating as a (pseudo-)importance sample the totality of points collected by MultiNest, including those previously discarded under the constrained likelihood sampling of the NS algorithm. We apply this technique to several challenging test problems and compare the accuracy of Bayesian evidences obtained with INS against those from vanilla NS.},
  archiveprefix = {arxiv},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Physics - Data Analysis Statistics and Probability,Statistics - Computation},
  note = {http://arxiv.org/abs/1306.2144},
  timestamp = {2023-05-31T10:17:02Z},
  file = {Feroz et al_2019_Importance Nested Sampling and the MultiNest Algorithm.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Feroz et al_2019_Importance Nested Sampling and the MultiNest Algorithm.pdf:application/pdf;arXiv.org Snapshot:/Users/simonmartin/Zotero/storage/BRSN927S/1306.html:text/html}
}

@article{fischerAnalysisMixturesPhysical,
  title = {Analysis of {{Mixtures}} in {{Physical Spectra}}},
  author = {Fischer, R and Dose, V},
  keywords = {linear physical models,markov chain monte carlo,mix-,reversible jump,tures},
  timestamp = {2019-08-06T07:21:30Z},
  file = {Fischer_Dose_Analysis of Mixtures in Physical Spectra.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Fischer_Dose_Analysis of Mixtures in Physical Spectra.pdf:application/pdf}
}

@article{fritschImprovedCriteriaClustering2009,
  title = {Improved Criteria for Clustering Based on the Posterior Similarity Matrix},
  author = {Fritsch, Arno and Ickstadt, Katja},
  year = {2009},
  month = jun,
  journal = {Bayesian Analysis},
  volume = {4},
  number = {2},
  pages = {367--391},
  publisher = {International Society for Bayesian Analysis},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/09-BA414},
  urldate = {2024-02-09},
  abstract = {In this paper we address the problem of obtaining a single clustering estimate \${\textbackslash}hat\{c\}\$ based on an MCMC sample of clusterings \$c\^{}\{(1)\},c\^{}\{(2)\}{\textbackslash}ldots,c\^{}\{(M)\}\$ from the posterior distribution of a Bayesian cluster model. Methods to derive \${\textbackslash}hat\{c\}\$ when the number of groups \$K\$ varies between the clusterings are reviewed and discussed. These include the maximum a posteriori (MAP) estimate and methods based on the posterior similarity matrix, a matrix containing the posterior probabilities that the observations \$i\$ and \$j\$ are in the same cluster. The posterior similarity matrix is related to a commonly used loss function by Binder (1978). Minimization of the loss is shown to be equivalent to maximizing the Rand index between estimated and true clustering. We propose new criteria for estimating a clustering, which are based on the posterior expected adjusted Rand index. The criteria are shown to possess a shrinkage property and outperform Binder's loss in a simulation study and in an application to gene expression data. They also perform favorably compared to other clustering procedures.},
  keywords = {Adjusted Rand index,cluster analysis,Dirichlet process mixture model,Markov chain Monte Carlo},
  note = {https://projecteuclid.org/journals/bayesian-analysis/volume-4/issue-2/Improved-criteria-for-clustering-based-on-the-posterior-similarity-matrix/10.1214/09-BA414.full},
  timestamp = {2024-02-09T15:23:33Z},
  file = {Fritsch_Ickstadt_2009_Improved criteria for clustering based on the posterior similarity matrix.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Fritsch_Ickstadt_2009_Improved criteria for clustering based on the posterior similarity matrix.pdf:application/pdf}
}

@article{gelmanAnalysisVarianceWhy2005,
  title = {Analysis of Variance---Why It Is More Important than Ever},
  author = {Gelman, Andrew},
  year = {2005},
  month = feb,
  journal = {The Annals of Statistics},
  volume = {33},
  number = {1},
  pages = {1--53},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/009053604000001048},
  urldate = {2023-04-28},
  abstract = {Analysis of variance (ANOVA) is an extremely important method in exploratory and confirmatory data analysis. Unfortunately, in complex problems (e.g., split-plot designs), it is not always easy to set up an appropriate ANOVA. We propose a hierarchical analysis that automatically gives the correct ANOVA comparisons even in complex scenarios. The inferences for all means and variances are performed under a model with a separate batch of effects for each row of the ANOVA table. We connect to classical ANOVA by working with finite-sample variance components: fixed and random effects models are characterized by inferences about existing levels of a factor and new levels, respectively. We also introduce a new graphical display showing inferences about the standard deviations of each batch of effects. We illustrate with two examples from our applied data analysis, first illustrating the usefulness of our hierarchical computations and displays, and second showing how the ideas of ANOVA are helpful in understanding a previously fit hierarchical model.},
  keywords = {62F15,62J05,62J07,62J10,62J12,ANOVA,Bayesian inference,fixed effects,hierarchical model,Linear regression,multilevel model,random effects,variance components},
  note = {https://projecteuclid.org/journals/annals-of-statistics/volume-33/issue-1/Analysis-of-variancewhy-it-is-more-important-than-ever/10.1214/009053604000001048.full},
  timestamp = {2023-04-28T14:27:15Z},
  file = {Gelman_2005_Analysis of variance—why it is more important than ever.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Gelman_2005_Analysis of variance—why it is more important than ever.pdf:application/pdf}
}

@misc{gelmanBayesianWorkflow2020,
  title = {Bayesian {{Workflow}}},
  author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C. and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and B{\"u}rkner, Paul-Christian and Modr{\'a}k, Martin},
  year = {2020},
  month = nov,
  number = {arXiv:2011.01808},
  eprint = {2011.01808},
  primaryclass = {stat},
  publisher = {arXiv},
  urldate = {2023-05-15},
  abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.},
  archiveprefix = {arxiv},
  howpublished = {http://arxiv.org/abs/2011.01808},
  keywords = {Statistics - Methodology},
  timestamp = {2023-05-15T09:55:54Z},
  file = {Gelman et al_2020_Bayesian Workflow.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Gelman et al_2020_Bayesian Workflow.pdf:application/pdf;arXiv.org Snapshot:/Users/simonmartin/Zotero/storage/YR95TN7D/2011.html:text/html}
}

@inproceedings{giffinUpdatingProbabilitiesData2007a,
  title = {Updating {{Probabilities}} with {{Data}} and {{Moments}}},
  booktitle = {{{AIP Conference Proceedings}}},
  author = {Giffin, Adom and Caticha, Ariel},
  year = {2007},
  volume = {954},
  eprint = {0708.1593},
  primaryclass = {physics, stat},
  pages = {74--84},
  issn = {0094243X},
  doi = {10.1063/1.2821302},
  urldate = {2023-08-21},
  abstract = {We use the method of Maximum (relative) Entropy to process information in the form of observed data and moment constraints. The generic "canonical" form of the posterior distribution for the problem of simultaneous updating with data and moments is obtained. We discuss the general problem of non-commuting constraints, when they should be processed sequentially and when simultaneously. As an illustration, the multinomial example of die tosses is solved in detail for two superficially similar but actually very different problems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Information Theory,Mathematics - Statistics Theory,Physics - Computational Physics,Physics - Data Analysis Statistics and Probability,Physics - Popular Physics,Statistics - Applications,Statistics - Computation,Statistics - Methodology},
  note = {http://arxiv.org/abs/0708.1593},
  timestamp = {2023-08-21T08:17:46Z},
  file = {Giffin_Caticha_2007_Updating Probabilities with Data and Moments.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Giffin_Caticha_2007_Updating Probabilities with Data and Moments2.pdf:application/pdf;arXiv.org Snapshot:/Users/simonmartin/Zotero/storage/FUTLTXX4/0708.html:text/html}
}

@article{gimlinBayesianAcceptanceSampling1972a,
  title = {Bayesian {{Acceptance Sampling}}},
  author = {Gimlin, Darrell R. and Breipohl, Arthur M.},
  year = {1972},
  month = aug,
  journal = {IEEE Transactions on Reliability},
  volume = {R-21},
  number = {3},
  pages = {176--180},
  issn = {1558-1721},
  doi = {10.1109/TR.1972.5215981},
  abstract = {This tutorial paper shows, via an example, how optimum sampling plans can be derived using Bayesian decision theory. Both nonsequential and sequential sampling plans are described. The Bayesian method is contrasted with classical methods. The salient difference between Bayesian and classical methods is that the Bayesian approach allows one formally to pose and answer the question: ``How many should I test?''.},
  keywords = {Bayesian methods,Costs,Decision theory,Manufacturing processes,Materials testing,Probability,Reliability engineering,Reliability theory,Sampling methods},
  timestamp = {2023-05-12T14:25:14Z},
  file = {IEEE Xplore Full Text PDF:/Users/simonmartin/Zotero/storage/QXT9FAX7/Gimlin and Breipohl - 1972 - Bayesian Acceptance Sampling.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/simonmartin/Zotero/storage/RXNBVGM3/stamp.html:text/html}
}

@article{goodmanEnsembleSamplersAffine2010,
  title = {Ensemble Samplers with Affine Invariance},
  author = {Goodman, Jonathan and Weare, Jonathan},
  year = {2010},
  month = jan,
  journal = {Communications in Applied Mathematics and Computational Science},
  volume = {5},
  number = {1},
  pages = {65--80},
  issn = {2157-5452, 1559-3940},
  doi = {10.2140/camcos.2010.5.65},
  urldate = {2023-05-31},
  langid = {english},
  note = {http://msp.org/camcos/2010/5-1/p04.xhtml},
  timestamp = {2023-05-31T10:13:29Z},
  file = {Goodman and Weare - 2010 - Ensemble samplers with affine invariance.pdf:/Users/simonmartin/Zotero/storage/QYMGZAR4/Goodman and Weare - 2010 - Ensemble samplers with affine invariance.pdf:application/pdf}
}

@book{gregory_2005,
  title = {Bayesian Logical Data Analysis for the Physical Sciences: {{A}} Comparative Approach with {{Mathematica}}{\textregistered} Support},
  author = {Gregory, Phil},
  year = {2005},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9780511791277},
  timestamp = {2023-12-16T18:41:15Z}
}

@article{Gregory2011a,
  title = {Bayesian Exoplanet Tests of a New Method for {{MCMC}} Sampling in Highly Correlated Model Parameter Spaces},
  author = {Gregory, Philip C.},
  year = {2011},
  month = jan,
  journal = {Monthly Notices of the Royal Astronomical Society},
  volume = {410},
  number = {1},
  pages = {94--110},
  issn = {00358711},
  doi = {10.1111/j.1365-2966.2010.17428.x},
  urldate = {2014-08-31},
  keywords = {1 i n t,a remarkable array of,as-,Astrometry,astronomi-,methods,Methods: numerical,Methods: statistical,new ground- and space-based,numerical,planetary systems,Planetary systems,radial velocities,ro d u c,statistical,t i o n,techniques,Techniques: radial velocities,trometry},
  note = {http://mnras.oxfordjournals.org/cgi/doi/10.1111/j.1365-2966.2010.17428.x},
  timestamp = {2023-03-29T12:58:39Z},
  file = {Gregory_2011_Bayesian exoplanet tests of a new method for MCMC sampling in highly correlated.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Gregory_2011_Bayesian exoplanet tests of a new method for MCMC sampling in highly correlated.pdf:application/pdf}
}

@incollection{gullBayesianInductiveInference1988,
  title = {Bayesian {{Inductive Inference}} and {{Maximum Entropy}}},
  booktitle = {Maximum-{{Entropy}} and {{Bayesian Methods}} in {{Science}} and {{Engineering}}},
  author = {Gull, Stephen F.},
  editor = {Erickson, Gary J. and Smith, C. Ray},
  year = {1988},
  pages = {53--74},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-009-3049-0_4},
  urldate = {2023-12-21},
  isbn = {978-94-010-7871-9 978-94-009-3049-0},
  langid = {english},
  note = {http://link.springer.com/10.1007/978-94-009-3049-0\_4},
  timestamp = {2023-12-21T10:35:04Z},
  file = {why.pdf:/Users/simonmartin/Zotero/storage/24M4XIQR/why.pdf:application/pdf}
}

@article{gunapatiVariationalInferenceAlternative2022,
  title = {Variational {{Inference}} as an Alternative to {{MCMC}} for Parameter Estimation and Model Selection},
  author = {Gunapati, Geetakrishnasai and Jain, Anirudh and Srijith, P. K. and Desai, Shantanu},
  year = {2022},
  journal = {Publications of the Astronomical Society of Australia},
  volume = {39},
  eprint = {1803.06473},
  primaryclass = {astro-ph, physics:hep-ex, physics:physics},
  pages = {e001},
  issn = {1323-3580, 1448-6083},
  doi = {10.1017/pasa.2021.64},
  urldate = {2024-01-29},
  abstract = {Most applications of Bayesian Inference for parameter estimation and model selection in astrophysics involve the use of Monte Carlo techniques such as Markov Chain Monte Carlo (MCMC) and nested sampling. However, these techniques are time consuming and their convergence to the posterior could be difficult to determine. In this work, we advocate Variational inference as an alternative to solve the above problems, and demonstrate its usefulness for parameter estimation and model selection in Astrophysics. Variational inference converts the inference problem into an optimization problem by approximating the posterior from a known family of distributions and using Kullback-Leibler divergence to characterize the difference. It takes advantage of fast optimization techniques, which make it ideal to deal with large datasets and makes it trivial to parallelize on a multicore platform. We also derive a new approximate evidence estimation based on variational posterior, and importance sampling technique called posterior weighted importance sampling for the calculation of evidence (PWISE), which is useful to perform Bayesian model selection. As a proof of principle, we apply variational inference to five different problems in astrophysics, where Monte Carlo techniques were previously used. These include assessment of significance of annual modulation in the COSINE-100 dark matter experiment, measuring exoplanet orbital parameters from radial velocity data, tests of periodicities in measurements of Newton's constant \$G\$, assessing the significance of a turnover in the spectral lag data of GRB 160625B and estimating the mass of a galaxy cluster using weak gravitational lensing. We find that variational inference is much faster than MCMC and nested sampling techniques for most of these problems while providing competitive results. All our analysis codes have been made publicly available.},
  archiveprefix = {arxiv},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,High Energy Physics - Experiment,Physics - Data Analysis Statistics and Probability},
  note = {http://arxiv.org/abs/1803.06473},
  timestamp = {2024-01-29T12:59:59Z},
  file = {Gunapati et al_2022_Variational Inference as an alternative to MCMC for parameter estimation and.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Gunapati et al_2022_Variational Inference as an alternative to MCMC for parameter estimation and.pdf:application/pdf;arXiv.org Snapshot:/Users/simonmartin/Zotero/storage/AHZH4DA9/1803.html:text/html}
}

@article{habeckNestedSamplingDemons2015a,
  title = {Nested Sampling with Demons},
  author = {Habeck, Michael},
  year = {2015},
  month = jan,
  journal = {AIP Conference Proceedings},
  volume = {1641},
  number = {1},
  pages = {121--129},
  issn = {0094-243X},
  doi = {10.1063/1.4905971},
  urldate = {2023-08-08},
  abstract = {This article looks at Skilling's nested sampling from a physical perspective and interprets it as a microcanonical demon algorithm. Using key quantities of statistical physics we investigate the performance of nested sampling on complex systems such as Ising, Potts and protein models. We show that releasing multiple demons helps to smooth the truncated prior and eases sampling from it because the demons keep the particle off the constraint boundary. For continuous systems it is straightforward to extend this approach and formulate a phase space version of nested sampling that benefits from correlated explorations guided by Hamiltonian dynamics.},
  note = {https://doi.org/10.1063/1.4905971},
  timestamp = {2023-08-08T09:12:03Z},
  file = {Habeck_2015_Nested sampling with demons.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Habeck_2015_Nested sampling with demons.pdf:application/pdf;Snapshot:/Users/simonmartin/Zotero/storage/HVAXU9CZ/Nested-sampling-with-demons.html:text/html}
}

@article{handleyPolyChordNestedSampling2015,
  title = {{{PolyChord}}: Nested Sampling for Cosmology},
  shorttitle = {{{PolyChord}}},
  author = {Handley, W. J. and Hobson, M. P. and Lasenby, A. N.},
  year = {2015},
  month = jun,
  journal = {Monthly Notices of the Royal Astronomical Society: Letters},
  volume = {450},
  number = {1},
  eprint = {1502.01856},
  primaryclass = {astro-ph},
  pages = {L61-L65},
  issn = {1745-3933, 1745-3925},
  doi = {10.1093/mnrasl/slv047},
  urldate = {2024-02-05},
  abstract = {PolyChord is a novel nested sampling algorithm tailored for high dimensional parameter spaces. In addition, it can fully exploit a hierarchy of parameter speeds such as is found in CosmoMC and CAMB. It utilises slice sampling at each iteration to sample within the hard likelihood constraint of nested sampling. It can identify and evolve separate modes of a posterior semi-independently and is parallelised using openMPI. PolyChord is available for download at: http://ccpforge.cse.rl.ac.uk/gf/project/polychord/},
  archiveprefix = {arxiv},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics,Astrophysics - Instrumentation and Methods for Astrophysics},
  note = {http://arxiv.org/abs/1502.01856},
  timestamp = {2024-02-05T08:41:11Z},
  file = {Handley et al_2015_PolyChord.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Handley et al_2015_PolyChord2.pdf:application/pdf;arXiv.org Snapshot:/Users/simonmartin/Zotero/storage/C4373XEH/1502.html:text/html}
}

@article{handleyPolyChordNextgenerationNested2015,
  title = {{{PolyChord}}: Next-Generation Nested Sampling},
  author = {Handley, W. J. and Hobson, M. P. and Lasenby, A. N.},
  year = {2015},
  month = may,
  doi = {10.1093/mnras/stv1911},
  abstract = {PolyChord is a novel nested sampling algorithm tailored for high-dimensional parameter spaces. This paper coincides with the release of PolyChord v1.3, and provides an extensive account of the algorithm. PolyChord utilises slice sampling at each iteration to sample within the hard likelihood constraint of nested sampling. It can identify and evolve separate modes of a posterior semi-independently, and is parallelised using openMPI. It is capable of exploiting a hierarchy of parameter speeds such as those present in CosmoMC and CAMB, and is now in use in the CosmoChord and ModeChord codes. PolyChord is available for download at: http://ccpforge.cse.rl.ac.uk/gf/project/polychord/},
  note = {http://arxiv.org/abs/1506.00171 http://dx.doi.org/10.1093/mnras/stv1911},
  timestamp = {2023-03-24T12:15:05Z},
  file = {Handley et al_2015_PolyChord.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Handley et al_2015_PolyChord.pdf:application/pdf}
}

@article{hansenBayesianEstimationHyperparameters2000,
  title = {Bayesian Estimation of Hyperparameters for Indirect {{Fourier}} Transformation in Small-Angle Scattering},
  author = {Hansen, Steen and Bayesian, {\textasciimacron}},
  year = {2000},
  journal = {J. Appl. Cryst},
  issn = {0021-8898},
  abstract = {Bayesian analysis is applied to the problem of estimation of hyperparameters, which are necessary for indirect Fourier transformation of small-angle scattering data. The hyperparameters most frequently needed are the overall noise level of the experiment and the maximum dimension of the scatterer. Bayesian methods allow the posterior probability distribution for the hyperparameters to be determined, making it possible to calculate the distance distribution function of interest as the weighted mean of all possible solutions to the indirect transformation problem. Consequently no choice of hyperparameters has to be made. The applicability of the method is demonstrated using simulated as well as real experimental data.},
  timestamp = {2024-02-02T14:47:06Z},
  file = {Hansen_2000_Bayesian estimation of hyperparameters for indirect Fourier transformation in.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Hansen_2000_Bayesian estimation of hyperparameters for indirect Fourier transformation in.pdf:application/pdf}
}

@misc{heavensMarginalLikelihoodsMonte2017,
  title = {Marginal {{Likelihoods}} from {{Monte Carlo Markov Chains}}},
  author = {Heavens, Alan and Fantaye, Yabebal and Mootoovaloo, Arrykrishna and Eggers, Hans and Hosenie, Zafiirah and Kroon, Steve and Sellentin, Elena},
  year = {2017},
  month = apr,
  number = {arXiv:1704.03472},
  eprint = {1704.03472},
  primaryclass = {astro-ph, stat},
  publisher = {arXiv},
  urldate = {2023-03-31},
  abstract = {In this paper, we present a method for computing the marginal likelihood, also known as the model likelihood or Bayesian evidence, from Markov Chain Monte Carlo (MCMC), or other sampled posterior distributions. In order to do this, one needs to be able to estimate the density of points in parameter space, and this can be challenging in high numbers of dimensions. Here we present a Bayesian analysis, where we obtain the posterior for the marginal likelihood, using \$k\$th nearest-neighbour distances in parameter space, using the Mahalanobis distance metric, under the assumption that the points in the chain (thinned if required) are independent. We generalise the algorithm to apply to importance-sampled chains, where each point is assigned a weight. We illustrate this with an idealised posterior of known form with an analytic marginal likelihood, and show that for chains of length \${\textbackslash}sim 10\^{}5\$ points, the technique is effective for parameter spaces with up to \${\textbackslash}sim 20\$ dimensions. We also argue that \$k=1\$ is the optimal choice, and discuss failure modes for the algorithm. In a companion paper (Heavens et al. 2017) we apply the technique to the main MCMC chains from the 2015 Planck analysis of cosmic background radiation data, to infer that quantitatively the simplest 6-parameter flat \${\textbackslash}Lambda\$CDM standard model of cosmology is preferred over all extensions considered.},
  archiveprefix = {arxiv},
  howpublished = {http://arxiv.org/abs/1704.03472},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics,Statistics - Computation},
  timestamp = {2023-03-31T14:52:08Z},
  file = {Heavens et al_2017_Marginal Likelihoods from Monte Carlo Markov Chains.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Heavens et al_2017_Marginal Likelihoods from Monte Carlo Markov Chains.pdf:application/pdf;arXiv.org Snapshot:/Users/simonmartin/Zotero/storage/RMVUQC7C/1704.html:text/html}
}

@article{higsonDynamicNestedSampling2019,
  title = {Dynamic Nested Sampling: An Improved Algorithm for Parameter Estimation and Evidence Calculation},
  shorttitle = {Dynamic Nested Sampling},
  author = {Higson, Edward and Handley, Will and Hobson, Michael and Lasenby, Anthony},
  year = {2019},
  month = sep,
  journal = {Statistics and Computing},
  volume = {29},
  number = {5},
  pages = {891--913},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-018-9844-0},
  urldate = {2023-03-28},
  abstract = {We introduce dynamic nested sampling: a generalisation of the nested sampling algorithm in which the number of ``live points'' varies to allocate samples more efficiently. In empirical tests the new method significantly improves calculation accuracy compared to standard nested sampling with the same number of samples; this increase in accuracy is equivalent to speeding up the computation by factors of up to {$\sim$} 72 for parameter estimation and {$\sim$} 7 for evidence calculations. We also show that the accuracy of both parameter estimation and evidence calculations can be improved simultaneously. In addition, unlike in standard nested sampling, more accurate results can be obtained by continuing the calculation for longer. Popular standard nested sampling implementations can be easily adapted to perform dynamic nested sampling, and several dynamic nested sampling software packages are now publicly available.},
  langid = {english},
  note = {http://link.springer.com/10.1007/s11222-018-9844-0},
  timestamp = {2023-03-28T11:23:31Z},
  file = {Higson et al_2019_Dynamic nested sampling.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Higson et al_2019_Dynamic nested sampling.pdf:application/pdf}
}

@article{higsonSamplingErrorsNested2018,
  title = {Sampling {{Errors}} in {{Nested Sampling Parameter Estimation}}},
  author = {Higson, Edward and Handley, Will and Hobson, Mike and Lasenby, Anthony},
  year = {2018},
  month = sep,
  journal = {Bayesian Analysis},
  volume = {13},
  number = {3},
  issn = {1936-0975},
  doi = {10.1214/17-BA1075},
  urldate = {2023-03-31},
  abstract = {Sampling errors in nested sampling parameter estimation differ from those in Bayesian evidence calculation, but have been little studied in the literature. This paper provides the first explanation of the two main sources of sampling errors in nested sampling parameter estimation, and presents a new diagrammatic representation for the process. We find no current method can accurately measure the parameter estimation errors of a single nested sampling run, and propose a method for doing so using a new algorithm for dividing nested sampling runs. We empirically verify our conclusions and the accuracy of our new method.},
  langid = {english},
  note = {https://projecteuclid.org/journals/bayesian-analysis/volume-13/issue-3/Sampling-Errors-in-Nested-Sampling-Parameter-Estimation/10.1214/17-BA1075.full},
  timestamp = {2023-05-30T16:15:49Z},
  file = {Higson et al_2018_Sampling Errors in Nested Sampling Parameter Estimation.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Higson et al_2018_Sampling Errors in Nested Sampling Parameter Estimation.pdf:application/pdf;Higson et al. - 2018 - Sampling Errors in Nested Sampling Parameter Estim.pdf:/Users/simonmartin/Zotero/storage/T7848Z2E/Higson et al. - 2018 - Sampling Errors in Nested Sampling Parameter Estim.pdf:application/pdf}
}

@book{hilbe_de_souza_ishida_2017,
  title = {Bayesian Models for Astrophysical Data: {{Using R}}, {{JAGS}}, Python, and Stan},
  author = {Hilbe, Joseph M. and {de Souza}, Rafael S. and Ishida, Emille E. O.},
  year = {2017},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9781316459515},
  timestamp = {2023-12-16T18:47:25Z}
}

@article{hoggDataAnalysisRecipes2018,
  title = {Data Analysis Recipes: {{Using Markov Chain Monte Carlo}}},
  shorttitle = {Data Analysis Recipes},
  author = {Hogg, David W. and {Foreman-Mackey}, Daniel},
  year = {2018},
  month = may,
  journal = {The Astrophysical Journal Supplement Series},
  volume = {236},
  number = {1},
  eprint = {1710.06068},
  primaryclass = {astro-ph, physics:physics, stat},
  pages = {11},
  issn = {1538-4365},
  doi = {10.3847/1538-4365/aab76e},
  urldate = {2023-12-15},
  abstract = {Markov Chain Monte Carlo (MCMC) methods for sampling probability density functions (combined with abundant computational resources) have transformed the sciences, especially in performing probabilistic inferences, or fitting models to data. In this primarily pedagogical contribution, we give a brief overview of the most basic MCMC method and some practical advice for the use of MCMC in real inference problems. We give advice on method choice, tuning for performance, methods for initialization, tests of convergence, troubleshooting, and use of the chain output to produce or report parameter estimates with associated uncertainties. We argue that autocorrelation time is the most important test for convergence, as it directly connects to the uncertainty on the sampling estimate of any quantity of interest. We emphasize that sampling is a method for doing integrals; this guides our thinking about how MCMC output is best used.},
  archiveprefix = {arxiv},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Physics - Data Analysis Statistics and Probability,Statistics - Computation},
  note = {http://arxiv.org/abs/1710.06068},
  timestamp = {2023-12-15T13:38:26Z},
  file = {Hogg_Foreman-Mackey_2018_Data analysis recipes.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Hogg_Foreman-Mackey_2018_Data analysis recipes.pdf:application/pdf;arXiv.org Snapshot:/Users/simonmartin/Zotero/storage/56MTSHTQ/1710.html:text/html}
}

@misc{Home,
  title = {Home},
  journal = {PyMC project website},
  urldate = {2024-01-29},
  abstract = {PyMC is a probabilistic programming library for Python that allows users to build Bayesian models with a simple Python API and fit them using Markov chain Monte Carlo (MCMC) methods. Features: PyMC...},
  howpublished = {https://www.pymc.io/welcome.html},
  langid = {english},
  timestamp = {2024-01-29T13:51:23Z},
  file = {Snapshot:/Users/simonmartin/Zotero/storage/7BUF6LXC/welcome.html:text/html}
}

@misc{jaynesProbabilityTheoryLogic,
  title = {Probability {{Theory}}: {{The Logic}} of {{Science}}},
  author = {Jaynes, E. T.},
  urldate = {2023-06-23},
  howpublished = {https://bayes.wustl.edu/etj/prob/book.pdf},
  timestamp = {2023-08-21T14:39:41Z},
  file = {book.pdf:/Users/simonmartin/Zotero/storage/QZ7T45DY/book.pdf:application/pdf}
}

@incollection{jaynesRELATIONBAYESIANMAXIMUM,
  title = {{{THE RELATION OF BAYESIAN AND MAXIMUM ENTROPY METHODS}}},
  booktitle = {Maximum-{{Entropy}} and {{Bayesian Methods}} in {{Science andEngineering}}},
  author = {Jaynes, E. T.},
  volume = {1},
  pages = {25--29},
  publisher = {Kluwer Academic Publishers},
  urldate = {2023-08-21},
  abstract = {Further progress in scientific inference must, in our view, come from some kind of unification of our present principles. As a prerequisite for this, we note briefly the great conceptual differences, and the equally great mathematical similarities, of Bayesian and Maximum Entropy methods.},
  note = {https://bayes.wustl.edu/etj/articles/relationship.pdf},
  timestamp = {2023-08-21T14:43:34Z},
  file = {relationship.pdf:/Users/simonmartin/Zotero/storage/MX3S2493/relationship.pdf:application/pdf}
}

@article{jefferysSharpeningOccamRazor1991,
  title = {Sharpening {{Occam}}'s {{Razor}} on a {{Bayesian}} Strop.},
  author = {Jefferys, W. {\textasciitilde}H and Berger, {\textasciitilde}O, J},
  year = {1991},
  journal = {Bulletin of the American Astronomical Society},
  volume = {23},
  pages = {1259},
  urldate = {2023-12-21},
  note = {https://home.cs.colorado.edu/{\textasciitilde}mozer/Teaching/syllabi/6622/papers/JefferysBerger1991.pdf},
  timestamp = {2023-12-21T10:15:20Z},
  file = {JefferysBerger1991.pdf:/Users/simonmartin/Zotero/storage/HTB4HDPZ/JefferysBerger1991.pdf:application/pdf}
}

@misc{Js850Nested_samplingPython,
  title = {Js850/Nested\_sampling: Python Based Implementation of the Nested Sampling Algorithm},
  shorttitle = {Js850/Nested\_sampling},
  urldate = {2024-02-09},
  howpublished = {https://github.com/js850/nested\_sampling},
  langid = {english},
  timestamp = {2024-02-09T15:42:35Z},
  file = {Snapshot:/Users/simonmartin/Zotero/storage/JTDAI9BT/nested_sampling.html:text/html}
}

@article{kellyAspectsMeasurementError2007,
  title = {Some {{Aspects}} of {{Measurement Error}} in {{Linear Regression}} of {{Astronomical Data}}},
  author = {Kelly, Brandon C.},
  year = {2007},
  month = aug,
  journal = {The Astrophysical Journal},
  volume = {665},
  number = {2},
  eprint = {0705.2774},
  primaryclass = {astro-ph},
  pages = {1489--1506},
  issn = {0004-637X, 1538-4357},
  doi = {10.1086/519947},
  urldate = {2023-05-15},
  abstract = {I describe a Bayesian method to account for measurement errors in linear regression of astronomical data. The method allows for heteroscedastic and possibly correlated measurement errors, and intrinsic scatter in the regression relationship. The method is based on deriving a likelihood function for the measured data, and I focus on the case when the intrinsic distribution of the independent variables can be approximated using a mixture of Gaussians. I generalize the method to incorporate multiple independent variables, non-detections, and selection effects (e.g., Malmquist bias). A Gibbs sampler is described for simulating random draws from the probability distribution of the parameters, given the observed data. I use simulation to compare the method with other common estimators. The simulations illustrate that the Gaussian mixture model outperforms other common estimators and can effectively give constraints on the regression parameters, even when the measurement errors dominate the observed scatter, source detection fraction is low, or the intrinsic distribution of the independent variables is not a mixture of Gaussians. I conclude by using this method to fit the X-ray spectral slope as a function of Eddington ratio using a sample of 39 z {$<$} 0.8 radio-quiet quasars. I confirm the correlation seen by other authors between the radio-quiet quasar X-ray spectral slope and the Eddington ratio, where the X-ray spectral slope softens as the Eddington ratio increases.},
  archiveprefix = {arxiv},
  keywords = {Astrophysics},
  note = {http://arxiv.org/abs/0705.2774},
  timestamp = {2023-05-15T10:03:47Z},
  file = {Kelly_2007_Some Aspects of Measurement Error in Linear Regression of Astronomical Data.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Kelly_2007_Some Aspects of Measurement Error in Linear Regression of Astronomical Data.pdf:application/pdf;arXiv.org Snapshot:/Users/simonmartin/Zotero/storage/DPKCK8XX/0705.html:text/html}
}

@article{kruschkeBayesianAnalysisReporting2021,
  title = {Bayesian {{Analysis Reporting Guidelines}}},
  author = {Kruschke, John K.},
  year = {2021},
  month = aug,
  journal = {Nature Human Behaviour},
  volume = {5},
  number = {10},
  pages = {1282--1291},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01177-7},
  urldate = {2023-04-24},
  abstract = {Abstract             Previous surveys of the literature have shown that reports of statistical analyses often lack important information, causing lack of transparency and failure of reproducibility. Editors and authors agree that guidelines for reporting should be encouraged. This Review presents a set of Bayesian analysis reporting guidelines (BARG). The BARG encompass the features of previous guidelines, while including many additional details for contemporary Bayesian analyses, with explanations. An extensive example of applying the BARG is presented. The BARG should be useful to researchers, authors, reviewers, editors, educators and students. Utilization, endorsement and promotion of the BARG may improve the quality, transparency and reproducibility of Bayesian analyses.},
  langid = {english},
  note = {https://www.nature.com/articles/s41562-021-01177-7},
  timestamp = {2023-04-24T14:27:15Z},
  file = {Kruschke - 2021 - Bayesian Analysis Reporting Guidelines.pdf:/Users/simonmartin/Zotero/storage/XML93ZQI/Kruschke - 2021 - Bayesian Analysis Reporting Guidelines.pdf:application/pdf}
}

@article{kruschkeBayesianEstimationSupersedes,
  title = {Bayesian {{Estimation Supersedes}} the t {{Test}}},
  author = {Kruschke, John},
  timestamp = {2024-02-02T14:45:31Z},
  file = {Kruschke_Bayesian Estimation Supersedes the t Test.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Kruschke_Bayesian Estimation Supersedes the t Test.pdf:application/pdf;Kruschke - 2013 - Bayesian estimation supersedes the t test..pdf:/Users/simonmartin/Zotero/storage/KWVMDR55/Kruschke - 2013 - Bayesian estimation supersedes the t test..pdf:application/pdf}
}

@article{kubschPvaluesUsingBayesian2021,
  title = {Beyond P-Values: {{Using Bayesian Data Analysis}} in {{Science Education Research}}},
  shorttitle = {Beyond P-Values},
  author = {Kubsch, Marcus and Stamer, Insa and Steiner, Mara and Neumann, Knut and Parchmann, Ilka},
  year = {2021},
  publisher = {University of Massachusetts Amherst},
  doi = {10.7275/VZPW-NG13},
  urldate = {2023-05-05},
  abstract = {In light of the replication crisis in psychology, null-hypothesis significance testing (NHST) and p-values have been heavily criticized and various alternatives have been proposed, ranging from slight modifications of the current paradigm to banning p-values from journals. Since the physics education research community often relies on quantitative statistical approaches, the challenges the replication crisis poses to these approaches need to be considered. p-values suffer primarily from the fact that they carry little information by themselves and lend themselves to misinterpretations. As one alternative, Bayesian approaches have become increasingly popular as the posterior distributions they provide carry more relevant information than p-values. In this paper, we discuss practical issues related to p-values with respect to interpreting and communicating results and how these issues can be addressed using a Bayesian approach. Drawing on a science education data set, we demonstrate how Bayesian data analysis methods go beyond p-values and can help to make more valid conclusions and to communicate them more easily in a manner that lends itself to less misinterpretations.},
  langid = {english},
  note = {https://scholarworks.umass.edu/pare/vol26/iss1/4/},
  timestamp = {2023-05-05T07:34:35Z},
  file = {Kubsch et al_2021_Beyond p-values.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Kubsch et al_2021_Beyond p-values.pdf:application/pdf}
}

@misc{lambertDistributionZoo,
  title = {The Distribution Zoo},
  author = {Lambert, Ben},
  urldate = {2024-01-17},
  howpublished = {https://ben18785.shinyapps.io/distribution-zoo/},
  timestamp = {2024-01-17T10:08:35Z},
  file = {The distribution zoo:/Users/simonmartin/Zotero/storage/UAQ4WCUB/distribution-zoo.html:text/html}
}

@book{lambertStudentGuideBayesian2018,
  title = {A Student's Guide to {{Bayesian}} Statistics},
  author = {Lambert, Ben},
  year = {2018},
  publisher = {SAGE},
  address = {Los Angeles London New Delhi Singapore Washington DC Melbourne},
  isbn = {978-1-4739-1635-7 978-1-4739-1636-4},
  langid = {english},
  timestamp = {2023-11-18T09:55:08Z},
  file = {Lambert - 2018 - A student's guide to Bayesian statistics.pdf:/Users/simonmartin/Zotero/storage/KKEF9GMJ/Lambert - 2018 - A student's guide to Bayesian statistics.pdf:application/pdf}
}

@article{lauBayesianModelBasedClustering2007,
  title = {Bayesian {{Model-Based Clustering Procedures}}},
  author = {Lau, John W. and Green, Peter J.},
  year = {2007},
  journal = {Journal of Computational and Graphical Statistics},
  volume = {16},
  number = {3},
  eprint = {27594259},
  eprinttype = {jstor},
  pages = {526--558},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
  issn = {1061-8600},
  doi = {10.1198/106186007X238855},
  urldate = {2024-02-09},
  abstract = {This article establishes a general formulation for Bayesian model-based clustering, in which subset labels are exchangeable, and items are also exchangeable, possibly up to covariate effects. The notational framework is rich enough to encompass a variety of existing procedures, including some recently discussed methods involving stochastic search or hierarchical clustering, but more importantly allows the formulation of clustering procedures that are optimal with respect to a specified loss function. Our focus is on loss functions based on pairwise coincidences, that is, whether pairs of items are clustered into the same subset or not. Optimization of the posterior expected loss function can be formulated as a binary integer programming problem, which can be readily solved by standard software when clustering a modest number of items, but quickly becomes impractical as problem scale increases. To combat this, a new heuristic item-swapping algorithm is introduced. This performs well in our numerical experiments, on both simulated and real data examples. The article includes a comparison of the statistical performance of the (approximate) optimal clustering with earlier methods that are model-based but ad hoc in their detailed definition.},
  note = {https://www.jstor.org/stable/27594259},
  timestamp = {2024-02-09T15:21:30Z},
  file = {Lau_Green_2007_Bayesian Model-Based Clustering Procedures.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Lau_Green_2007_Bayesian Model-Based Clustering Procedures.pdf:application/pdf}
}

@article{lemoineMovingNoninformativePriors2019b,
  title = {Moving beyond Noninformative Priors: Why and How to Choose Weakly Informative Priors in {{Bayesian}} Analyses},
  shorttitle = {Moving beyond Noninformative Priors},
  author = {Lemoine, Nathan P.},
  year = {2019},
  month = jul,
  journal = {Oikos},
  volume = {128},
  number = {7},
  pages = {912--928},
  issn = {0030-1299, 1600-0706},
  doi = {10.1111/oik.05985},
  urldate = {2023-12-14},
  abstract = {Throughout the last two decades, Bayesian statistical methods have proliferated throughout ecology and evolution. Numerous previous references established both philosophical and computational guidelines for implementing Bayesian methods. However, protocols for incorporating prior information, the defining characteristic of Bayesian philosophy, are nearly nonexistent in the ecological literature. Here, I hope to encourage the use of weakly informative priors in ecology and evolution by providing a `consumer's guide' to weakly informative priors. The first section outlines three reasons why ecologists should abandon noninformative priors: 1) common flat priors are not always noninformative, 2) noninformative priors provide the same result as simpler frequentist methods, and 3) noninformative priors suffer from the same high type I and type M error rates as frequentist methods. The second section provides a guide for implementing informative priors, wherein I detail convenient `reference' prior distributions for common statistical models (i.e. regression, ANOVA, hierarchical models). I then use simulations to visually demonstrate how informative priors influence posterior parameter estimates. With the guidelines provided here, I hope to encourage the use of weakly informative priors for Bayesian analyses in ecology. Ecologists can and should debate the appropriate form of prior information, but should consider weakly informative priors as the new `default' prior for any Bayesian model.},
  langid = {english},
  note = {https://onlinelibrary.wiley.com/doi/10.1111/oik.05985},
  timestamp = {2023-12-14T09:17:00Z},
  file = {Lemoine - 2019 - Moving beyond noninformative priors why and how t.pdf:/Users/simonmartin/Zotero/storage/6ZILRB3Q/Lemoine - 2019 - Moving beyond noninformative priors why and how t.pdf:application/pdf}
}

@article{leonardPersonalHistoryBayesian2014,
  title = {A Personal History of {{Bayesian}} Statistics},
  author = {Leonard, Thomas Hoskyns},
  year = {2014},
  journal = {WIREs Computational Statistics},
  volume = {6},
  number = {2},
  pages = {80--115},
  issn = {1939-0068},
  doi = {10.1002/wics.1293},
  urldate = {2023-12-21},
  abstract = {The history of Bayesian statistics is traced, from a personal perspective, through various strands and via its re-genesis during the 1960s to the current day. Emphasis is placed on broad-sense Bayesian methodology that can be used to meaningfully analyze observed datasets. Over 750 people in science, medicine, and socioeconomics, who have influenced the evolution of the Bayesian approach into the powerful paradigm that it is today, are highlighted. The frequentist/Bayesian controversy is addressed, together with the ways in which many Bayesians combine the two ideologies as a Bayes/non-Bayes compromise, e.g., when drawing inferences about unknown parameters or when investigating the choice of sampling model in relation to its real-life background. A number of fundamental issues are discussed and critically examined, and some elementary explanations for nontechnical readers and some personal reminiscences are included. Some of the Bayesian contributions of the 21st century are subjected to more detailed critique, so that readers may learn more about the quality and relevance of the ongoing research. A recent resolution of Lindley's paradox by Baskurt and Evans is reported. The axioms of subjective probability are reassessed, some state-of-the-art alternatives to Leonard Savage's axioms of utility are discussed, and Deborah Mayo and Michael Evan's refutation of Allan Birnbaum's 1962 justification of the likelihood principle in terms of the sufficiency and conditionality principles is addressed. WIREs Comput Stat 2014, 6:80--115. doi: 10.1002/wics.1293 This article is categorized under: Statistical and Graphical Methods of Data Analysis {$>$} Bayesian Methods and Theory},
  langid = {english},
  keywords = {Adams rape case,AIC,Alfred Dreyfus case,Allais paradox,axioms of subjective probability,Axioms of utility,Baskurt-Evans,Bayes factor,Bayes/non-Bayes compromise,Bayesian analysis of categorical data,Bayesian Econometrics,Bayesian Genomics,Bayesian inference,Bayesian methodology,Bayesian model choice,Bayesian semiparametric methods,BIC,Birnbaum's proof,conditionality principle,conjugate analysis,cross-validation,Daniel Bernoulli,De Finetti axioms,decision theory,Dennis Lindley,DIC,Dirichlet prior,DNA evidence,empirical Bayesian,Essen-Moller formula,expected utility hypothesis,frequentist/Bayesian controversy,hierarchical Bayesian,history of Bayesian statistics,importance sampling,improper prior,inverse probability,inverted Wishart prior,Jeffreys prior,Kalman filter,Laplacian approximation,legal statistics,Leonard Savage,likelihood principle,Lindley's paradox,M-group regression,Marginalization paradox,MCMC,measure of evidence,Metropolis-Hastings algorithm,Michael Evans,multiple time series,Nurse Atkinson case,O.J. Simpson case,Occam's razor,Pierre-Simon de Laplace,random effects models,reference prior,Richard Price,roughness penalty,Sally Clark case,Savage axioms,smoothing splines,spatial processes,statistical model checking and comparison,statistical probability,stochastic volatility,subjective probability,sufficiency principle,the Rosie and the ten construction workers case,Thomas Bayes},
  note = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.1293},
  timestamp = {2023-12-21T10:38:32Z},
  file = {Leonard_2014_A personal history of Bayesian statistics.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Leonard_2014_A personal history of Bayesian statistics.pdf:application/pdf;Snapshot:/Users/simonmartin/Zotero/storage/HGDBMFTS/wics.html:text/html}
}

@article{lewisEfficientSamplingFast2013,
  title = {Efficient Sampling of Fast and Slow Cosmological Parameters},
  author = {Lewis, Antony},
  year = {2013},
  month = may,
  journal = {Physical Review D},
  volume = {87},
  number = {10},
  pages = {103529},
  issn = {1550-7998, 1550-2368},
  doi = {10.1103/PhysRevD.87.103529},
  urldate = {2023-03-28},
  langid = {english},
  note = {https://link.aps.org/doi/10.1103/PhysRevD.87.103529},
  timestamp = {2023-03-28T08:51:25Z},
  file = {Lewis_2013_Efficient sampling of fast and slow cosmological parameters.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Lewis_2013_Efficient sampling of fast and slow cosmological parameters.pdf:application/pdf}
}

@article{llorenteMarginalLikelihoodComputation2023,
  title = {Marginal Likelihood Computation for Model Selection and Hypothesis Testing: An Extensive Review},
  shorttitle = {Marginal Likelihood Computation for Model Selection and Hypothesis Testing},
  author = {Llorente, Fernando and Martino, Luca and Delgado, David and {Lopez-Santiago}, Javier},
  year = {2023},
  month = feb,
  journal = {SIAM Review},
  volume = {65},
  number = {1},
  eprint = {2005.08334},
  primaryclass = {cs, stat},
  pages = {3--58},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/20M1310849},
  urldate = {2023-05-04},
  abstract = {This is an up-to-date introduction to, and overview of, marginal likelihood computation for model selection and hypothesis testing. Computing normalizing constants of probability models (or ratio of constants) is a fundamental issue in many applications in statistics, applied mathematics, signal processing and machine learning. This article provides a comprehensive study of the state-of-the-art of the topic. We highlight limitations, benefits, connections and differences among the different techniques. Problems and possible solutions with the use of improper priors are also described. Some of the most relevant methodologies are compared through theoretical comparisons and numerical experiments.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation},
  note = {http://arxiv.org/abs/2005.08334},
  timestamp = {2023-05-04T13:00:18Z},
  file = {Llorente et al_2023_Marginal likelihood computation for model selection and hypothesis testing.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Llorente et al_2023_Marginal likelihood computation for model selection and hypothesis testing.pdf:application/pdf;arXiv.org Snapshot:/Users/simonmartin/Zotero/storage/IARCKRPI/2005.html:text/html}
}

@article{Loredo1992,
  title = {Promise of {{Bayesian Inference}} for {{Astrophysics}}},
  author = {Loredo, Thomas J.},
  year = {1992},
  journal = {Statistical Challenges in Modern Astronomy},
  volume = {297},
  pages = {275--297},
  doi = {10.1007/978-1-4613-9290-3_31},
  abstract = {The "frequentist" approach to statistics, currently dominating statistical practice in astrophysics, is compared to the historically older Bayesian approach, which is now growing in popularity in other scientific disciplines, and which provides unique, optimal solutions to well-posed problems. The two approaches address the same questions with very different calculations, but in simple cases often give the same final results, confusing the issue of whether one is superior to the other. Here frequentist and Bayesian methods are applied to problems where such a mathematical coincidence does not occur, allowing assessment of their relative merits based on their performance, rather than philosophical argument.},
  timestamp = {2020-07-24T11:32:10Z}
}

@incollection{loredoBayesianAstrostatisticsBackward2013,
  title = {Bayesian Astrostatistics: A Backward Look to the Future},
  shorttitle = {Bayesian Astrostatistics},
  author = {Loredo, Thomas J.},
  year = {2013},
  volume = {1},
  eprint = {1208.3036},
  primaryclass = {astro-ph, physics:physics, stat},
  pages = {15--40},
  doi = {10.1007/978-1-4614-3508-2_2},
  urldate = {2023-05-15},
  abstract = {This perspective chapter briefly surveys: (1) past growth in the use of Bayesian methods in astrophysics; (2) current misconceptions about both frequentist and Bayesian statistical inference that hinder wider adoption of Bayesian methods by astronomers; and (3) multilevel (hierarchical) Bayesian modeling as a major future direction for research in Bayesian astrostatistics, exemplified in part by presentations at the first ISI invited session on astrostatistics, commemorated in this volume. It closes with an intentionally provocative recommendation for astronomical survey data reporting, motivated by the multilevel Bayesian perspective on modeling cosmic populations: that astronomers cease producing catalogs of estimated fluxes and other source properties from surveys. Instead, summaries of likelihood functions (or marginal likelihood functions) for source properties should be reported (not posterior probability density functions), including nontrivial summaries (not simply upper limits) for candidate objects that do not pass traditional detection thresholds.},
  archiveprefix = {arxiv},
  keywords = {85A35 62F15,Astrophysics - Instrumentation and Methods for Astrophysics,Physics - Data Analysis Statistics and Probability,Statistics - Applications},
  note = {http://arxiv.org/abs/1208.3036},
  timestamp = {2023-05-15T10:07:39Z},
  file = {Loredo_2013_Bayesian astrostatistics.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Loredo_2013_Bayesian astrostatistics.pdf:application/pdf;arXiv.org Snapshot:/Users/simonmartin/Zotero/storage/36P8PNHM/1208.html:text/html}
}

@misc{lotfiBayesianModelSelection2022,
  title = {Bayesian {{Model Selection}}, the {{Marginal Likelihood}}, and {{Generalization}}},
  author = {Lotfi, Sanae and Izmailov, Pavel and Benton, Gregory and Goldblum, Micah and Wilson, Andrew Gordon},
  year = {2022},
  month = jun,
  number = {arXiv:2202.11678},
  eprint = {2202.11678},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-05-02},
  abstract = {How do we compare between hypotheses that are entirely consistent with observations? The marginal likelihood (aka Bayesian evidence), which represents the probability of generating our observations from a prior, provides a distinctive approach to this foundational question, automatically encoding Occam's razor. Although it has been observed that the marginal likelihood can overfit and is sensitive to prior assumptions, its limitations for hyperparameter learning and discrete model comparison have not been thoroughly investigated. We first revisit the appealing properties of the marginal likelihood for learning constraints and hypothesis testing. We then highlight the conceptual and practical issues in using the marginal likelihood as a proxy for generalization. Namely, we show how marginal likelihood can be negatively correlated with generalization, with implications for neural architecture search, and can lead to both underfitting and overfitting in hyperparameter learning. We provide a partial remedy through a conditional marginal likelihood, which we show is more aligned with generalization, and practically valuable for large-scale hyperparameter learning, such as in deep kernel learning.},
  archiveprefix = {arxiv},
  howpublished = {http://arxiv.org/abs/2202.11678},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  timestamp = {2023-12-14T09:00:03Z},
  file = {Lotfi et al_2022_Bayesian Model Selection, the Marginal Likelihood, and Generalization.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Lotfi et al_2022_Bayesian Model Selection, the Marginal Likelihood, and Generalization.pdf:application/pdf;arXiv.org Snapshot:/Users/simonmartin/Zotero/storage/EXG3ZRY6/2202.html:text/html}
}

@article{luParameterEstimationBatch2004a,
  title = {Parameter {{Estimation}} for {{Batch Processes Using}} a {{Bayesian Approach}}},
  author = {Lu, Zhen and Martin, Elaine B. and Morris, Julian},
  year = {2004},
  journal = {IFAC Proceedings Volumes},
  volume = {37},
  number = {9},
  pages = {703--708},
  publisher = {Elsevier},
  issn = {14746670},
  doi = {10.1016/S1474-6670(17)31892-X},
  keywords = {bayesian analysis,Bayesian Analysis,Bayesian AnalysisParameter estimationPolymerizat,parameter estimation,Parameter estimation,Polymerizat,polymerization},
  note = {https://linkinghub.elsevier.com/retrieve/pii/S147466701731892X},
  timestamp = {2023-12-22T14:17:18Z},
  file = {Lu et al_2004_Parameter Estimation for Batch Processes Using a Bayesian Approach.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Lu et al_2004_Parameter Estimation for Batch Processes Using a Bayesian Approach.pdf:application/pdf}
}

@article{lyBayesianMethodologySir2020,
  title = {The {{Bayesian Methodology}} of {{Sir Harold Jeffreys}} as a {{Practical Alternative}} to the {{P Value Hypothesis Test}}},
  author = {Ly, Alexander and Stefan, Angelika and Van Doorn, Johnny and Dablander, Fabian and Van Den Bergh, Don and Sarafoglou, Alexandra and Kucharsk{\'y}, {\u S}imon and Derks, Koen and Gronau, Quentin F. and Raj, Akash and Boehm, Udo and Van Kesteren, Erik-Jan and Hinne, Max and Matzke, Dora and Marsman, Maarten and Wagenmakers, Eric-Jan},
  year = {2020},
  month = jun,
  journal = {Computational Brain \& Behavior},
  volume = {3},
  number = {2},
  pages = {153--161},
  issn = {2522-0861, 2522-087X},
  doi = {10.1007/s42113-019-00070-x},
  urldate = {2024-01-09},
  abstract = {Despite an ongoing stream of lamentations, many empirical disciplines still treat the p value as the sole arbiter to separate the scientific wheat from the chaff. The continued reign of the p value is arguably due in part to a perceived lack of workable alternatives. In order to be workable, any alternative methodology must be (1) relevant: it has to address the practitioners' research question, which---for better or for worse---most often concerns the test of a hypothesis, and less often concerns the estimation of a parameter; (2) available: it must have a concrete implementation for practitioners' statistical workhorses such as the t test, regression, and ANOVA; and (3) easy to use: methods that demand practitioners switch to the theoreticians' programming tools will face an uphill struggle for adoption. The above desiderata are fulfilled by Harold Jeffreys's Bayes factor methodology as implemented in the open-source software JASP. We explain Jeffreys's methodology and showcase its practical relevance with two examples.},
  langid = {english},
  note = {https://link.springer.com/10.1007/s42113-019-00070-x},
  timestamp = {2024-01-09T13:02:18Z},
  file = {Ly et al. - 2020 - The Bayesian Methodology of Sir Harold Jeffreys as.pdf:/Users/simonmartin/Zotero/storage/KSMN4FW2/Ly et al. - 2020 - The Bayesian Methodology of Sir Harold Jeffreys as.pdf:application/pdf}
}

@article{lyBayesianMethodologySir2020a,
  title = {The {{Bayesian Methodology}} of {{Sir Harold Jeffreys}} as a {{Practical Alternative}} to the {{P Value Hypothesis Test}}},
  author = {Ly, Alexander and Stefan, Angelika and Van Doorn, Johnny and Dablander, Fabian and Van Den Bergh, Don and Sarafoglou, Alexandra and Kucharsk{\'y}, {\u S}imon and Derks, Koen and Gronau, Quentin F. and Raj, Akash and Boehm, Udo and Van Kesteren, Erik-Jan and Hinne, Max and Matzke, Dora and Marsman, Maarten and Wagenmakers, Eric-Jan},
  year = {2020},
  month = jun,
  journal = {Computational Brain \& Behavior},
  volume = {3},
  number = {2},
  pages = {153--161},
  issn = {2522-0861, 2522-087X},
  doi = {10.1007/s42113-019-00070-x},
  urldate = {2024-02-06},
  abstract = {Despite an ongoing stream of lamentations, many empirical disciplines still treat the p value as the sole arbiter to separate the scientific wheat from the chaff. The continued reign of the p value is arguably due in part to a perceived lack of workable alternatives. In order to be workable, any alternative methodology must be (1) relevant: it has to address the practitioners' research question, which---for better or for worse---most often concerns the test of a hypothesis, and less often concerns the estimation of a parameter; (2) available: it must have a concrete implementation for practitioners' statistical workhorses such as the t test, regression, and ANOVA; and (3) easy to use: methods that demand practitioners switch to the theoreticians' programming tools will face an uphill struggle for adoption. The above desiderata are fulfilled by Harold Jeffreys's Bayes factor methodology as implemented in the open-source software JASP. We explain Jeffreys's methodology and showcase its practical relevance with two examples.},
  langid = {english},
  note = {https://link.springer.com/10.1007/s42113-019-00070-x},
  timestamp = {2024-02-06T10:34:19Z},
  file = {Ly et al. - 2020 - The Bayesian Methodology of Sir Harold Jeffreys as.pdf:/Users/simonmartin/Zotero/storage/7R3JRUAF/Ly et al. - 2020 - The Bayesian Methodology of Sir Harold Jeffreys as.pdf:application/pdf}
}

@article{machidaBayesianEstimationXPS2021,
  title = {Bayesian Estimation for {{XPS}} Spectral Analysis at Multiple Core Levels},
  author = {Machida, Atsushi and Nagata, Kenji and Murakami, Ryo and Shinotsuka, Hiroshi and Shouno, Hayaru and Yoshikawa, Hideki and Okada, Masato},
  year = {2021},
  month = jan,
  journal = {Science and Technology of Advanced Materials: Methods},
  volume = {1},
  number = {1},
  pages = {123--133},
  publisher = {Taylor \& Francis},
  issn = {null},
  doi = {10.1080/27660400.2021.1943172},
  urldate = {2024-02-09},
  abstract = {X-ray photoelectron spectroscopy (XPS) is a widely used measurement technique in material surface analysis, but its analysis is subject to operator arbitrariness in the results. In a previous paper, a method based on genetic algorithms was proposed to estimate the composition ratios of compounds from XPS data using reference spectra and it was shown that it is possible to analyze them automatically from the reference spectra data. In this paper, we newly proposed a Bayesian spectral decomposition method based on the exchange Monte Carlo method and tested it on artificial data. This method provides a posterior distribution of the model parameters. This not only allows the estimation of compositional ratios for samples, but also allows statistical reliability assessment. In addition, we simulated an artificial data analysis to clarify the effect on the identification of compounds and the estimation of their compositional ratios by varying the signal-to-noise ratio of the data.},
  keywords = {Bayesian estimation,Material informatics,Materials data analysis,surface analysis},
  note = {https://doi.org/10.1080/27660400.2021.1943172},
  timestamp = {2024-03-27T10:07:16Z},
  file = {Machida et al_2021_Bayesian estimation for XPS spectral analysis at multiple core levels.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Machida et al_2021_Bayesian estimation for XPS spectral analysis at multiple core levels.pdf:application/pdf;Machida et al. - 2021 - Bayesian estimation for XPS spectral analysis at m.pdf:/Users/simonmartin/Zotero/storage/EY3CNTVI/Machida et al. - 2021 - Bayesian estimation for XPS spectral analysis at m.pdf:application/pdf}
}

@article{mackayBayesianInterpolation1992,
  title = {Bayesian {{Interpolation}}},
  author = {MacKay, David J. C.},
  year = {1992},
  month = may,
  journal = {Neural Computation},
  volume = {4},
  number = {3},
  pages = {415--447},
  issn = {0899-7667},
  doi = {10.1162/neco.1992.4.3.415},
  urldate = {2023-12-21},
  abstract = {Although Bayesian analysis has been in use since Laplace, the Bayesian method of model-comparison has only recently been developed in depth. In this paper, the Bayesian approach to regularization and model-comparison is demonstrated by studying the inference problem of interpolating noisy data. The concepts and methods described are quite general and can be applied to many other data modeling problems. Regularizing constants are set by examining their posterior probability distribution. Alternative regularizers (priors) and alternative basis sets are objectively compared by evaluating the evidence for them. ``Occam's razor'' is automatically embodied by this process. The way in which Bayes infers the values of regularizing constants and noise levels has an elegant interpretation in terms of the effective number of parameters determined by the data set. This framework is due to Gull and Skilling.},
  note = {https://doi.org/10.1162/neco.1992.4.3.415},
  timestamp = {2023-12-21T11:00:43Z},
  file = {MacKay_1992_Bayesian Interpolation.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/MacKay_1992_Bayesian Interpolation.pdf:application/pdf;Snapshot:/Users/simonmartin/Zotero/storage/BKCM929N/Bayesian-Interpolation.html:text/html}
}

@incollection{mackayIntroductionMonteCarlo1998,
  title = {Introduction to {{Monte Carlo Methods}}},
  booktitle = {Learning in {{Graphical Models}}},
  author = {Mackay, D. J. C.},
  editor = {Jordan, Michael I.},
  year = {1998},
  series = {{{NATO ASI Series}}},
  pages = {175--204},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-011-5014-9_7},
  urldate = {2024-01-29},
  abstract = {This chapter describes a sequence of Monte Carlo methods: importance sampling, rejection sampling, the Metropolis method, and Gibbs sampling. For each method, we discuss whether the method is expected to be useful for high---dimensional problems such as arise in inference with graphical models. After the methods have been described, the terminology of Markov chain Monte Carlo methods is presented. The chapter concludes with a discussion of advanced methods, including methods for reducing random walk behaviour.},
  isbn = {978-94-011-5014-9},
  langid = {english},
  keywords = {Gibbs Sampling,Importance Sampling,Ising Model,Markov Chain,Monte Carlo Method},
  note = {https://doi.org/10.1007/978-94-011-5014-9\_7},
  timestamp = {2024-01-29T14:07:01Z},
  file = {Mackay_1998_Introduction to Monte Carlo Methods.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Mackay_1998_Introduction to Monte Carlo Methods.pdf:application/pdf}
}

@inproceedings{malakarEntropyBasedSearchAlgorithm2011,
  title = {Entropy-{{Based Search Algorithm}} for {{Experimental Design}}},
  author = {Malakar, N. K. and Knuth, K. H.},
  year = {2011},
  eprint = {1008.4973},
  primaryclass = {physics, stat},
  pages = {157--164},
  doi = {10.1063/1.3573612},
  urldate = {2023-04-25},
  abstract = {The scientific method relies on the iterated processes of inference and inquiry. The inference phase consists of selecting the most probable models based on the available data; whereas the inquiry phase consists of using what is known about the models to select the most relevant experiment. Optimizing inquiry involves searching the parameterized space of experiments to select the experiment that promises, on average, to be maximally informative. In the case where it is important to learn about each of the model parameters, the relevance of an experiment is quantified by Shannon entropy of the distribution of experimental outcomes predicted by a probable set of models. If the set of potential experiments is described by many parameters, we must search this high-dimensional entropy space. Brute force search methods will be slow and computationally expensive. We present an entropy-based search algorithm, called nested entropy sampling, to select the most informative experiment for efficient experimental design. This algorithm is inspired by Skilling's nested sampling algorithm used in inference and borrows the concept of a rising threshold while a set of experiment samples are maintained. We demonstrate that this algorithm not only selects highly relevant experiments, but also is more efficient than brute force search. Such entropic search techniques promise to greatly benefit autonomous experimental design.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Physics - Computational Physics,Physics - Data Analysis Statistics and Probability,Statistics - Machine Learning},
  note = {http://arxiv.org/abs/1008.4973},
  timestamp = {2023-04-25T14:54:50Z},
  file = {Malakar_Knuth_2011_Entropy-Based Search Algorithm for Experimental Design.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Malakar_Knuth_2011_Entropy-Based Search Algorithm for Experimental Design.pdf:application/pdf;arXiv.org Snapshot:/Users/simonmartin/Zotero/storage/B7ZW2ULK/1008.html:text/html}
}

@article{marsagliaRandomNumbersFall1968,
  title = {Random {{Numbers Fall Mainly}} in the {{Planes}}},
  author = {Marsaglia, George},
  year = {1968},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {61},
  number = {1},
  eprint = {58853},
  eprinttype = {jstor},
  pages = {25--28},
  publisher = {National Academy of Sciences},
  issn = {0027-8424},
  urldate = {2023-09-04},
  note = {https://www.jstor.org/stable/58853},
  timestamp = {2023-09-04T10:20:22Z},
  file = {Marsaglia_1968_Random Numbers Fall Mainly in the Planes.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Marsaglia_1968_Random Numbers Fall Mainly in the Planes.pdf:application/pdf}
}

@book{mcelreathStatisticalRethinkingBayesian2020,
  title = {Statistical {{Rethinking}}: {{A Bayesian Course}} with {{Examples}} in {{R}} and {{Stan}}, 2nd {{Edition}}},
  author = {McElreath, Richard},
  year = {2020},
  edition = {2},
  publisher = {CRC Press},
  note = {http://xcelab.net/rm/statistical-rethinking/},
  timestamp = {2023-11-21T14:41:41Z}
}

@book{miocevicSmallSampleSize2020,
  title = {Small {{Sample Size Solutions}}: {{A Guide}} for {{Applied Researchers}} and {{Practitioners}}},
  shorttitle = {Small {{Sample Size Solutions}}},
  editor = {Mio{\v c}evi{\'c}, Milica, Rens van de Schoot},
  year = {2020},
  month = feb,
  publisher = {Routledge},
  address = {London},
  doi = {10.4324/9780429273872},
  abstract = {Researchers often have difficulties collecting enough data to test their hypotheses, either because target groups are small or hard to access, or because data collection entails prohibitive costs. Such obstacles may result in data sets that are too small for the complexity of the statistical model needed to answer the research question. This unique~book provides guidelines and tools for implementing solutions to issues that arise in small sample research. Each chapter~illustrates statistical methods that allow researchers to apply the optimal statistical model for their research question when the sample is too small. This~essential book will enable social and behavioral science researchers to test their hypotheses even when the statistical model required for answering their research question is too complex for the sample sizes they can collect.~The statistical models in the book range~from the estimation of a population mean to models with latent variables and nested observations, and solutions include both classical and Bayesian methods.~All proposed solutions are described in steps researchers can implement with their own data and are accompanied with annotated syntax in R.  The methods described in this book will be useful for researchers across the social and behavioral sciences, ranging from medical sciences and epidemiology to psychology, marketing, and economics.},
  isbn = {978-0-429-27387-2},
  timestamp = {2023-12-21T16:31:18Z},
  file = {Miočević_2020_Small Sample Size Solutions.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Miočević_2020_Small Sample Size Solutions.pdf:application/pdf}
}

@article{Miro-Quesada2004,
  title = {A {{Bayesian}} Approach for Multiple Response Surface Optimization in the Presence of Noise Variables},
  author = {{Mir{\'o}-Quesada}, Guillermo and Del Castillo, Enrique and Peterson, John J.},
  year = {2004},
  journal = {Journal of Applied Statistics},
  volume = {31},
  number = {3},
  pages = {251--270},
  issn = {02664763},
  doi = {10.1080/0266476042000184019},
  abstract = {An approach for the multiple response robust parameter design problem based on a methodology by Peterson (2000) is presented. The approach is Bayesian, and consists of maximizing the posterior predictive probability that the process satisfies a set of constraints on the responses. In order to find a solution robust to variation in the noise variables, the predictive density is integrated not only with respect to the response variables but also with respect to the assumed distribution of the noise variables. The maximization problem involves repeated Monte Carlo integrations, and two different methods to solve it are evaluated. A Matlab code was written that rapidly finds an optimal (robust) solution in case it exists. Two examples taken from the literature are used to illustrate the proposed method.},
  isbn = {1167-7422 (Print) 1167-7422 (Linking)},
  pmid = {15751179},
  keywords = {Bayesian statistics,Monte Carlo integration,Response surface methodology,Robust parameter design},
  timestamp = {2023-06-14T07:47:01Z},
  file = {Miró-Quesada et al_2004_A Bayesian approach for multiple response surface optimization in the presence.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Miró-Quesada et al_2004_A Bayesian approach for multiple response surface optimization in the presence.pdf:application/pdf}
}

@article{Moores2016,
  title = {Bayesian Modelling and Quantification of {{Raman}} Spectroscopy},
  author = {Moores, Matthew and Gracie, Kirsten and Carson, Jake and Faulds, Karen and Graham, Duncan and Girolami, Mark},
  year = {2016},
  eprint = {1604.07299},
  pages = {1--24},
  abstract = {Raman spectroscopy can be used to identify molecules such as DNA by the characteristic scattering of light from a laser. It is sensitive at very low concentrations and can accurately quantify the amount of a given molecule in a sample. The presence of a large, nonuniform background presents a major challenge to analysis of these spectra. To overcome this challenge, we introduce a sequential Monte Carlo (SMC) algorithm to separate each observed spectrum into a series of peaks plus a smoothly-varying baseline, corrupted by additive white noise. The peaks are modelled as Lorentzian, Gaussian, or pseudo-Voigt functions, while the baseline is estimated using a penalised cubic spline. This latent continuous representation accounts for differences in resolution between measurements. The posterior distribution can be incrementally updated as more data becomes available, resulting in a scalable algorithm that is robust to local maxima. By incorporating this representation in a Bayesian hierarchical regression model, we can quantify the relationship between molecular concentration and peak intensity, thereby providing an improved estimate of the limit of detection, which is of major importance to analytical chemistry.},
  archiveprefix = {arxiv},
  keywords = {chemometrics,functional data analysis,multivariate calibration,nanotech-,nology,sequential monte carlo},
  note = {http://arxiv.org/abs/1604.07299},
  timestamp = {2023-04-28T14:32:41Z},
  file = {Moores et al_2016_Bayesian modelling and quantification of Raman spectroscopy.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Moores et al_2016_Bayesian modelling and quantification of Raman spectroscopy.pdf:application/pdf}
}

@article{morrisBayesianHierarchicalSpatial2019,
  title = {Bayesian Hierarchical Spatial Models: {{Implementing}} the {{Besag York Molli{\'e}}} Model in Stan},
  shorttitle = {Bayesian Hierarchical Spatial Models},
  author = {Morris, Mitzi and {Wheeler-Martin}, Katherine and Simpson, Dan and Mooney, Stephen J. and Gelman, Andrew and DiMaggio, Charles},
  year = {2019},
  month = nov,
  journal = {Spatial and Spatio-temporal Epidemiology},
  volume = {31},
  pages = {100301},
  issn = {18775845},
  doi = {10.1016/j.sste.2019.100301},
  urldate = {2024-03-13},
  langid = {english},
  note = {https://linkinghub.elsevier.com/retrieve/pii/S1877584518301175},
  timestamp = {2024-03-13T09:47:12Z},
  file = {Morris et al. - 2019 - Bayesian hierarchical spatial models Implementing.pdf:/Users/simonmartin/Zotero/storage/KF7UFHF9/Morris et al. - 2019 - Bayesian hierarchical spatial models Implementing.pdf:application/pdf}
}

@article{muthukumaranaBayesianAnalysisOrdinal2014,
  title = {Bayesian {{Analysis}} of {{Ordinal Survey Data Using}} the {{Dirichlet Process}} to {{Account}} for {{Respondent Personality Traits}}},
  author = {Muthukumarana, Saman and Swartz, Tim B.},
  year = {2014},
  month = jan,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {43},
  number = {1},
  pages = {82--98},
  issn = {0361-0918, 1532-4141},
  doi = {10.1080/03610918.2012.698773},
  urldate = {2023-08-21},
  abstract = {This paper presents a Bayesian latent variable model used to analyze ordinal response survey data by taking into account the characteristics of respondents. The ordinal response data are viewed as multivariate responses arising from continuous latent variables with known cut-points. Each respondent is characterized by two parameters that have a Dirichlet process as their joint prior distribution. The proposed mechanism adjusts for classes of personalities. The model is applied to student survey data in course evaluations. Goodness-of-fit (gof) procedures are developed for assessing the validity of the model. The proposed gof procedures are simple, intuitive and do not seem to be a part of current Bayesian practice.},
  langid = {english},
  note = {http://www.tandfonline.com/doi/abs/10.1080/03610918.2012.698773},
  timestamp = {2023-08-21T13:08:23Z},
  file = {Muthukumarana and Swartz - 2014 - Bayesian Analysis of Ordinal Survey Data Using the.pdf:/Users/simonmartin/Zotero/storage/QIQJDCS8/Muthukumarana and Swartz - 2014 - Bayesian Analysis of Ordinal Survey Data Using the.pdf:application/pdf}
}

@article{Nagata2012,
  title = {Bayesian Spectral Deconvolution with the Exchange {{Monte Carlo}} Method},
  author = {Nagata, Kenji and Sugita, Seiji and Okada, Masato},
  year = {2012},
  journal = {Neural Networks},
  volume = {28},
  pages = {82--89},
  publisher = {Elsevier Ltd},
  issn = {08936080},
  doi = {10.1016/j.neunet.2011.12.001},
  abstract = {An analytical method to deconvolute spectral data into a number of simple bands is extremely important in the analysis of the chemical properties of matter. However, there are two fundamental problems with such deconvolution methods. One is how to determine the number of bands without resorting to heuristics. The other is difficulty in avoiding the parameter solution trapped into local minima due to the hierarchy and the nonlinearity of the system. In this study, we propose a novel method of spectral deconvolution based on Bayesian estimation with the exchange Monte Carlo method, which is an application of the integral approximation of stochastic complexity and the exchange Monte Carlo method. We also experimentally show its effectiveness on synthetic data and on reflectance spectral data of olivine, one of the most common minerals of terrestrial planets. {\copyright} 2011 Elsevier Ltd.},
  keywords = {Bayesian estimation,Exchange Monte Carlo method,Spectral deconvolution,Spectroanalysis},
  note = {http://dx.doi.org/10.1016/j.neunet.2011.12.001},
  timestamp = {2023-03-29T12:59:30Z},
  file = {1-s2.0-S0893608011003169-main.pdf:/Users/simonmartin/Zotero/storage/AFT9XRTV/1-s2.0-S0893608011003169-main.pdf:application/pdf}
}

@article{Nagata2018,
  title = {Bayesian {{Spectral Deconvolution Based}} on {{Poisson Distribution}}: {{Bayesian Measurement}} and {{Virtual Measurement Analytics}} ({{VMA}})},
  author = {Nagata, Kenji and Mototake, Yoh-ichi and Muraoka, Rei and Sasaki, Takehiko and Okada, Masato},
  year = {2018},
  eprint = {1812.05501},
  pages = {1--8},
  abstract = {In this paper, we propose a new method of Bayesian measurement for spectral deconvolution, which regresses spectral data into the sum of unimodal basis function such as Gaussian or Lorentzian functions. Bayesian measurement is a framework for considering not only the target physical model but also the measurement model as a probabilistic model, and enables us to estimate the parameter of a physical model with its confidence interval through a Bayesian posterior distribution given a measurement data set. The measurement with Poisson noise is one of the most effective system to apply our proposed method. Since the measurement time is strongly related to the signal-to-noise ratio for the Poisson noise model, Bayesian measurement with Poisson noise model enables us to clarify the relationship between the measurement time and the limit of estimation. In this study, we establish the probabilistic model with Poisson noise for spectral deconvolution. Bayesian measurement enables us to perform virtual and computer simulation for a certain measurement through the established probabilistic model. This property is called "Virtual Measurement Analytics(VMA)" in this paper. We also show that the relationship between the measurement time and the limit of estimation can be extracted by using the proposed method in a simulation of synthetic data and real data for XPS measurement of MoS\$\_2\$.},
  archiveprefix = {arxiv},
  note = {http://arxiv.org/abs/1812.05501},
  timestamp = {2023-04-28T14:33:11Z},
  file = {Nagata et al_2018_Bayesian Spectral Deconvolution Based on Poisson Distribution.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Nagata et al_2018_Bayesian Spectral Deconvolution Based on Poisson Distribution.pdf:application/pdf}
}

@misc{nealSliceSampling2000,
  title = {Slice {{Sampling}}},
  author = {Neal, Radford M.},
  year = {2000},
  month = sep,
  number = {arXiv:physics/0009028},
  eprint = {physics/0009028},
  publisher = {arXiv},
  urldate = {2023-03-28},
  abstract = {Markov chain sampling methods that automatically adapt to characteristics of the distribution being sampled can be constructed by exploiting the principle that one can sample from a distribution by sampling uniformly from the region under the plot of its density function. A Markov chain that converges to this uniform distribution can be constructed by alternating uniform sampling in the vertical direction with uniform sampling from the horizontal `slice' defined by the current vertical position, or more generally, with some update that leaves the uniform distribution over this slice invariant. Variations on such `slice sampling' methods are easily implemented for univariate distributions, and can be used to sample from a multivariate distribution by updating each variable in turn. This approach is often easier to implement than Gibbs sampling, and more efficient than simple Metropolis updates, due to the ability of slice sampling to adaptively choose the magnitude of changes made. It is therefore attractive for routine and automated use. Slice sampling methods that update all variables simultaneously are also possible. These methods can adaptively choose the magnitudes of changes made to each variable, based on the local properties of the density function. More ambitiously, such methods could potentially allow the sampling to adapt to dependencies between variables by constructing local quadratic approximations. Another approach is to improve sampling efficiency by suppressing random walks. This can be done using `overrelaxed' versions of univariate slice sampling procedures, or by using `reflective' multivariate slice sampling methods, which bounce off the edges of the slice.},
  archiveprefix = {arxiv},
  howpublished = {http://arxiv.org/abs/physics/0009028},
  keywords = {Physics - Computational Physics,Physics - Data Analysis Statistics and Probability},
  timestamp = {2023-03-28T14:55:22Z},
  file = {Neal_2000_Slice Sampling.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Neal_2000_Slice Sampling.pdf:application/pdf;arXiv.org Snapshot:/Users/simonmartin/Zotero/storage/BG23DWKQ/0009028.html:text/html}
}

@article{nealSliceSampling2003,
  title = {Slice Sampling},
  author = {Neal, Radford M.},
  year = {2003},
  month = jun,
  journal = {The Annals of Statistics},
  volume = {31},
  number = {3},
  pages = {705--767},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1056562461},
  urldate = {2024-02-15},
  abstract = {Markov chain sampling methods that adapt to characteristics of the distribution being sampled can be constructed using the principle that one can ample from a distribution by sampling uniformly from the region under the plot of its density function. A Markov chain that converges to this uniform distribution can be constructed by alternating uniform sampling in the vertical direction with uniform sampling from the horizontal "slice" defined by the current vertical position, or more generally, with some update that leaves the uniform distribution over this slice invariant. Such "slice sampling" methods are easily implemented for univariate distributions, and can be used to sample from a multivariate distribution by updating each variable in turn. This approach is often easier to implement than Gibbs sampling and more efficient than simple Metropolis updates, due to the ability of slice sampling to adaptively choose the magnitude of changes made. It is therefore attractive for routine and automated use. Slice sampling methods that update all variables simultaneously are also possible. These methods can adaptively choose the magnitudes of changes made to each variable, based on the local properties of the density function. More ambitiously, such methods could potentially adapt to the dependencies between variables by constructing local quadratic approximations. Another approach is to improve sampling efficiency by suppressing random walks. This can be done for univariate slice sampling by "overrelaxation," and for multivariate slice sampling by "reflection" from the edges of the slice.},
  keywords = {65C05,65C60,Adaptive methods,auxiliary variables,dynamical methods,Gibbs sampling,Markov chain Monte Carlo,Metropolis algorithm,overrelaxation},
  note = {https://projecteuclid.org/journals/annals-of-statistics/volume-31/issue-3/Slice-sampling/10.1214/aos/1056562461.full},
  timestamp = {2024-02-15T13:21:13Z},
  file = {Neal_2003_Slice sampling.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Neal_2003_Slice sampling.pdf:application/pdf}
}

@misc{NestleNestleDocumentation,
  title = {Nestle --- Nestle 0.2.0 Documentation},
  urldate = {2024-02-09},
  howpublished = {http://kylebarbary.com/nestle/},
  timestamp = {2024-02-09T15:41:45Z},
  file = {Nestle — nestle 0.2.0 documentation:/Users/simonmartin/Zotero/storage/GMCLK4VK/nestle.html:text/html}
}

@article{oharaNotLogtransformCount2010,
  title = {Do Not Log-Transform Count Data},
  author = {O'Hara, Robert B. and Kotze, D. Johan},
  year = {2010},
  journal = {Methods in Ecology and Evolution},
  volume = {1},
  number = {2},
  pages = {118--122},
  issn = {2041-210X},
  doi = {10.1111/j.2041-210X.2010.00021.x},
  urldate = {2023-04-24},
  abstract = {1. Ecological count data (e.g. number of individuals or species) are often log-transformed to satisfy parametric test assumptions. 2. Apart from the fact that generalized linear models are better suited in dealing with count data, a log-transformation of counts has the additional quandary in how to deal with zero observations. With just one zero observation (if this observation represents a sampling unit), the whole data set needs to be fudged by adding a value (usually 1) before transformation. 3. Simulating data from a negative binomial distribution, we compared the outcome of fitting models that were transformed in various ways (log, square root) with results from fitting models using quasi-Poisson and negative binomial models to untransformed count data. 4. We found that the transformations performed poorly, except when the dispersion was small and the mean counts were large. The quasi-Poisson and negative binomial models consistently performed well, with little bias. 5. We recommend that count data should not be analysed by log-transforming it, but instead models based on Poisson and negative binomial distributions should be used.},
  langid = {english},
  keywords = {generalized linear models,linear models,overdispersion,Poisson,transformation},
  note = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2041-210X.2010.00021.x},
  timestamp = {2023-04-24T15:33:37Z},
  file = {O’Hara_Kotze_2010_Do not log-transform count data.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/O’Hara_Kotze_2010_Do not log-transform count data.pdf:application/pdf;Snapshot:/Users/simonmartin/Zotero/storage/ZAREBDR2/j.2041-210X.2010.00021.html:text/html}
}

@phdthesis{Osullivan2013a,
  title = {Bayesian {{Latent Variable Models}} with {{Applications}}},
  author = {O'sullivan, Aidan Michael},
  year = {2013},
  school = {Imperial College},
  timestamp = {2023-08-09T10:58:37Z},
  file = {O'sullivan_2013_Bayesian Latent Variable Models with Applications.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/O'sullivan_2013_Bayesian Latent Variable Models with Applications.pdf:application/pdf}
}

@article{padillaCosmologicalParameterInference2021,
  title = {Cosmological Parameter Inference with {{Bayesian}} Statistics},
  author = {Padilla, Luis E. and Tellez, Luis O. and Escamilla, Luis A. and Vazquez, J. Alberto},
  year = {2021},
  month = jun,
  journal = {Universe},
  volume = {7},
  number = {7},
  eprint = {1903.11127},
  primaryclass = {astro-ph},
  pages = {213},
  issn = {2218-1997},
  doi = {10.3390/universe7070213},
  urldate = {2023-08-21},
  abstract = {Bayesian statistics and Markov Chain Monte Carlo (MCMC) algorithms have found their place in the field of Cosmology. They have become important mathematical and numerical tools, especially in parameter estimation and model comparison. In this paper, we review some fundamental concepts to understand Bayesian statistics and then introduce MCMC algorithms and samplers that allow us to perform the parameter inference procedure. We also introduce a general description of the standard cosmological model, known as the \${\textbackslash}Lambda\$CDM model, along with several alternatives, and current datasets coming from astrophysical and cosmological observations. Finally, with the tools acquired, we use an MCMC algorithm implemented in python to test several cosmological models and find out the combination of parameters that best describes the Universe.},
  archiveprefix = {arxiv},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics},
  note = {http://arxiv.org/abs/1903.11127},
  timestamp = {2023-08-21T13:49:01Z},
  file = {Padilla et al_2021_Cosmological parameter inference with Bayesian statistics.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Padilla et al_2021_Cosmological parameter inference with Bayesian statistics.pdf:application/pdf;arXiv.org Snapshot:/Users/simonmartin/Zotero/storage/HURC4HIS/1903.html:text/html}
}

@article{pardoBayesianAnalysisQENS,
  title = {Bayesian {{Analysis}} of {{QENS}} Data: {{From}} Parameter Determination to Model Selection},
  author = {Pardo, L C and Busch, S and Unruh, T},
  volume = {2},
  eprint = {0907.3711v3},
  pages = {1--7},
  archiveprefix = {arxiv},
  timestamp = {2024-02-02T14:46:45Z},
  file = {Pardo et al_Bayesian Analysis of QENS data.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Pardo et al_Bayesian Analysis of QENS data2.pdf:application/pdf}
}

@article{partayNestedSamplingMaterials2021,
  title = {Nested Sampling for Materials},
  author = {P{\'a}rtay, Livia B. and Cs{\'a}nyi, G{\'a}bor and Bernstein, Noam},
  year = {2021},
  month = aug,
  journal = {The European Physical Journal B},
  volume = {94},
  number = {8},
  pages = {159},
  issn = {1434-6028, 1434-6036},
  doi = {10.1140/epjb/s10051-021-00172-1},
  urldate = {2023-03-28},
  abstract = {We review the materials science applications of the nested sampling (NS) method, which was originally conceived for calculating the evidence in Bayesian inference. We describe how NS can be adapted to sample the potential energy surface (PES) of atomistic systems, providing a straightforward approximation for the partition function and allowing the evaluation of thermodynamic variables at arbitrary temperatures. After an overview of the basic method, we describe a number of extensions, including using variable cells for constant pressure sampling, the semi-grand-canonical approach for multicomponent systems, parallelizing the algorithm, and visualizing the results. We cover the range of materials applications of NS from the past decade, from exploring the PES of Lennard--Jones clusters to that of multicomponent condensed phase systems. We highlight examples how the information gained via NS promotes the understanding of materials properties through a novel way of visualizing the PES, identifying thermodynamically relevant basins, and calculating the entire pressure--temperature(--composition) phase diagram.},
  langid = {english},
  note = {https://link.springer.com/10.1140/epjb/s10051-021-00172-1},
  timestamp = {2023-03-28T11:48:36Z},
  file = {Pártay et al_2021_Nested sampling for materials.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Pártay et al_2021_Nested sampling for materials.pdf:application/pdf}
}

@techreport{pennycookCRADAFinalReport,
  title = {{{CRADA}} Final Report -- Pixon},
  author = {Pennycook and Lupini},
  urldate = {2023-09-08},
  note = {https://www.osti.gov/servlets/purl/939888},
  timestamp = {2023-12-14T09:02:32Z},
  file = {939888.pdf:/Users/simonmartin/Zotero/storage/VSRYFMRL/939888.pdf:application/pdf}
}

@article{perreaultDetectionChangesPrecipitation1999,
  title = {Detection of Changes in Precipitation and Runoff over Eastern {{Canada}} and {{U}}.{{S}}. Using a {{Bayesian}} Approach},
  author = {Perreault, L. and Hach{\'e}, M. and Slivitzky, M. and Bob{\'e}e, B.},
  year = {1999},
  month = jun,
  journal = {Stochastic Environmental Research and Risk Assessment},
  volume = {13},
  number = {3},
  pages = {201--216},
  issn = {1436-3259},
  doi = {10.1007/s004770050039},
  urldate = {2023-07-19},
  abstract = {Inference about the existence and characteristics of changes in mean level of hydrometeorological sequences that may be generated by climatic variability is an important step before developing management rules in water resources systems. This paper presents a Bayesian approach, based on a single shifting model, which can be used to study a change in the mean level of a set of independent normal random variables. Two different problems are considered: the first is the detection of a change, while the second is the estimation of the change-point and its amplitude under the assumption that a change has occurred. This method is applied to precipitation and runoff data series over eastern Canada and U.S. during the twentieth century. The main results show an increase in the late sixties in the Eastern North American precipitation. This supports conclusions drawn from a number of studies which identified the late sixties to early seventies as a period of possible change.},
  langid = {english},
  keywords = {Bayes factor,change-point,climatic variability,hydrology.,Key words: Bayesian methods,time series},
  note = {https://doi.org/10.1007/s004770050039},
  timestamp = {2023-07-19T06:13:06Z},
  file = {Perreault et al_1999_Detection of changes in precipitation and runoff over eastern Canada and U.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Perreault et al_1999_Detection of changes in precipitation and runoff over eastern Canada and U.pdf:application/pdf}
}

@article{perreaultDetectionChangesPrecipitation1999a,
  title = {Detection of Changes in Precipitation and Runoff over Eastern {{Canada}} and {{U}}.{{S}}. Using a {{Bayesian}} Approach},
  author = {Perreault, L. and Hach{\'e}, M. and Slivitzky, M. and Bob{\'e}e, B.},
  year = {1999},
  month = jun,
  journal = {Stochastic Environmental Research and Risk Assessment (SERRA)},
  volume = {13},
  number = {3},
  pages = {201--216},
  issn = {1436-3240, 1436-3259},
  doi = {10.1007/s004770050039},
  urldate = {2023-08-21},
  abstract = {Inference about the existence and characteristics of changes in mean level of hydrometeorological sequences that may be generated by climatic variability is an important step before developing management rules in water resources systems. This paper presents a Bayesian approach, based on a single shifting model, which can be used to study a change in the mean level of a set of independent normal random variables. Two different problems are considered: the {\textregistered}rst is the detection of a change, while the second is the estimation of the change-point and its amplitude under the assumption that a change has occurred. This method is applied to precipitation and runoff data series over eastern Canada and U.S. during the twentieth century. The main results show an increase in the late sixties in the Eastern North American precipitation. This supports conclusions drawn from a number of studies which identi{\textregistered}ed the late sixties to early seventies as a period of possible change.},
  langid = {english},
  note = {http://link.springer.com/10.1007/s004770050039},
  timestamp = {2023-08-21T13:12:46Z},
  file = {Perreault et al. - 1999 - Detection of changes in precipitation and runoff o.pdf:/Users/simonmartin/Zotero/storage/URRTY5G9/Perreault et al. - 1999 - Detection of changes in precipitation and runoff o.pdf:application/pdf}
}

@book{polpoBayesianInferenceMaximum2018,
  title = {Bayesian {{Inference}} and {{Maximum Entropy Methods}} in {{Science}} and {{Engineering}}: {{MaxEnt}} 37, {{Jarinu}}, {{Brazil}}, {{July}} 09--14, 2017},
  shorttitle = {Bayesian {{Inference}} and {{Maximum Entropy Methods}} in {{Science}} and {{Engineering}}},
  editor = {Polpo, Adriano and Stern, Julio and Louzada, Francisco and Izbicki, Rafael and Takada, Hellinton},
  year = {2018},
  series = {Springer {{Proceedings}} in {{Mathematics}} \& {{Statistics}}},
  volume = {239},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-91143-4},
  urldate = {2023-05-02},
  isbn = {978-3-319-91142-7 978-3-319-91143-4},
  langid = {english},
  note = {http://link.springer.com/10.1007/978-3-319-91143-4},
  timestamp = {2023-05-02T12:03:22Z},
  file = {Polpo et al. - 2018 - Bayesian Inference and Maximum Entropy Methods in .pdf:/Users/simonmartin/Zotero/storage/BAEGHVAN/Polpo et al. - 2018 - Bayesian Inference and Maximum Entropy Methods in .pdf:application/pdf}
}

@article{poulopoulouMethodRemovalChromium1998,
  title = {A {{Method}} for the {{Removal}} of {{Chromium}} from {{Tanned Leather Wastes}}},
  author = {Poulopoulou, Vassiliki G. and Katakis, Dimitris and Vrachnou, Ersi},
  year = {1998},
  month = sep,
  journal = {Journal of the Air \& Waste Management Association},
  volume = {48},
  number = {9},
  pages = {846--852},
  issn = {1096-2247, 2162-2906},
  doi = {10.1080/10473289.1998.10463735},
  urldate = {2024-03-25},
  langid = {english},
  note = {https://www.tandfonline.com/doi/full/10.1080/10473289.1998.10463735},
  timestamp = {2024-03-27T09:49:49Z},
  file = {Poulopoulou et al_1998_A Method for the Removal of Chromium from Tanned Leather Wastes.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Poulopoulou et al_1998_A Method for the Removal of Chromium from Tanned Leather Wastes.pdf:application/pdf}
}

@misc{Price,
  title = {Bayesian Prediction of Mean Indoor Radon Concentration for {{Minnesota}} Counties},
  author = {Price, P. N. and Nero, A. V. and Gelman, A},
  timestamp = {2014-02-08T22:19:25Z}
}

@misc{PriorChoiceRecommendations,
  title = {Prior {{Choice Recommendations}}},
  journal = {GitHub},
  urldate = {2024-01-17},
  abstract = {Stan development repository. The master branch contains the current release. The develop branch contains the latest stable development.  See the Developer Process Wiki for details.   - stan-dev/stan},
  howpublished = {https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations},
  langid = {english},
  timestamp = {2024-01-17T10:12:17Z},
  file = {Snapshot:/Users/simonmartin/Zotero/storage/IR9DRS5R/Prior-Choice-Recommendations.html:text/html}
}

@inproceedings{puetterMaximumEntropyFractal1994,
  title = {Beyond Maximum Entropy: {{Fractal Pixon-based}} Image Reconstruction},
  shorttitle = {Beyond Maximum Entropy},
  author = {Puetter, Richard C. and Pina, R. K.},
  year = {1994},
  month = apr,
  urldate = {2023-09-02},
  abstract = {We have developed a new Bayesian image reconstruction method that has been shown to be superior to the best implementations of other competing methods, including Goodness-of-Fit methods such as Least-Squares fitting and Lucy-Richardson reconstruction, as well as Maximum Entropy (ME) methods such as those embodied in the MEMSYS algorithms. Our new method is based on the concept of the pixon, the fundamental, indivisible unit of picture information. Use of the pixon concept provides an improved image model, resulting in an image prior which is superior to that of standard ME. Our past work has shown how uniform information content pixons can be used to develop a 'Super-ME' method in which entropy is maximized exactly. Recently, however, we have developed a superior pixon basis for the image, the Fractal Pixon Basis (FPB). Unlike the Uniform Pixon Basis (UPB) of our 'Super-ME' method, the FPB basis is selected by employing fractal dimensional concepts to assess the inherent structure in the image. The Fractal Pixon Basis results in the best image reconstructions to date, superior to both UPB and the best ME reconstructions. In this paper, we review the theory of the UPB and FPB pixon and apply our methodology to the reconstruction of far-infrared imaging of the galaxy M51. The results of our reconstruction are compared to published reconstructions of the same data using the Lucy-Richardson algorithm, the Maximum Correlation Method developed at IPAC, and the MEMSYS ME algorithms. The results show that our reconstructed image has a spatial resolution a factor of two better than best previous methods (and a factor of 20 finer than the width of the point response function), and detects sources two orders of magnitude fainter than other methods.},
  keywords = {Documentation And Information Science},
  annotation = {NTRS Author Affiliations: California Univ. NTRS Document ID: 19950015340 NTRS Research Center: Legacy CDMS (CDMS)},
  note = {https://ntrs.nasa.gov/citations/19950015340},
  timestamp = {2023-09-02T18:44:59Z},
  file = {Puetter_Pina_1994_Beyond maximum entropy.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Puetter_Pina_1994_Beyond maximum entropy.pdf:application/pdf;Snapshot:/Users/simonmartin/Zotero/storage/IE7R9CTZ/19950015340.html:text/html}
}

@incollection{puetterModernMethodsImage1997a,
  title = {Modern {{Methods}} of {{Image Reconstruction}}},
  booktitle = {Instrumentation for {{Large Telescopes}}},
  author = {Puetter, R. C.},
  editor = {Espinosa, Jose M. Rodriguez and Herrero, Artemio and S{\'a}nchez, Francisco},
  year = {1997},
  month = oct,
  edition = {1},
  pages = {75--122},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9780511564932.004},
  urldate = {2023-09-11},
  isbn = {978-0-521-58291-9 978-0-511-56493-2},
  langid = {english},
  note = {https://www.cambridge.org/core/product/identifier/CBO9780511564932A025/type/book\_part},
  timestamp = {2023-09-11T14:56:04Z},
  file = {Puetter - 1997 - Modern Methods of Image Reconstruction.pdf:/Users/simonmartin/Zotero/storage/VPB9N7KT/Puetter - 1997 - Modern Methods of Image Reconstruction.pdf:application/pdf}
}

@article{puetterPixonbasedMultiresolutionImage1995,
  title = {Pixon-Based Multiresolution Image Reconstruction and the Quantification of Picture Information Content},
  author = {Puetter, R. C.},
  year = 1995,
  journal = {International Journal of Imaging Systems and Technology},
  volume = {6},
  number = {4},
  pages = {314--331},
  issn = {08999457, 10981098},
  doi = {10.1002/ima.1850060405},
  urldate = {2023-09-02},
  abstract = {This article reviews pixon-based image reconstruction, which in its current formulation uses a multiresolution language to quantify an image's algorithmic information content (AIC) using Bayesian techniques. Each pixon (or its generalization, the informaton) represents a fundamentalquanta of an image'sAIC, and an image's pixon basis represents the minimum degrees of freedom necessary to describe the image within the accuracy of the noise. We demonstrate with a number of examples that pixon-based image reconstruction yields results consistently superior to popular competing methods, including maximum likelihood and maximum entropy methods. Typical improvements include higher spatial resolution, greater sensitivity to faint sources, and immunity to the production of spurious sources and signal correlated residuals. Finally, we show how the pixon provides a generalization of the Akaike information criterion, and how it relates to concepts of ''coarse graining'' and the role of the Heisenberg uncertainty principle in statistical mechanics, provides a mechanism for optimal data compression, and represents a more optimal basis for image compression or reconstruction than wavelets. 01995 John Wiley \& Sons, Inc.},
  langid = {english},
  note = {https://onlinelibrary.wiley.com/doi/10.1002/ima.1850060405},
  timestamp = {2023-09-02T19:36:13Z},
  file = {Puetter - 1995 - Pixon-based multiresolution image reconstruction a.pdf:/Users/simonmartin/Zotero/storage/WLQ7PMAZ/Puetter - 1995 - Pixon-based multiresolution image reconstruction a.pdf:application/pdf}
}

@article{puetterPixonbasedMultiresolutionImage1995a,
  title = {Pixon-Based Multiresolution Image Reconstruction and the Quantification of Picture Information Content},
  author = {Puetter, R. C.},
  year = 1995,
  journal = {International Journal of Imaging Systems and Technology},
  volume = {6},
  number = {4},
  pages = {314--331},
  issn = {08999457, 10981098},
  doi = {10.1002/ima.1850060405},
  urldate = {2023-09-11},
  abstract = {This article reviews pixon-based image reconstruction, which in its current formulation uses a multiresolution language to quantify an image's algorithmic information content (AIC) using Bayesian techniques. Each pixon (or its generalization, the informaton) represents a fundamentalquanta of an image'sAIC, and an image's pixon basis represents the minimum degrees of freedom necessary to describe the image within the accuracy of the noise. We demonstrate with a number of examples that pixon-based image reconstruction yields results consistently superior to popular competing methods, including maximum likelihood and maximum entropy methods. Typical improvements include higher spatial resolution, greater sensitivity to faint sources, and immunity to the production of spurious sources and signal correlated residuals. Finally, we show how the pixon provides a generalization of the Akaike information criterion, and how it relates to concepts of ''coarse graining'' and the role of the Heisenberg uncertainty principle in statistical mechanics, provides a mechanism for optimal data compression, and represents a more optimal basis for image compression or reconstruction than wavelets. 01995 John Wiley \& Sons, Inc.},
  langid = {english},
  note = {https://onlinelibrary.wiley.com/doi/10.1002/ima.1850060405},
  timestamp = {2023-09-11T14:55:54Z},
  file = {Puetter - 1995 - Pixon-based multiresolution image reconstruction a.pdf:/Users/simonmartin/Zotero/storage/NAAXQVMS/Puetter - 1995 - Pixon-based multiresolution image reconstruction a.pdf:application/pdf}
}

@misc{puetterPixonMethodImage1999,
  title = {The {{Pixon}} Method of Image Reconstruction},
  author = {Puetter, Richard C. and Yahil, Amos},
  year = {1999},
  month = jan,
  number = {arXiv:astro-ph/9901063},
  eprint = {astro-ph/9901063},
  publisher = {arXiv},
  doi = {10.48550/arXiv.astro-ph/9901063},
  urldate = {2023-09-02},
  abstract = {The Pixon method is a high-performance, nonlinear image reconstruction method that provides statistically unbiased photometry and robust rejection of spurious sources. Relative to other methods, it can increase linear spatial resolution by a factor of a few and sensitivity by an order of magnitude or more. All of these benefits are achieved in computation times that can be orders of magnitude faster than its best competitors.},
  archiveprefix = {arxiv},
  howpublished = {http://arxiv.org/abs/astro-ph/9901063},
  keywords = {Astrophysics},
  timestamp = {2023-09-02T18:53:45Z},
  file = {Puetter_Yahil_1999_The Pixon method of image reconstruction.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Puetter_Yahil_1999_The Pixon method of image reconstruction.pdf:application/pdf;arXiv.org Snapshot:/Users/simonmartin/Zotero/storage/VUBKHUG4/9901063.html:text/html}
}

@book{rasmussenGaussianProcessesMachine2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  year = {2006},
  series = {Adaptive Computation and Machine Learning},
  publisher = {MIT Press},
  address = {Cambridge, Mass},
  isbn = {978-0-262-18253-9},
  langid = {english},
  lccn = {QA274.4 .R37 2006},
  keywords = {Data processing,Gaussian processes,Machine learning,Mathematical models},
  annotation = {OCLC: ocm61285753},
  timestamp = {2023-04-03T08:25:30Z},
  file = {Rasmussen_Williams_2006_Gaussian processes for machine learning.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Rasmussen_Williams_2006_Gaussian processes for machine learning.pdf:application/pdf}
}

@article{richardsonBayesianApproachMeasurement1993,
  title = {A {{Bayesian Approach}} to {{Measurement Error Problems}} in {{Epidemiology Using Conditional Independence Models}}},
  author = {Richardson, Sylvia and Gilks, Walter R.},
  year = {1993},
  month = sep,
  journal = {American Journal of Epidemiology},
  volume = {138},
  number = {6},
  pages = {430--442},
  issn = {1476-6256, 0002-9262},
  doi = {10.1093/oxfordjournals.aje.a116875},
  urldate = {2023-05-15},
  langid = {english},
  note = {https://academic.oup.com/aje/article/104939/A},
  timestamp = {2023-05-15T09:53:10Z},
  file = {Richardson_Gilks_1993_A Bayesian Approach to Measurement Error Problems in Epidemiology Using.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Richardson_Gilks_1993_A Bayesian Approach to Measurement Error Problems in Epidemiology Using.pdf:application/pdf}
}

@book{robertBayesianIdeasData2012,
  title = {Bayesian {{Ideas}} and {{Data Analysis}}},
  author = {Robert, Christian and Christensen, Ronald and Johnson, Wesley and Branscum, Adam and Hanson, Timothy},
  year = {2012},
  volume = {25},
  doi = {10.1080/09332480.2012.685376},
  abstract = {In this book our intention is to emphasize: I. That a primary role of statistics in society is to provide appropriate tools for addressing scientific questions. II. That appropriate statistical analysis of data involves a collaborative effort between subject mat- ter scientists and statisticians. III. That it is both appropriate and necessary to incorporate the scientist's expertise into making decisions related to the data. IV. That foundational issues matter in statistics. V. That prediction is of fundamental importance. Science and technology are about making accurate predictions. They are about being able to say that in certain circumstances, certain things will happen. Objectivity and understanding are only of permanent scientific value when they lead to testably accurate predictions.},
  isbn = {978-1-4398-0355-4},
  note = {http://www.tandfonline.com/doi/full/10.1080/09332480.2012.685376},
  timestamp = {2023-03-29T12:58:58Z},
  file = {Robert et al_2012_Bayesian Ideas and Data Analysis.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Robert et al_2012_Bayesian Ideas and Data Analysis.pdf:application/pdf}
}

@misc{robertHaroldJeffreysTheory2008,
  title = {Harold {{Jeffreys}}'s {{Theory}} of {{Probability Revisited}}},
  author = {Robert, Christian P. and Chopin, Nicolas and Rousseau, Judith},
  year = {2008},
  month = apr,
  journal = {arXiv.org},
  doi = {10.1214/09-STS284},
  urldate = {2023-12-20},
  abstract = {Published exactly seventy years ago, Jeffreys's Theory of Probability (1939) has had a unique impact on the Bayesian community and is now considered to be one of the main classics in Bayesian Statistics as well as the initiator of the objective Bayes school. In particular, its advances on the derivation of noninformative priors as well as on the scaling of Bayes factors have had a lasting impact on the field. However, the book reflects the characteristics of the time, especially in terms of mathematical rigor. In this paper we point out the fundamental aspects of this reference work, especially the thorough coverage of testing problems and the construction of both estimation and testing noninformative priors based on functional divergences. Our major aim here is to help modern readers in navigating in this difficult text and in concentrating on passages that are still relevant today.},
  howpublished = {https://arxiv.org/abs/0804.3173v7},
  langid = {english},
  timestamp = {2023-12-20T15:07:02Z},
  file = {Robert et al_2008_Harold Jeffreys's Theory of Probability Revisited.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Robert et al_2008_Harold Jeffreys's Theory of Probability Revisited.pdf:application/pdf}
}

@article{robertHaroldJeffreysTheory2009,
  title = {Harold {{Jeffreys}}'s {{Theory}} of {{Probability Revisited}}},
  author = {Robert, Christian P. and Chopin, Nicolas and Rousseau, Judith},
  year = {2009},
  month = may,
  journal = {Statistical Science},
  volume = {24},
  number = {2},
  eprint = {0804.3173},
  primaryclass = {math, stat},
  issn = {0883-4237},
  doi = {10.1214/09-STS284},
  urldate = {2023-12-20},
  abstract = {Published exactly seventy years ago, Jeffreys's Theory of Probability (1939) has had a unique impact on the Bayesian community and is now considered to be one of the main classics in Bayesian Statistics as well as the initiator of the objective Bayes school. In particular, its advances on the derivation of noninformative priors as well as on the scaling of Bayes factors have had a lasting impact on the field. However, the book reflects the characteristics of the time, especially in terms of mathematical rigor. In this paper we point out the fundamental aspects of this reference work, especially the thorough coverage of testing problems and the construction of both estimation and testing noninformative priors based on functional divergences. Our major aim here is to help modern readers in navigating in this difficult text and in concentrating on passages that are still relevant today.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Mathematics - History and Overview,Mathematics - Statistics Theory},
  note = {http://arxiv.org/abs/0804.3173},
  timestamp = {2023-12-20T15:09:50Z},
  file = {Robert et al. - 2009 - Harold Jeffreys's Theory of Probability Revisited.pdf:/Users/simonmartin/Zotero/storage/2MUUQ4ZA/Robert et al. - 2009 - Harold Jeffreys's Theory of Probability Revisited.pdf:application/pdf}
}

@article{russelModelSelectionParameter2019,
  title = {Model {{Selection}} and {{Parameter Inference}} in {{Phylogenetics Using Nested Sampling}}},
  author = {Russel, Patricio Maturana and Brewer, Brendon J. and Klaere, Steffen and Bouckaert, Remco R.},
  year = {2019},
  journal = {Systematic Biology},
  volume = {68},
  number = {2},
  eprint = {26753207},
  eprinttype = {jstor},
  pages = {219--233},
  publisher = {[Oxford University Press, Society of Systematic Biologists]},
  issn = {1063-5157},
  urldate = {2023-05-04},
  abstract = {Bayesian inference methods rely on numerical algorithms for both model selection and parameter inference. In general, these algorithms require a high computational effort to yield reliable estimates. One of the major challenges in phylogenetics is the estimation of the marginal likelihood. This quantity is commonly used for comparing different evolutionary models, but its calculation, even for simple models, incurs high computational cost. Another interesting challenge relates to the estimation of the posterior distribution. Often, long Markov chains are required to get sufficient samples to carry out parameter inference, especially for tree distributions. In general, these problems are addressed separately by using different procedures. Nested sampling (NS) is a Bayesian computation algorithm, which provides the means to estimate marginal likelihoods together with their uncertainties, and to sample fromthe posterior distribution at no extra cost. The methods currently used in phylogenetics for marginal likelihood estimation lack in practicality due to their dependence on many tuning parameters and their inability of most implementations to provide a direct way to calculate the uncertainties associated with the estimates, unlike NS. In this article, we introduce NS to phylogenetics. Its performance is analysed under different scenarios and compared to established methods. We conclude that NS is a competitive and attractive algorithm for phylogenetic inference. An implementation is available as a package for BEAST 2 under the LGPL licence, accessible at https://github.com/BEAST2-Dev/nested-sampling.},
  note = {https://www.jstor.org/stable/26753207},
  timestamp = {2023-05-04T12:21:23Z},
  file = {Russel et al_2019_Model Selection and Parameter Inference in Phylogenetics Using Nested Sampling.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Russel et al_2019_Model Selection and Parameter Inference in Phylogenetics Using Nested Sampling.pdf:application/pdf}
}

@misc{salomoneUnbiasedConsistentNested2018,
  title = {Unbiased and {{Consistent Nested Sampling}} via {{Sequential Monte Carlo}}},
  author = {Salomone, Robert and South, Leah F. and Drovandi, Christopher C. and Kroese, Dirk P.},
  year = {2018},
  month = nov,
  number = {arXiv:1805.03924},
  eprint = {1805.03924},
  primaryclass = {stat},
  publisher = {arXiv},
  urldate = {2023-04-26},
  abstract = {We introduce a new class of sequential Monte Carlo methods called Nested Sampling via Sequential Monte Carlo (NS-SMC), which reframes the Nested Sampling method of Skilling (2006) in terms of sequential Monte Carlo techniques. This new framework allows convergence results to be obtained in the setting when Markov chain Monte Carlo (MCMC) is used to produce new samples. An additional benefit is that marginal likelihood estimates are unbiased. In contrast to NS, the analysis of NS-SMC does not require the (unrealistic) assumption that the simulated samples be independent. As the original NS algorithm is a special case of NS-SMC, this provides insights as to why NS seems to produce accurate estimates despite a typical violation of its assumptions. For applications of NS-SMC, we give advice on tuning MCMC kernels in an automated manner via a preliminary pilot run, and present a new method for appropriately choosing the number of MCMC repeats at each iteration. Finally, a numerical study is conducted where the performance of NS-SMC and temperature-annealed SMC is compared on several challenging and realistic problems. MATLAB code for our experiments is made available at https://github.com/LeahPrice/SMC-NS .},
  archiveprefix = {arxiv},
  howpublished = {http://arxiv.org/abs/1805.03924},
  keywords = {Statistics - Computation},
  timestamp = {2023-04-26T10:39:47Z},
  file = {arXiv Fulltext PDF:/Users/simonmartin/Zotero/storage/NDUWE64D/Salomone et al. - 2018 - Unbiased and Consistent Nested Sampling via Sequen.pdf:application/pdf;arXiv.org Snapshot:/Users/simonmartin/Zotero/storage/QZCFHFYD/1805.html:text/html}
}

@article{schnuerchMeaningfulComparisonsOrdinalScale,
  title = {Meaningful {{Comparisons}} with {{Ordinal-Scale Items}}},
  author = {Schnuerch, Martin and Haaf, Julia M and Sarafoglou, Alexandra and Rouder, Jeffrey N},
  abstract = {Ordinal-scale items---say items that assess agreement with a proposition on an ordinal rating scale from strongly disagree to strongly agree---are exceedingly popular in psychological research. A common research question concerns the comparison of response distributions on ordinal-scale items across conditions. In this context, there is often a lingering question of whether metric-level descriptions of the results and parametric tests are appropriate. We consider a different problem, perhaps one that supersedes the parametric-vs-nonparametric issue: When is it appropriate to reduce the comparison of two (ordinal) distributions to the comparison of simple summary statistics (e.g., measures of location)? In this paper, we provide a Bayesian modeling approach to help researchers perform meaningful comparisons of two response distributions and draw appropriate inferences from ordinal-scale items. We develop four statistical models that represent possible relationships between two distributions: an unconstrained model representing a complex, non-ordinal relationship, a nonparametric stochastic-dominance model, a parametric shift model, and a null model representing equivalence in distribution. We show how these models can be compared in light of data with Bayes factors and illustrate their usefulness with two real-world examples. We also provide a freely available web applet for researchers who wish to adopt the approach.},
  langid = {english},
  timestamp = {2023-08-21T13:04:40Z},
  file = {Schnuerch et al. - Meaningful Comparisons with Ordinal-Scale Items.pdf:/Users/simonmartin/Zotero/storage/5ACTVQIJ/Schnuerch et al. - Meaningful Comparisons with Ordinal-Scale Items.pdf:application/pdf}
}

@article{Sivia,
  title = {Bayesian {{Inductive Inference Maximum Entropy}} \& {{Neutron Scattering}}},
  author = {Sivia, Devinder Singh},
  journal = {Los Alamos Science},
  timestamp = {2011-06-22T10:52:50Z},
  file = {Sivia_Bayesian Inductive Inference Maximum Entropy & Neutron Scattering.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Sivia_Bayesian Inductive Inference Maximum Entropy & Neutron Scattering.pdf:application/pdf}
}

@article{Sivia1998,
  title = {The {{Bayesian}} Approach to Reflectivity Data},
  author = {Sivia, D and Webster, J.R.P.},
  year = {1998},
  month = jun,
  journal = {Physica B: Condensed Matter},
  volume = {248},
  number = {1-4},
  pages = {327--337},
  issn = {09214526},
  doi = {10.1016/S0921-4526(98)00259-2},
  keywords = {bayesian probability theory,experimental design,maximum entropy,model selection},
  note = {http://linkinghub.elsevier.com/retrieve/pii/S0921452698002592},
  timestamp = {2011-05-26T09:45:06Z},
  file = {Sivia_Webster_1998_The Bayesian approach to reflectivity data.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Sivia_Webster_1998_The Bayesian approach to reflectivity data3.pdf:application/pdf}
}

@book{Sivia2006,
  title = {Data Analysis - a Bayesian Tutorial},
  author = {Sivia, D. S. and Skilling, J.},
  year = {2006},
  series = {Oxford Science Publications},
  edition = {2},
  publisher = {Oxford University Press},
  added-at = {2019-03-11T21:00:05.000+0100},
  citeulike-article-id = {1731310},
  interhash = {967121cedc716b9bfd7a098f9b59590e},
  intrahash = {72f2fe4eff9fbfb391f8d72f8dd93872},
  posted-at = {2012-10-30 12:06:16},
  priority = {2},
  keywords = {*file-import-13-09-19 bayesian},
  timestamp = {2019-03-11T21:06:37.000+0100}
}

@article{siviaIntroductionBayesianModel1993b,
  title = {An Introduction to {{Bayesian}} Model Selection},
  author = {Sivia, D.S. and David, W.I.F. and Knight, K.S. and Gull, S.F.},
  year = {1993},
  month = jun,
  journal = {Physica D: Nonlinear Phenomena},
  volume = {66},
  number = {1-2},
  pages = {234--242},
  issn = {01672789},
  doi = {10.1016/0167-2789(93)90241-R},
  urldate = {2023-04-27},
  langid = {english},
  note = {https://linkinghub.elsevier.com/retrieve/pii/016727899390241R},
  timestamp = {2023-04-27T16:17:13Z},
  file = {Sivia et al. - 1993 - An introduction to Bayesian model selection.pdf:/Users/simonmartin/Zotero/storage/MVR4WXB7/Sivia et al. - 1993 - An introduction to Bayesian model selection.pdf:application/pdf}
}

@techreport{Skilling2004,
  title = {Bayessys and {{MassInf}}},
  author = {Skilling, John},
  year = {2004},
  timestamp = {2011-07-14T13:46:23Z},
  file = {Skilling_2004_Bayessys and MassInf.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Skilling_2004_Bayessys and MassInf.pdf:application/pdf}
}

@article{skillingAXIOMSMAIMUMENTROPYa,
  title = {{{THE AXIOMS OF MAIMUM ENTROPY}}},
  author = {Skilling, John},
  abstract = {Maximum entropy is presented as a universal method of finding a "best" positive distribution constrained by incomplete data. The generalised entropy E (f - m - flog ({\pounds}1m) ) ) is the only form which selects acceptable distributions f in particular cases. It holds even if f is not normalised, so that maximum entropy applies directly to physical distributions other than probabilities. Furthermore, maximum entropy should also be used to select "best" parameters if the underlying mopel m has such freedom.},
  langid = {english},
  keywords = {No DOI found},
  timestamp = {2024-03-06T16:44:17Z},
  file = {Skilling - THE AXIOMS OF MAIMUM ENTROPY.pdf:/Users/simonmartin/Zotero/storage/IPKCUBGE/Skilling - THE AXIOMS OF MAIMUM ENTROPY.pdf:application/pdf}
}

@incollection{skillingClassicMaximumEntropy1989,
  title = {Classic {{Maximum Entropy}}},
  booktitle = {Maximum {{Entropy}} and {{Bayesian Methods}}},
  author = {Skilling, John},
  editor = {Skilling, J.},
  year = {1989},
  pages = {45--52},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-015-7860-8_3},
  urldate = {2023-08-21},
  abstract = {This paper presents a fully Bayesian derivation of maximum entropy image reconstruction. The argument repeatedly goes from the particular to the general, in that if there are general theories then they must apply to special cases. Two. such special cases, formalised as the "Cox axioms" , lead to the well-known fact that Bayesian probability theory is the only consistent language of inference. Further cases, formalised as the axioms of maximum entropy, show that the prior probability distribution for any positive, additive distribution must be monotonic in the entropy. Finally, a quantified special case shows that this monotonic function must be the exponential, leaving only a single dimensional scaling factor to be determined a posteriori. Many types of distribution, including probability distributions themselves, are positive and additive, so the entropy exponential is very general. The following paper (Gull 1989) applies these ideas to image reconstruction, showing how a sophisticated treatment can incorporate prior expectation of spatial correlations.},
  isbn = {978-90-481-4044-2 978-94-015-7860-8},
  langid = {english},
  note = {http://link.springer.com/10.1007/978-94-015-7860-8\_3},
  timestamp = {2023-08-21T14:44:30Z},
  file = {Skilling - 1989 - Classic Maximum Entropy.pdf:/Users/simonmartin/Zotero/storage/5XT97LF2/Skilling - 1989 - Classic Maximum Entropy.pdf:application/pdf}
}

@incollection{skillingClassicMaximumEntropy1989a,
  title = {Classic {{Maximum Entropy}}},
  booktitle = {Maximum {{Entropy}} and {{Bayesian Methods}}},
  author = {Skilling, John},
  editor = {Skilling, J.},
  year = {1989},
  pages = {45--52},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-015-7860-8_3},
  urldate = {2024-03-06},
  abstract = {This paper presents a fully Bayesian derivation of maximum entropy image reconstruction. The argument repeatedly goes from the particular to the general, in that if there are general theories then they must apply to special cases. Two. such special cases, formalised as the "Cox axioms" , lead to the well-known fact that Bayesian probability theory is the only consistent language of inference. Further cases, formalised as the axioms of maximum entropy, show that the prior probability distribution for any positive, additive distribution must be monotonic in the entropy. Finally, a quantified special case shows that this monotonic function must be the exponential, leaving only a single dimensional scaling factor to be determined a posteriori. Many types of distribution, including probability distributions themselves, are positive and additive, so the entropy exponential is very general. The following paper (Gull 1989) applies these ideas to image reconstruction, showing how a sophisticated treatment can incorporate prior expectation of spatial correlations.},
  isbn = {978-90-481-4044-2 978-94-015-7860-8},
  langid = {english},
  note = {http://link.springer.com/10.1007/978-94-015-7860-8\_3},
  timestamp = {2024-03-06T16:44:20Z},
  file = {Skilling - 1989 - Classic Maximum Entropy.pdf:/Users/simonmartin/Zotero/storage/83XMN2F6/Skilling - 1989 - Classic Maximum Entropy.pdf:application/pdf}
}

@article{skillingNestedSamplingGeneral2006,
  title = {Nested Sampling for General {{Bayesian}} Computation},
  author = {Skilling, John},
  year = {2006},
  month = dec,
  journal = {Bayesian Analysis},
  volume = {1},
  number = {4},
  pages = {833--859},
  publisher = {International Society for Bayesian Analysis},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/06-BA127},
  urldate = {2023-04-25},
  abstract = {Nested sampling estimates directly how the likelihood function relates to prior mass. The evidence (alternatively the marginal likelihood, marginal density of the data, or the prior predictive) is immediately obtained by summation. It is the prime result of the computation, and is accompanied by an estimate of numerical uncertainty. Samples from the posterior distribution are an optional by-product, obtainable for any temperature. The method relies on sampling within a hard constraint on likelihood value, as opposed to the softened likelihood of annealing methods. Progress depends only on the shape of the "nested" contours of likelihood, and not on the likelihood values. This invariance (over monotonic re-labelling) allows the method to deal with a class of phase-change problems which effectively defeat thermal annealing.},
  keywords = {algorithm,annealing,Bayesian computation,evidence,marginal likelihood,Model selection,nest,phase change},
  note = {https://projecteuclid.org/journals/bayesian-analysis/volume-1/issue-4/Nested-sampling-for-general-Bayesian-computation/10.1214/06-BA127.full},
  timestamp = {2023-04-25T15:28:32Z},
  file = {Full Text PDF:/Users/simonmartin/Zotero/storage/EUYVNKJF/Skilling - 2006 - Nested sampling for general Bayesian computation.pdf:application/pdf}
}

@article{skillingProbabilisticDataAnalysis1998,
  title = {Probabilistic Data Analysis: An Introductory Guide},
  author = {Skilling, J.},
  year = {1998},
  month = apr,
  journal = {Journal of Microscopy},
  volume = {190},
  number = {1-2},
  pages = {28--36},
  issn = {0022-2720},
  doi = {10.1046/j.1365-2818.1998.2780835.x},
  keywords = {algorithm,bayes,c,gibbs,image reconstruction,inference,journal of microscopy,metropolis,probability,simulated annealing,uncertainty},
  note = {http://doi.wiley.com/10.1046/j.1365-2818.1998.2780835.x},
  timestamp = {2019-04-11T12:44:37Z},
  file = {Skilling_1998_Probabilistic data analysis.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Skilling_1998_Probabilistic data analysis.pdf:application/pdf}
}

@article{speagleDynestyDynamicNested2019,
  title = {Dynesty: {{A Dynamic Nested Sampling Package}} for {{Estimating Bayesian Posteriors}} and {{Evidences}}},
  author = {Speagle, Joshua S.},
  year = {2019},
  month = apr,
  doi = {10.1093/mnras/staa278},
  abstract = {We present dynesty, a public, open-source, Python package to estimate Bayesian posteriors and evidences (marginal likelihoods) using Dynamic Nested Sampling. By adaptively allocating samples based on posterior structure, Dynamic Nested Sampling has the benefits of Markov Chain Monte Carlo algorithms that focus exclusively on posterior estimation while retaining Nested Sampling's ability to estimate evidences and sample from complex, multi-modal distributions. We provide an overview of Nested Sampling, its extension to Dynamic Nested Sampling, the algorithmic challenges involved, and the various approaches taken to solve them. We then examine dynesty's performance on a variety of toy problems along with several astronomical applications. We find in particular problems dynesty can provide substantial improvements in sampling efficiency compared to popular MCMC approaches in the astronomical literature. More detailed statistical results related to Nested Sampling are also included in the Appendix.},
  note = {http://arxiv.org/abs/1904.02180 http://dx.doi.org/10.1093/mnras/staa278},
  timestamp = {2023-03-24T12:15:06Z},
  file = {Speagle_2019_dynesty.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Speagle_2019_dynesty.pdf:application/pdf}
}

@article{St1968,
  title = {Laplace's 1774 {{Memoir}} on {{Inverse Probability}}},
  author = {Stigler, Stephen M.},
  year = {1968},
  journal = {Statistical Science},
  volume = {1},
  number = {3},
  pages = {359--379},
  timestamp = {2023-12-21T15:13:46Z},
  file = {Stigler_1968_Laplace's 1774 Memoir on Inverse Probability.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Stigler_1968_Laplace's 1774 Memoir on Inverse Probability2.pdf:application/pdf}
}

@article{stiglerGaussInventionLeast1981,
  title = {Gauss and the {{Invention}} of {{Least Squares}}},
  author = {Stigler, Stephen M.},
  year = {1981},
  month = may,
  journal = {The Annals of Statistics},
  volume = {9},
  number = {3},
  issn = {0090-5364},
  doi = {10.1214/aos/1176345451},
  urldate = {2023-08-08},
  note = {https://projecteuclid.org/journals/annals-of-statistics/volume-9/issue-3/Gauss-and-the-Invention-of-Least-Squares/10.1214/aos/1176345451.full},
  timestamp = {2023-08-08T07:55:30Z},
  file = {1176345451.pdf:/Users/simonmartin/Zotero/storage/PK3MT63U/1176345451.pdf:application/pdf}
}

@article{surowiecGeneralizedSubsetDesigns2017,
  title = {Generalized {{Subset Designs}} in {{Analytical Chemistry}}},
  author = {Surowiec, Izabella and Vikstr{\"o}m, Ludvig and Hector, Gustaf and Johansson, Erik and Vikstr{\"o}m, Conny and Trygg, Johan},
  year = {2017},
  month = jun,
  journal = {Analytical Chemistry},
  volume = {89},
  number = {12},
  pages = {6491--6497},
  publisher = {American Chemical Society},
  issn = {0003-2700},
  doi = {10.1021/acs.analchem.7b00506},
  urldate = {2024-01-22},
  abstract = {Design of experiments (DOE) is an established methodology in research, development, manufacturing, and production for screening, optimization, and robustness testing. Two-level fractional factorial designs remain the preferred approach due to high information content while keeping the number of experiments low. These types of designs, however, have never been extended to a generalized multilevel reduced design type that would be capable to include both qualitative and quantitative factors. In this Article we describe a novel generalized fractional factorial design. In addition, it also provides complementary and balanced subdesigns analogous to a fold-over in two-level reduced factorial designs. We demonstrate how this design type can be applied with good results in three different applications in analytical chemistry including (a) multivariate calibration using microwave resonance spectroscopy for the determination of water in tablets, (b) stability study in drug product development, and (c) representative sample selection in clinical studies. This demonstrates the potential of generalized fractional factorial designs to be applied in many other areas of analytical chemistry where representative, balanced, and complementary subsets are required, especially when a combination of quantitative and qualitative factors at multiple levels exists.},
  note = {https://doi.org/10.1021/acs.analchem.7b00506},
  timestamp = {2024-01-22T16:33:43Z}
}

@article{TerBraak2006a,
  title = {A {{Markov Chain Monte Carlo}} Version of the Genetic Algorithm {{Differential Evolution}}: {{Easy Bayesian}} Computing for Real Parameter Spaces},
  author = {Ter Braak, Cajo J F},
  year = {2006},
  journal = {Statistics and Computing},
  volume = {16},
  number = {3},
  pages = {239--249},
  issn = {09603174},
  doi = {10.1007/s11222-006-8769-1},
  abstract = {Differential Evolution (DE) is a simple genetic al- gorithm for numerical optimization in real parameter spaces. In a statistical context one would not just want the optimum but also its uncertainty. The uncertainty distribution can be obtained by a Bayesian analysis (after specifying prior and likelihood) using Markov Chain Monte Carlo (MCMC) sim- ulation. This paper integrates the essential ideas of DE and MCMC, resulting in Differential Evolution Markov Chain (DE-MC). DE-MC is a population MCMC algorithm, in which multiple chains are run in parallel. DE-MC solves an important problem inMCMC,namely that of choosing an appropriate scale and orientation for the jumping distribu- tion. In DE-MC the jumps are simply a fixed multiple of the differences of two random parameter vectors that are cur- rently in the population. The selection process of DE-MC works via the usual Metropolis ratio which defines the prob- ability with which a proposal is accepted. In tests with known uncertainty distributions, the efficiency of DE-MC with re- spect to random walk Metropolis with optimal multivariate Normal jumps ranged from 68\% for small population sizes to 100\% for large population sizes and even to 500\% for the 97.5\% point of a variable from a 50-dimensional Student distribution. Two Bayesian examples illustrate the potential of DE-MC in practice. DE-MC is shown to facilitate mul- tidimensional updates in a multi-chain ``Metropolis-within- Gibbs'' sampling approach. The advantage of DE-MC over conventionalMCMCare simplicity, speed of calculation and convergence, even for nearly collinear parameters and mul- timodal densities.},
  keywords = {Block updating,Evolutionary Monte Carlo,Metropolis algorithm,Population Markov Chain Monte Carlo,Simulated Annealing,Simulated Tempering,Theophylline Kinetics},
  timestamp = {2019-08-08T11:46:51Z},
  file = {s11222-006-8769-1.pdf:/Users/simonmartin/Zotero/storage/E8DDQEGB/s11222-006-8769-1.pdf:application/pdf}
}

@misc{ThinkBayesThink,
  title = {Think {{Bayes}} 2 --- {{Think Bayes}}},
  urldate = {2024-01-11},
  howpublished = {http://allendowney.github.io/ThinkBayes2/index.html},
  timestamp = {2024-01-11T14:22:19Z},
  file = {Think Bayes 2 — Think Bayes:/Users/simonmartin/Zotero/storage/YBX77UK2/index.html:text/html}
}

@article{trottaBayesSkyBayesian2008,
  title = {Bayes in the Sky: {{Bayesian}} Inference and Model Selection in Cosmology},
  shorttitle = {Bayes in the Sky},
  author = {Trotta, Roberto},
  year = {2008},
  month = mar,
  journal = {Contemporary Physics},
  volume = {49},
  number = {2},
  pages = {71--104},
  publisher = {Taylor \& Francis},
  issn = {0010-7514},
  doi = {10.1080/00107510802066753},
  urldate = {2023-08-21},
  abstract = {The application of Bayesian methods in cosmology and astrophysics has flourished over the past decade, spurred by data sets of increasing size and complexity. In many respects, Bayesian methods have proven to be vastly superior to more traditional statistical tools, offering the advantage of higher efficiency and of a consistent conceptual basis for dealing with the problem of induction in the presence of uncertainty. This trend is likely to continue in the future, when the way we collect, manipulate and analyse observations and compare them with theoretical models will assume an even more central role in cosmology. This review is an introduction to Bayesian methods in cosmology and astrophysics and recent results in the field. I first present Bayesian probability theory and its conceptual underpinnings, Bayes' Theorem and the role of priors. I discuss the problem of parameter inference and its general solution, along with numerical techniques such as Monte Carlo Markov Chain methods. I then review the theory and application of Bayesian model comparison, discussing the notions of Bayesian evidence and effective model complexity, and how to compute and interpret those quantities. Recent developments in cosmological parameter extraction and Bayesian cosmological model building are summarised, highlighting the challenges that lie ahead.},
  keywords = {Bayesian methods,cosmology,data analysis,model comparison,parameter inference,statistical methods},
  note = {https://doi.org/10.1080/00107510802066753},
  timestamp = {2023-08-21T13:50:20Z},
  file = {Trotta_2008_Bayes in the sky.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Trotta_2008_Bayes in the sky.pdf:application/pdf}
}

@article{vandenberghTutorialConductingInterpreting2020,
  title = {{A Tutorial on Conducting and Interpreting a~Bayesian ANOVA in JASP}},
  author = {{van den Bergh}, Don and {van Doorn}, Johnny and Marsman, Maarten and Draws, Tim and {van Kesteren}, Erik-Jan and Derks, Koen and Dablander, Fabian and Gronau, Quentin F. and Kucharsk{\'y}, {\v S}imon and Gupta, Akash R. Komarlu Narendra and Sarafoglou, Alexandra and Voelkel, Jan G. and Stefan, Angelika and Ly, Alexander and Hinne, Max and Matzke, Dora and Wagenmakers, Eric-Jan},
  year = {2020},
  journal = {L'Ann{\'e}e psychologique},
  volume = {120},
  number = {1},
  pages = {73--96},
  publisher = {Presses Universitaires de France},
  address = {Paris cedex 14},
  issn = {0003-5033},
  doi = {10.3917/anpsy1.201.0073},
  urldate = {2023-08-07},
  abstract = {L'analyse de variance (ANOVA) est la proc{\'e}dure standard utilis{\'e}e pour l'inf{\'e}rence statistique dans les plans factoriels. En r{\`e}gle g{\'e}n{\'e}rale, les analyses de variance sont ex{\'e}cut{\'e}es {\`a} l'aide de statistiques fr{\'e}quentistes, o{\`u} les valeurs p d{\'e}terminent la significativit{\'e} statistique en termes de <<~tout ou rien~>>. Ces derni{\`e}res ann{\'e}es, l'approche bay{\'e}sienne de la statistique inf{\'e}rentielle est de plus en plus consid{\'e}r{\'e}e comme une alternative l{\'e}gitime {\`a} la valeur p. Toutefois, l'adoption g{\'e}n{\'e}ralis{\'e}e des statistiques bay{\'e}siennes, et en particulier de l'ANOVA bay{\'e}sienne, est limit{\'e}e par le fait que les concepts bay{\'e}siens sont rarement enseign{\'e}s dans les cours de statistiques appliqu{\'e}es. Par cons{\'e}quent, les utilisateurs peuvent ne pas savoir comment effectuer une ANOVA bay{\'e}sienne et en interpr{\'e}ter les r{\'e}sultats. Nous fournissons ici un guide pour r{\'e}aliser et interpr{\'e}ter une ANOVA bay{\'e}sienne avec JASP, un logiciel statistique open-source ayant une interface utilisateur graphique. Nous expliquons les concepts cl{\'e}s de l'ANOVA bay{\'e}sienne {\`a} l'aide de deux exemples empiriques.},
  langid = {french},
  keywords = {analyse de la variance,distribution posterieure,facteur de Bayes,JASP,test d'hypothese,tutoriel},
  note = {https://www.cairn.info/revue-l-annee-psychologique-2020-1-page-73.htm},
  timestamp = {2023-08-07T16:16:08Z},
  file = {van den Bergh et al_2020_A Tutorial on Conducting and Interpreting a Bayesian ANOVA in JASP.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/van den Bergh et al_2020_A Tutorial on Conducting and Interpreting a Bayesian ANOVA in JASP.pdf:application/pdf;Snapshot:/Users/simonmartin/Zotero/storage/JC5VAX46/revue-l-annee-psychologique-2020-1-page-73.html:text/html}
}

@article{vandongenBayesianPerspectiveSeverity,
  title = {A {{Bayesian Perspective}} on {{Severity}}: {{Risky Predictions}} and {{Specific Hypotheses}}},
  author = {{van Dongen}, Noah and Sprenger, Jan and Wagenmakers, Eric-Jan},
  abstract = {A tradition that goes back to Sir Karl R. Popper assesses the value of a statistical test primarily by its severity: was there an honest and stringent attempt to prove the tested hypothesis wrong? For ``error statisticians'' such as Mayo (1996, 2018), and frequentists more generally, severity is a key virtue in hypothesis tests. Conversely, failure to incorporate severity into statistical inference, as allegedly happens in Bayesian inference, counts as a major methodological shortcoming.},
  langid = {english},
  keywords = {Multiple DOI},
  timestamp = {2024-03-06T16:42:34Z},
  file = {van Dongen et al. - A Bayesian Perspective on Severity Risky Predicti.pdf:/Users/simonmartin/Zotero/storage/SUPC49ZJ/van Dongen et al. - A Bayesian Perspective on Severity Risky Predicti.pdf:application/pdf}
}

@article{vanravenzwaaijSimpleIntroductionMarkov2018,
  title = {A Simple Introduction to {{Markov Chain Monte}}--{{Carlo}} Sampling},
  author = {Van Ravenzwaaij, Don and Cassey, Pete and Brown, Scott D.},
  year = {2018},
  month = feb,
  journal = {Psychonomic Bulletin \& Review},
  volume = {25},
  number = {1},
  pages = {143--154},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-016-1015-8},
  urldate = {2023-12-15},
  abstract = {Markov Chain Monte--Carlo (MCMC) is an increasingly popular method for obtaining information about distributions, especially for estimating posterior distributions in Bayesian inference. This article provides a very basic introduction to MCMC sampling. It describes what MCMC is, and what it can be used for, with simple illustrative examples. Highlighted are some of the benefits and limitations of MCMC sampling, as well as different approaches to circumventing the limitations most likely to trouble cognitive scientists.},
  langid = {english},
  note = {http://link.springer.com/10.3758/s13423-016-1015-8},
  timestamp = {2023-12-15T12:41:18Z},
  file = {Van Ravenzwaaij et al. - 2018 - A simple introduction to Markov Chain Monte–Carlo .pdf:/Users/simonmartin/Zotero/storage/MZ5Y9BKN/Van Ravenzwaaij et al. - 2018 - A simple introduction to Markov Chain Monte–Carlo .pdf:application/pdf}
}

@misc{wagenmakersVisualizingEquationSample2023a,
  title = {Visualizing the {{Equation}} for the {{Sample Correlation Coefficient}}},
  author = {Wagenmakers, Eric-Jan and Godmann, Henrik R. and de Ruiter, Jan and van Doorn, Johnny},
  year = {2023},
  month = jun,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/gwu8b},
  urldate = {2023-08-10},
  abstract = {The equation for the Pearson correlation coefficient can be represented in a scatter plot as the difference in area between concordant and discordant rectangles, scaled by an area that represents the maximum possible concordance. Rarely employed in statistics textbooks, this simple visualizationmay facilitate a deeper understanding of the nature of the correlation coefficient.},
  howpublished = {https://psyarxiv.com/gwu8b/},
  langid = {american},
  keywords = {concordance,covariance,invariance,Quantitative Methods,Social and Behavioral Sciences,Statistical Methods,Teaching statistics},
  timestamp = {2023-08-10T14:37:17Z},
  file = {Wagenmakers et al_2023_Visualizing the Equation for the Sample Correlation Coefficient.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Wagenmakers et al_2023_Visualizing the Equation for the Sample Correlation Coefficient2.pdf:application/pdf}
}

@article{warrenQuantitativeCriticalThinking2018,
  title = {Quantitative Critical Thinking: {{Student}} Activities Using {{Bayesian}} Updating},
  shorttitle = {Quantitative Critical Thinking},
  author = {Warren, Aaron R.},
  year = {2018},
  month = may,
  journal = {American Journal of Physics},
  volume = {86},
  number = {5},
  pages = {368--380},
  issn = {0002-9505},
  doi = {10.1119/1.5012750},
  urldate = {2023-05-09},
  abstract = {One of the central roles of physics education is the development of students' ability to evaluate proposed hypotheses and models. This ability is important not just for students' understanding of physics but also to prepare students for future learning beyond physics. In particular, it is often hoped that students will better understand the manner in which physicists leverage the availability of prior knowledge to guide and constrain the construction of new knowledge. Here, we discuss how the use of Bayes' Theorem to update the estimated likelihood of hypotheses and models can help achieve these educational goals through its integration with evaluative activities that use hypothetico-deductive reasoning. Several types of classroom and laboratory activities are presented that engage students in the practice of Bayesian likelihood updating on the basis of either consistency with experimental data or consistency with pre-established principles and models. This approach is sufficiently simple for introductory physics students while offering a robust mechanism to guide relatively sophisticated student reflection concerning models, hypotheses, and problem-solutions. A quasi-experimental study utilizing algebra-based introductory courses is presented to assess the impact of these activities on student epistemological development. The results indicate gains on the Epistemological Beliefs Assessment for Physical Science (EBAPS) at a minimal cost of class-time.},
  note = {https://doi.org/10.1119/1.5012750},
  timestamp = {2023-05-09T11:39:04Z},
  file = {Full Text PDF:/Users/simonmartin/Zotero/storage/ZW2SJZA6/Warren - 2018 - Quantitative critical thinking Student activities.pdf:application/pdf;Snapshot:/Users/simonmartin/Zotero/storage/5DGRL8V8/Quantitative-critical-thinking-Student-activities.html:text/html}
}

@article{Werman2001,
  title = {A {{Bayesian Method}} for Fitting {{Parametric}} and {{Nonparametric Models}} to {{Noisy Data}}},
  author = {Werman, Michael and Keren, Daniel},
  year = {2001},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {23},
  number = {5},
  pages = {528--534},
  abstract = {We present a simple paradigm for fitting models, parametric and nonparametric, to noisy data, which resolves some of the problems associated with classical MSE algorithms. This is done by considering each point on the model as a possible source for each data point. The paradigm can be used to solve problems which are ill-posed in the classical MSE approach, such as fitting a segment (as opposed to a line). It is shown to be nonbiased and to achieve excellent results for general curves, even in the presence of strong discontinuities. Results are shown for a number of fitting problems, including lines, circles, elliptic arcs, segments, rectangles, and general curves, contaminated by Gaussian and uniform noise.},
  keywords = {Bayesian fitting,nonparametric models,parametric models},
  timestamp = {2023-03-29T12:48:43Z},
  file = {Werman_Keren_2001_A Bayesian Method for fitting Parametric and Nonparametric Models to Noisy Data.pdf:/Users/simonmartin/OneDrive - Loughborough University/ZoteroArchive/Werman_Keren_2001_A Bayesian Method for fitting Parametric and Nonparametric Models to Noisy Data.pdf:application/pdf}
}

@inproceedings{Yan2012,
  title = {A {{Bayesian}} Approach for Identification of Structural Crack Using Strain Measurements},
  booktitle = {Proceedings of the 6th {{European Workshop}} - {{Structural Health Monitoring}} 2012, {{EWSHM}} 2012},
  author = {Yan, G.},
  year = {2012},
  volume = {1},
  pages = {763--770},
  abstract = {This paper proposes a statistical approach for identifying crack in structure using strain measurements and Bayesian inference, in which uncertainties from modeling error and measurement noise are explicitly included. The Bayesian approach is a model-based method, the crack is first represented by a set of parameters, i.e., coordinates of the two endpoints of the crack. An array of strain sensors is mounted on the structure to gather strain measurements under a known static loading. A forward model based on extended finite element method (XFEM) characterizing the strain responses of the structure with crack is incorporated in the identification procedure. By combining the measurement data and the prior information, Bayes' Theorem is used to update the probability distributions of the parameters of crack. A Markov chain Monte Carlo (MCMC) algorithm is employed for sampling the parameters' posterior distributions. Numerical study is conducted to demonstrate the effectiveness of the proposed method.},
  isbn = {9783940283412},
  note = {http://www.ndt.net/?id=14088},
  timestamp = {2023-06-14T07:48:03Z},
  file = {Yan_2012_A Bayesian approach for identification of structural crack using strain.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Yan_2012_A Bayesian approach for identification of structural crack using strain.pdf:application/pdf}
}

@article{Zegler1991a,
  title = {Generalized Linear Models with Random Effects; {{A}} Gibbs Sampling Approach},
  author = {Zegler, S.C. and Karim, M. Rezaul},
  year = {1991},
  journal = {Journal of the American Statictical Society},
  volume = {86},
  number = {413},
  pages = {79--86},
  timestamp = {2014-03-05T14:50:10Z},
  file = {Zegler_Karim_1991_Generalized linear models with random effects\; A gibbs sampling approach.pdf:/Users/simonmartin/Library/CloudStorage/OneDrive-LoughboroughUniversity/ZoteroArchive/Zegler_Karim_1991_Generalized linear models with random effects\; A gibbs sampling approach2.pdf:application/pdf}
}
