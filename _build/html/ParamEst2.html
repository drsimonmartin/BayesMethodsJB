

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>A Brief Introduction to Parameter Estimation the Bayesian way &#8212; MPP001 Research Methods</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'ParamEst2';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="A (very) Brief Introduction to Model Selection the Bayesian way" href="ModelSelection.html" />
    <link rel="prev" title="Probability and Bayes’ theorem" href="BayesTheorem.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="MPP001 Research Methods - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="MPP001 Research Methods - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Introduction to Bayesian Data Analysis
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="BayesTheorem.html">Probability and Bayes’ theorem</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">A Brief Introduction to Parameter Estimation the Bayesian way</a></li>
<li class="toctree-l1"><a class="reference internal" href="ModelSelection.html">A (very) Brief Introduction to Model Selection the Bayesian way</a></li>
<li class="toctree-l1"><a class="reference internal" href="PracticalBayes1.html">Getting Bayes Done</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/drsimonmartin/PythonTutorial" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/drsimonmartin/PythonTutorial/issues/new?title=Issue%20on%20page%20%2FParamEst2.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/ParamEst2.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>A Brief Introduction to Parameter Estimation the Bayesian way</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-first-application-of-bayes-theorem-the-fair-coin">A first application of Bayes’ Theorem – the fair coin</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-statistics">Summary statistics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#best-estimates">Best estimates</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reliability-of-fits">Reliability of fits</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-mode-and-median">Mean, mode, and median</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#credible-intervals">Credible intervals</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-use-and-the-choice-of-priors">The use and the choice of priors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix-coin-flipping-code">Appendix - coin flipping code:</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="a-brief-introduction-to-parameter-estimation-the-bayesian-way">
<h1>A Brief Introduction to Parameter Estimation the Bayesian way<a class="headerlink" href="#a-brief-introduction-to-parameter-estimation-the-bayesian-way" title="Permalink to this heading">#</a></h1>
<p>In engineering and science data analysis we often want to find the values of parameters in a model that best fit the data. A common example is fitting the slope and intercept in a straight line model. This chapter introduces how Bayes’ theorem can be used to achieve this parameter estimation. The examples used are chosen to make the process clear.</p>
<section id="a-first-application-of-bayes-theorem-the-fair-coin">
<h2>A first application of Bayes’ Theorem – the fair coin<a class="headerlink" href="#a-first-application-of-bayes-theorem-the-fair-coin" title="Permalink to this heading">#</a></h2>
<p>The first draft of these notes is being written at the time of the inaugral Las Vegas F1 Grand Prix (the street circuit, not the infamous cruise round the casino car-park of the early 1980s) so it seems apt to consider the fair coin problem.</p>
<p>On a visit to Las Vegas you are presented with a coin, it looks a little odd so you ask yourself the question “What is the probability that this coin is fair?” A fair coin is one that when tossed is unbiased - a long run of coin tosses will given on average the same number of heads (H) as tails (T).</p>
<p>In the following we will deal with the proportionality <a class="reference internal" href="BayesTheorem.html#equation-bayesprop">(5)</a> as this will allow us to see the bayesian updating process in action without overally complex computations being required. This is fine because the denominator of equation <a class="reference internal" href="BayesTheorem.html#equation-bayes">(4)</a> provides normalisation so the optimum value of a parameter will not be affected.</p>
<p>We begin by considering the model, or hypothesis, that we are interested in assessing. All we know is that the coin is from Las Vegas so there is a chance it may be biased - the proportion of heads generated by flipping the coin could vary from 0 (all tails) to 1 (all heads). We can model this in terms of a set of hypotheses that cover this range <span class="math notranslate nohighlight">\(H_i\)</span> (monotonically) and ask which of these is the most likely, <span class="math notranslate nohighlight">\(H_0\)</span>? If we then take a set of flips of the coin (data) we can find out which of the <span class="math notranslate nohighlight">\(H_i\)</span> best matches the data i.e. <span class="math notranslate nohighlight">\(H_0\)</span>. A <span class="math notranslate nohighlight">\(H_0\)</span> value of <span class="math notranslate nohighlight">\(0.5\)</span> corresponds to a completely fair coin.</p>
<p>It can be shown that that if the flips of the coin are independent the probability of obtaining <span class="math notranslate nohighlight">\(R\)</span> heads in <span class="math notranslate nohighlight">\(N\)</span> is given by:</p>
<div class="math notranslate nohighlight" id="equation-flipprob">
<span class="eqno">(7)<a class="headerlink" href="#equation-flipprob" title="Permalink to this equation">#</a></span>\[ \mathrm{prob}(\{data\}|H,I)\propto H^R(1-H)^{N-R}\]</div>
<p>This is a binomial distribution. It can be shown that this is the correct form for this probability. Wikipedia probably has a page on this should you feel the need to confirm this. However, hopefully it looks reasonable to you. <span class="math notranslate nohighlight">\(H\)</span> is the probability of getting a head and there are <span class="math notranslate nohighlight">\(R\)</span> of them and it follows that <span class="math notranslate nohighlight">\((1-H)\)</span> is the propability of getting a tail of which there have been <span class="math notranslate nohighlight">\((N-R)\)</span>.</p>
<p>We also need a <em>prior</em> for our model. Before any flips of the coin are made we have no reason to favour any particular value for <span class="math notranslate nohighlight">\(H\)</span> so could reasonably apply the same probability to each - this is known as a flat, or noninformative, prior–see plot below. In forthcoming calculations the range of <span class="math notranslate nohighlight">\(H_i\)</span> will be divided up into 100 segments - the flat prior value has been chosen so that the total probability will sum to one.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot of prior for fair coin problem</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">stats</span>



<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>  <span class="c1"># Create a figure containing a single axes.</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">0.01</span><span class="p">])</span>  <span class="c1"># Plot prior on the axes.</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Hypothesis&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Prior probability density&#39;</span><span class="p">)</span>
<span class="c1">#plt.show()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Prior probability density&#39;)
</pre></div>
</div>
<img alt="_images/92604ba68c8ec0057e7001dcf9f6e9cf007a7d156c577605d755a3db69c7aed1.png" src="_images/92604ba68c8ec0057e7001dcf9f6e9cf007a7d156c577605d755a3db69c7aed1.png" />
</div>
</div>
<p>Let’s assume the coin is tossed and returns a head. This means we have 1 head in 1 flip of the coin and equation <a class="reference internal" href="#equation-flipprob">(7)</a> gives us:</p>
<div class="math notranslate nohighlight">
\[\textrm{p}(\{\textrm{data=`H`\},|H,I})\propto H\]</div>
<p>The product of this with our flat prior probability will give us the posterior probability. This will be <span class="math notranslate nohighlight">\(\propto H\)</span> - a straight line starting from zero (probability of only tails) and reaching a maximum at <span class="math notranslate nohighlight">\(H=1\)</span> (probability of only heads). This tells us that the coin is most likely one that always gives heads, but it is not stongly in favour of this model.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;retina&#39;
<span class="n">success_true</span> <span class="o">=</span> <span class="mf">0.3</span> <span class="c1"># this is the &quot;true&quot; value for the odds of the coin giving a head</span>

<span class="n">RANDOM_SEED</span> <span class="o">=</span> <span class="mi">8927</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">RANDOM_SEED</span><span class="p">)</span>
<span class="n">az</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;arviz-darkgrid&quot;</span><span class="p">)</span>

<span class="n">grid_points</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># use lower number for testing</span>

<span class="k">def</span> <span class="nf">uniform_prior</span><span class="p">(</span><span class="n">grid_points</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns Uniform prior density</span>

<span class="sd">            Parameters:</span>
<span class="sd">                grid_points (numpy.array): Array of prior values</span>

<span class="sd">            Returns:</span>
<span class="sd">                density (numpy.array): Uniform density of prior values</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">grid_points</span><span class="p">)</span>


<span class="c1"># define grid</span>
<span class="n">p_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">grid_points</span><span class="p">)</span> <span class="c1"># these are the x values</span>

<span class="c1"># define prior</span>
<span class="n">prior</span> <span class="o">=</span> <span class="n">uniform_prior</span><span class="p">(</span><span class="n">grid_points</span><span class="p">)</span> <span class="c1"># y values </span>

<span class="c1"># compute likelihood at each point in the grid</span>
<span class="c1"># For single toss that gives 1 head, binomial function gives likelihood = H</span>
<span class="c1"># compute product of likelihood and prior</span>
<span class="c1">#unstd_posterior = likelihood * prior</span>
<span class="n">likelihood</span> <span class="o">=</span> <span class="n">p_grid</span><span class="o">*</span><span class="n">prior</span>
<span class="n">unstd_posterior</span> <span class="o">=</span> <span class="n">likelihood</span> <span class="o">*</span> <span class="n">prior</span>
<span class="c1"># standardize the posterior, so it sums to 1</span>
<span class="c1">#posterior = unstd_posterior / unstd_posterior.sum()</span>
<span class="c1">#return p_grid, posterior</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>  <span class="c1"># Create a figure containing a single axes.</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_grid</span><span class="p">,</span><span class="n">unstd_posterior</span><span class="p">)</span>  <span class="c1"># Plot prior on the axes.</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Hypothesis&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Probability&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Probability&#39;)
</pre></div>
</div>
<img alt="_images/b868cfba2b73e21eb1fadd9dc94ba4b542f247aa26adc0895edd4b07c0f4314b.png" src="_images/b868cfba2b73e21eb1fadd9dc94ba4b542f247aa26adc0895edd4b07c0f4314b.png" />
</div>
</div>
<p>A second coin toss also gives a head. Now we have <span class="math notranslate nohighlight">\(N=2\)</span> and <span class="math notranslate nohighlight">\(R=2\)</span> for <a class="reference internal" href="#equation-flipprob">(7)</a> and the prior is now <span class="math notranslate nohighlight">\(H\)</span>. The product of these gives us <span class="math notranslate nohighlight">\(\textrm{prob}\propto H^2\)</span>. This is even more strongly in favour of the heads-only scenario–this is not unreasonable as at this stage we have no data to indicate that there is even a tail side to the coin. This is shown below in the left plot. If the third toss of the coin results in a tail then the right hand plot results. Notice that <span class="math notranslate nohighlight">\(H=1\)</span> is no longer possible as we know that the coin can generate tails as well as heads</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;arviz-darkgrid&quot;</span><span class="p">)</span>

<span class="n">grid_points</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># use lower number for testing</span>

<span class="k">def</span> <span class="nf">uniform_prior</span><span class="p">(</span><span class="n">grid_points</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns Uniform prior density</span>

<span class="sd">            Parameters:</span>
<span class="sd">                grid_points (numpy.array): Array of prior values</span>

<span class="sd">            Returns:</span>
<span class="sd">                density (numpy.array): Uniform density of prior values</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">grid_points</span><span class="p">)</span>


<span class="c1"># define grid</span>
<span class="n">p_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">grid_points</span><span class="p">)</span> <span class="c1"># these are the x values</span>

<span class="c1"># define prior</span>
<span class="n">prior</span> <span class="o">=</span> <span class="n">uniform_prior</span><span class="p">(</span><span class="n">grid_points</span><span class="p">)</span> <span class="c1"># y values </span>

<span class="c1"># compute likelihood at each point in the grid</span>
<span class="c1"># For single toss that gives 1 head, binomial function gives likelihood = H</span>
<span class="c1"># compute product of likelihood and prior</span>
<span class="c1">#unstd_posterior = likelihood * prior</span>
<span class="n">likelihood</span> <span class="o">=</span> <span class="n">p_grid</span><span class="o">*</span><span class="n">prior</span>
<span class="n">unstd_posterior</span> <span class="o">=</span> <span class="n">likelihood</span> <span class="o">*</span> <span class="n">prior</span>

<span class="n">prior</span> <span class="o">=</span> <span class="n">unstd_posterior</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span> <span class="c1"># old posterior becomes new prior</span>

<span class="n">likelihood</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p_grid</span><span class="p">)</span>
<span class="n">unstd_posterior</span> <span class="o">=</span> <span class="n">likelihood</span> <span class="o">*</span> <span class="n">prior</span>
<span class="c1"># standardize the posterior, so it sums to 1</span>
<span class="n">posterior</span> <span class="o">=</span> <span class="n">unstd_posterior</span> <span class="o">/</span> <span class="n">unstd_posterior</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="c1">#return p_grid, posterior</span>
<span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># Create a figure containing a single axes.</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_grid</span><span class="p">,</span><span class="n">posterior</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Two heads, 0 Tails&#39;</span><span class="p">)</span>  <span class="c1"># Plot prior on the axes.</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Hypothesis&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Probability&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>

<span class="c1"># Now add in a tail</span>
<span class="n">prior</span> <span class="o">=</span> <span class="n">unstd_posterior</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span> <span class="c1"># old posterior becomes new prior</span>
<span class="n">likelihood</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p_grid</span><span class="p">)</span>
<span class="n">unstd_posterior</span> <span class="o">=</span> <span class="n">likelihood</span> <span class="o">*</span> <span class="n">prior</span>
<span class="c1"># standardize the posterior, so it sums to 1</span>
<span class="n">posterior</span> <span class="o">=</span> <span class="n">unstd_posterior</span> <span class="o">/</span> <span class="n">unstd_posterior</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="c1">#return p_grid, posterior</span>

<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_grid</span><span class="p">,</span><span class="n">posterior</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Two heads, 1 Tail&#39;</span><span class="p">)</span>  <span class="c1"># Plot prior on the axes.</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">ax1</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span><span class="n">ax1</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># make plots have same range</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Hypothesis&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Probability&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x141e6b490&gt;
</pre></div>
</div>
<img alt="_images/165f1e09f8e0588784667901d37613a92fba8f386e3b9e550acf750c4066c685.png" src="_images/165f1e09f8e0588784667901d37613a92fba8f386e3b9e550acf750c4066c685.png" />
</div>
</div>
<p>The next figure shows what happens as the number of draws increases from 1 to just over 1000. Notice that the peak probability value converges towards <span class="math notranslate nohighlight">\(~0.3\)</span> - this is the value chosen for this simulation.</p>
<figure class="align-default" id="flips-fig">
<img alt="_images/NCoinFlips.png" src="_images/NCoinFlips.png" />
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">Development of Posterior Probability as number of coin flips increase from 1 to 1024.</span><a class="headerlink" href="#flips-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The code used to generate <a class="reference internal" href="#flips-fig"><span class="std std-numref">Figure 1</span></a> is given at the end of this chapter. You could use it to explore the impact of the prior probability on the outcome. A good starting place is to assume that the coin is at least close to fair and use a normal distribution centred on <span class="math notranslate nohighlight">\(H=0.5\)</span>. After all, even in Las Vegas two-headed coins are rare.</p>
</section>
<section id="summary-statistics">
<h2>Summary statistics<a class="headerlink" href="#summary-statistics" title="Permalink to this heading">#</a></h2>
<p>Pictoral summaries of results such as those in <a class="reference internal" href="#flips-fig"><span class="std std-numref">Figure 1</span></a> are great, but often we want a short summary of the results – factors such as the expected value (best estimate), the standard deviation and a confidence interval.</p>
<section id="best-estimates">
<h3>Best estimates<a class="headerlink" href="#best-estimates" title="Permalink to this heading">#</a></h3>
<p>Using the notation from the coin flipping case we want to find the <span class="math notranslate nohighlight">\(H\)</span> that corresponds to the maximum probability. This best estimate <span class="math notranslate nohighlight">\(H_0\)</span> occurs where:</p>
<div class="math notranslate nohighlight" id="equation-1stderiv">
<span class="eqno">(8)<a class="headerlink" href="#equation-1stderiv" title="Permalink to this equation">#</a></span>\[\frac{dP}{d H}\biggr\rvert_{H_0} = 0\]</div>
<p>(note, strictly we also require that the 2nd derivative is less than zero at this point to ensure that we are at a maximum).</p>
</section>
<section id="reliability-of-fits">
<h3>Reliability of fits<a class="headerlink" href="#reliability-of-fits" title="Permalink to this heading">#</a></h3>
<p>This is a measure of the width/spread of the distribution around <span class="math notranslate nohighlight">\(H_0\)</span>. If this is narrow then we can be reasonably confident that we have a reliable measure of the parameter(s) and <span class="math notranslate nohighlight">\(H_0\)</span> otherwise we should treat the model with caution.</p>
<p>A useful approach is to consider a Taylor expansion of the posterior probability distribution around <span class="math notranslate nohighlight">\(H_0\)</span>. This expansion gives a way to treat the local behavior of the function as a low-order polynomial. Now, the coin flipping example has a reasonable straightforward posterior distribution, but in general this will not be the case and the form of the distribution is likely to be complex. In order to minimise the impact of this structure it is conventional to work with the logarithm <span class="math notranslate nohighlight">\(L\)</span> which is a slower changing function. The Taylor expansion of <span class="math notranslate nohighlight">\(L\)</span> around <span class="math notranslate nohighlight">\(H_0\)</span> and neglecting terms higher than the second derivative is:</p>
<div class="math notranslate nohighlight" id="equation-taylorl">
<span class="eqno">(9)<a class="headerlink" href="#equation-taylorl" title="Permalink to this equation">#</a></span>\[L = L(H_0) + \frac{1}{2}\frac{d^2L}{dH^2}\biggr\rvert_{H_0}(H-H_0)^2\]</div>
<p>Only the second term on the right hand side contributes to the shape of this function. Taking the exponential of equation <a class="reference internal" href="#equation-taylorl">(9)</a> gives:</p>
<div class="math notranslate nohighlight">
\[P(H|{data},I) \approx A\exp\left(\frac{1}{2}\frac{d^2L}{d\theta^2}\biggr\rvert_{H_0}(H-H_0)^2\right)\]</div>
<p>Thus, if higher order terms in equation <a class="reference internal" href="#equation-taylorl">(9)</a> can be neglected then the posterior probability function can be approximated by a normal distribution with a standard deviation given by:</p>
<div class="math notranslate nohighlight">
\[\sigma = \left(-\frac{d^2L}{2dH^2}\biggr\rvert_{H_0}\right)\]</div>
<p>The above is presented for the coin flipping model, but the same analysis applies to a broad range of data/models.</p>
<div class="dropdown admonition">
<p class="admonition-title">Exercise</p>
<p>Show that for the coin example:</p>
<div class="math notranslate nohighlight">
\[H_0=\frac{R}{N}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\sigma = \sqrt{\frac{H_0(1-H_0)}{N}}\]</div>
</div>
</section>
<section id="mean-mode-and-median">
<h3>Mean, mode, and median<a class="headerlink" href="#mean-mode-and-median" title="Permalink to this heading">#</a></h3>
<p>The mean, mode, and median are examples of point estimates of the most likely value for the parameter according to the posterior probability density (distribution). In the coin flipping example these estimates converge when there is a sufficient amount of data. However, early in the process illustrated in <a class="reference internal" href="#flips-fig"><span class="std std-numref">Figure 1</span></a> this is not the case. Indeed for many real-life situations, particularly when there is a small amount of data, these estimates will not coincide and a decision often has to be made as to which to report. Ben Lambert recomends the following hierarchy<span id="id1">[<a class="reference internal" href="#id77" title="Ben Lambert. A Student's Guide to Bayesian Statistics. SAGE, Los Angeles London New Delhi Singapore Washington DC Melbourne, 2018. ISBN 978-1-4739-1635-7 978-1-4739-1636-4.">ALam18</a>]</span>:</p>
<ul class="simple">
<li><p>Mean: <span class="math notranslate nohighlight">\(E(\theta|data)=\int\theta prob(\theta|data)d\theta\)</span></p></li>
<li><p>Median: <span class="math notranslate nohighlight">\(\widetilde{\theta}\)</span> such that <span class="math notranslate nohighlight">\(\frac{1}{2}=\int_{-\infty}^{\widetilde{\theta}}prob(\theta|y)d\theta\)</span></p></li>
<li><p>Mode: <span class="math notranslate nohighlight">\(\theta_M \equiv \arg\{\max_\theta[prob(\theta|y)]\}\)</span></p></li>
</ul>
</section>
<section id="credible-intervals">
<h3>Credible intervals<a class="headerlink" href="#credible-intervals" title="Permalink to this heading">#</a></h3>
<p>In standard statistic the confidence interval is often used as a measure of the uncertainty of the value of our point estimate. However, these are problematic to calculate and there is often confusion over what they mean. Bayesian methods allow an alternative measure to be calculates the <em>credible interval</em>. This is the range that contains a given percentage of the probability density of the posterior probability. It is calculated via:</p>
<div class="math notranslate nohighlight" id="equation-ci">
<span class="eqno">(10)<a class="headerlink" href="#equation-ci" title="Permalink to this equation">#</a></span>\[\int_Rd\theta Prob(\theta|D,I)=C\]</div>
<p>where <span class="math notranslate nohighlight">\(C=.95\)</span> for a 95% credible interval. Other intervals are possible and indeed particle physics uses them, but other fields are not so free-thinking.</p>
<p>There is more that can be done with analytical approaches to Bayesian data analysis. In general the tractable problems tend to be relatively simple. Useful descriptions can be found in Devinda Sivia’s ‘Data Analysis - a Bayesian Tutorial’<span id="id2">[<a class="reference internal" href="ModelSelection.html#id134" title="D. S. Sivia and J. Skilling. Data Analysis - a Bayesian Tutorial. Oxford Science Publications. Oxford University Press, 2 edition, 2006.">SS06</a>]</span> and ‘Bayesian Logical Data Analysis…’ by Phil Gregory<span id="id3">[<a class="reference internal" href="#id54" title="Phil Gregory. Bayesian Logical Data Analysis for the Physical Sciences: A Comparative Approach with Mathematica® Support. Cambridge University Press, Cambridge, 2005. doi:10.1017/CBO9780511791277.">AGre05</a>]</span>.</p>
</section>
</section>
<section id="the-use-and-the-choice-of-priors">
<h2>The use and the choice of priors<a class="headerlink" href="#the-use-and-the-choice-of-priors" title="Permalink to this heading">#</a></h2>
<p>The use of a prior probability distribution is one of the key differentiators between Bayesian and frequentist methods. These notes will not dwell on this only to note that there have been somewhat heated arguments about the use of priors over the years.  The reality is that the methods do work and life is too short to worry about the nuances of the arguments.</p>
<p>The coin flipping problem discussed above used a flat prior - all probabilities are equally likely. These are sometimes referred to as <em>noninformative</em> priors. Although easy to implement they are often not very useful. This can be very important when there are low numbers of data. It took about fifty coin flips to get a values of <span class="math notranslate nohighlight">\(H\)</span> less than <span class="math notranslate nohighlight">\(~0.5\)</span>. In general we have a lot of knowledge about a system, so it makes sense to make use of this. When you are crossing a road, your brain uses what it knows about typical traffic speeds to help it estimate the speed of vehicles that you can see - why wouldn’t you use what you know about a system to help model it?</p>
<p>Working out just what sort of prior to use can be a daunting step. These notes will introduce some as they procede. In science/engineering it can be reasonably easy to produce useful <em>weakly informative</em> priors. We generally know what are reasonable upper and lower limits for parameters. In fact it is arguable that the prior used in the coin flipping example was weakly informative - in that prior the coin could have had up to two heads/tails, but no more, but mathematically the coin we modelled could have had any number of sides.</p>
<p>Ben Lambert (the author of <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">student's</span> <span class="pre">guide</span> <span class="pre">to</span> <span class="pre">bayesian</span> <span class="pre">statistics</span></code><span id="id4">[<a class="reference internal" href="#id77" title="Ben Lambert. A Student's Guide to Bayesian Statistics. SAGE, Los Angeles London New Delhi Singapore Washington DC Melbourne, 2018. ISBN 978-1-4739-1635-7 978-1-4739-1636-4.">ALam18</a>]</span>) has made a handy web-browser based tool for visualising a large range of distributions<span id="id5">[<a class="reference internal" href="#id76" title="Ben Lambert. The distribution zoo. https://ben18785.shinyapps.io/distribution-zoo/.">ALam</a>]</span>. You can use this to get a feel for the properties of probability distributions so that you can consider what might be best for your model and data.</p>
<p>There are useful notes/advice on this page:<a class="reference external" href="https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations">Stan: Prior choice recommendations</a>. This is a work in progress document and contains some reasonably high-level material.</p>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this heading">#</a></h2>
<p>In this section we have seen how Bayes’ theorem can be used to deduce the posterior probability distribution and hence the infer the most likely vaue for a parameter (we could even deduce a credible range for the paremeter).</p>
<div class="docutils container" id="id6">
<dl class="citation">
<dt class="label" id="id54"><span class="brackets"><a class="fn-backref" href="#id3">AGre05</a></span></dt>
<dd><p>Phil Gregory. <em>Bayesian Logical Data Analysis for the Physical Sciences: A Comparative Approach with Mathematica® Support</em>. Cambridge University Press, Cambridge, 2005. <a class="reference external" href="https://doi.org/10.1017/CBO9780511791277">doi:10.1017/CBO9780511791277</a>.</p>
</dd>
<dt class="label" id="id76"><span class="brackets"><a class="fn-backref" href="#id5">ALam</a></span></dt>
<dd><p>Ben Lambert. The distribution zoo. https://ben18785.shinyapps.io/distribution-zoo/.</p>
</dd>
<dt class="label" id="id77"><span class="brackets">ALam18</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id4">2</a>)</span></dt>
<dd><p>Ben Lambert. <em>A Student's Guide to Bayesian Statistics</em>. SAGE, Los Angeles London New Delhi Singapore Washington DC Melbourne, 2018. ISBN 978-1-4739-1635-7 978-1-4739-1636-4.</p>
</dd>
</dl>
</div>
</section>
<section id="appendix-coin-flipping-code">
<h2>Appendix - coin flipping code:<a class="headerlink" href="#appendix-coin-flipping-code" title="Permalink to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">stats</span>

<span class="c1"># This code uses a loop to repeat the process used above </span>
<span class="c1"># - a posterior is calculated as the product of the prior and likelihood</span>
<span class="c1"># the posterior becomes the prior for the next iteration of the loop.</span>
<span class="c1"># each iteratin of the loop gets the number of tosses from the list</span>
<span class="c1"># N_toss_list and counts the number of heads in the simulation up to that</span>
<span class="c1"># number of coin tosses. </span>

<span class="k">def</span> <span class="nf">countHeads</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">n</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;counts the number of heads (1) in first n elements of array data&quot;&quot;&quot;</span>
    <span class="nb">sum</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span> <span class="nb">sum</span> <span class="o">=</span> <span class="nb">sum</span> <span class="o">+</span><span class="mi">1</span>
    <span class="k">return</span> <span class="nb">sum</span>

    
<span class="n">grid_points</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># use lower number for testing</span>

<span class="c1"># create an array of coin toss results: 1=H, 0=T</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
<span class="c1"># results of first three tosses are defined above</span>
<span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">results</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">results</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="c1"># now fill the rst of the array with coin tosses head probability given by success_true</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1024</span><span class="p">):</span>
    <span class="n">results</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">success_true</span><span class="p">)</span>

<span class="c1"># make an array of plots showing the effect of adding the tosses successively</span>
<span class="c1"># make a list of the number of tosses to show plots for</span>
<span class="n">N_toss_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]</span>


<span class="c1"># set up uniform prior for start of calculation</span>
<span class="c1"># define grid</span>
<span class="n">p_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">grid_points</span><span class="p">)</span> <span class="c1"># these are the x values</span>

<span class="c1"># initial settings for prior and likelihood</span>
<span class="n">prior</span> <span class="o">=</span> <span class="n">uniform_prior</span><span class="p">(</span><span class="n">grid_points</span><span class="p">)</span> <span class="c1"># y values </span>
<span class="n">likelihood</span> <span class="o">=</span> <span class="n">uniform_prior</span><span class="p">(</span><span class="n">grid_points</span><span class="p">)</span>
<span class="c1"># use an array of subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">ncols</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="k">for</span> <span class="n">frame</span><span class="p">,</span> <span class="n">N</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">N_toss_list</span><span class="p">):</span>
    <span class="c1"># each iteration of this loop adds another plot to the array </span>
    <span class="c1"># of outputs</span>
    <span class="n">unstd_posterior</span> <span class="o">=</span> <span class="n">likelihood</span> <span class="o">*</span> <span class="n">prior</span>
    <span class="c1"># update likelihood and prior</span>
    <span class="n">prior</span> <span class="o">=</span> <span class="n">unstd_posterior</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span> <span class="c1"># old posterior becomes new prior</span>
    <span class="n">nheads</span> <span class="o">=</span> <span class="n">countHeads</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span> 
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">nheads</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p_grid</span><span class="p">)</span>
    <span class="n">unstd_posterior</span> <span class="o">=</span> <span class="n">likelihood</span> <span class="o">*</span> <span class="n">prior</span>
    <span class="c1"># standardize the posterior, so it sums to 1</span>
    <span class="n">posterior</span> <span class="o">=</span> <span class="n">unstd_posterior</span> <span class="o">/</span> <span class="n">unstd_posterior</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="c1"># next line puts data into next frame in sequence</span>
    <span class="c1"># it uses integer maths to work out the row and column numbers</span>
    <span class="n">ax</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">frame</span><span class="o">/</span><span class="mi">4</span><span class="p">),</span><span class="n">frame</span><span class="o">%</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_grid</span><span class="p">,</span><span class="n">posterior</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;N=</span><span class="si">{</span><span class="n">N_toss_list</span><span class="p">[</span><span class="n">frame</span><span class="p">]</span><span class="si">}</span><span class="s2">, H=</span><span class="si">{</span><span class="n">nheads</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># Plot prior on the axes.</span>
    <span class="n">ax</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">frame</span><span class="o">/</span><span class="mi">4</span><span class="p">),</span><span class="n">frame</span><span class="o">%</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">frame</span><span class="o">/</span><span class="mi">4</span><span class="p">),</span><span class="n">frame</span><span class="o">%</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">frame</span><span class="o">/</span><span class="mi">4</span><span class="p">),</span><span class="n">frame</span><span class="o">%</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s2">&quot;NCoinFlips.pdf&quot;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="n">fig</span><span class="o">.</span><span class="n">dpi</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="BayesTheorem.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Probability and Bayes’ theorem</p>
      </div>
    </a>
    <a class="right-next"
       href="ModelSelection.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">A (very) Brief Introduction to Model Selection the Bayesian way</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-first-application-of-bayes-theorem-the-fair-coin">A first application of Bayes’ Theorem – the fair coin</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-statistics">Summary statistics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#best-estimates">Best estimates</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reliability-of-fits">Reliability of fits</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-mode-and-median">Mean, mode, and median</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#credible-intervals">Credible intervals</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-use-and-the-choice-of-priors">The use and the choice of priors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix-coin-flipping-code">Appendix - coin flipping code:</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Simon Martin
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024-25.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>